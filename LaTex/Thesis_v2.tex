\documentclass{article}
\usepackage{tikz}
\usetikzlibrary{decorations.pathreplacing}
\usetikzlibrary{positioning}
\usepackage{hyperref}
\usepackage{apacite}
\usepackage{float}
\usepackage{geometry}
\usepackage{setspace}
\usepackage{adjustbox}
\usepackage{multirow}
\usepackage[T1]{fontenc} % for smaller printing smaller than sign correctly in plots
\usepackage[labelfont=bf]{caption} % for captions
\usepackage[toc]{appendix} % toc to show in table of contents
\usepackage{tabularx} % expanding tables
\usepackage{xltabular} % multipage tabularx
\usepackage{amsmath} % Align equations
\usepackage{booktabs} % toprule etc..
\usepackage{parskip}
\setstretch{1.5}
\geometry{left=20mm, right=50mm, top=20mm, bottom=10mm, includefoot}
\pagecolor{white}

\title{Cross-sectional predictability of stock returns in Nordic stock markets using machine learning methods}
\author{Jesse Keränen}
%add time period to all plot and tables once final

\begin{document}
\begin{titlepage}
\begin{center}
\vspace*{5em}
Master thesis submitted in partial fulfillment of the requirements for the degree\\
Master of Science \\
at Technische Universität München 

\vspace{7em}

\Large\textbf{Cross-sectional predictability of stock returns in Nordic stock markets using machine learning methods}

\vspace{7em}

\end{center}

\begin{tabular}{p{10em} l} 
\multirow[t]{4}{*}{Reviewer} 		& Prof. Dr. Christoph Kaserer\\ 
							& Department of Financial Management and Capital Markets\\ 
							& TUM School of Management\\ 
							& Technische Universität München\\[3ex]
Advisor: 						& Noorhan Elkhayat\\[3ex]
Study program: 				& TUM-BWL\\[3ex]
\multirow[t]{5}{*}{Composed by:} 	& Jesse Keränen\\
							& Motorstraße 64\\
							& 80809 Munich\\
							& Tel.: +49 (0) 1628410926\\
							& Matriculation number: 03748837\\[3ex]
Submitted on: 					& \today \\
\end{tabular}

\thispagestyle{empty}
\end{titlepage}

\newpage
\pagenumbering{Roman}

\tableofcontents

\newpage

\listoffigures
\listoftables

\newpage

\section{Introduction} \label{Introduction}
\pagenumbering{arabic}
In 1808 world was in many ways different compared to what it is today. In 1808 Napoleon was the Emperor of the French Empire and Maximilian I was the King of Kingdom of Bavaria. In 1808 Finnish war broke between Kingdom of Sweden and the Russian Empire which would ultimately lead to establishment of autonomous Grand Duchy of Finland. It would still take more than 100 years for Finland to gain its independence. Same year began the Dano-Swedish war between Sweden and Denmark-Norway. Something historically far less remarkable, but essential for this study happened in 1808 as well. First stock exchange in a Nordic country was opened in Copenhagen. Slowly after that rest of the Nordic countries would open their own stock exchanges as well. Upon facilitated change of ownership of assets, investors were left with a question how to price these assets. \par

Major breakthrough in this topic happened when capital asset pricing model was developed in the sixties (sharpe, treynor etc...). In the eighties persofrmance of capital asset pricing model was questioned and scholars came up with so called stock market factors (Earnings-to-price = Basu(1977), Earnings-to-price = Basu(1977), Profitability = e.g., Basu, 1983). During these time machine learning gained large interest and artificial neural networks were popularized. Next big step in asset pricing happened when Eugene Fama and Kenneth French combined these factors to three factor asset pricing model (fama french 1993). Two years after that machine learning method called random forest was introduced (Ho in 1995). Although three factor model was remarkable improvement compared to capital asset pricing model it was not able to explain variation in stock returns completely. In recent years lot of research has applied machine learning methods to capture abnormal returns in stock markets. \par

%Nordic market introduction
Objective of this study is to apply set of machine learning methods to well established asset pricing factors to capture abnormal stock return patterns. This study will focus on four Nordic stock markets namely Denmark, Finland, Norway and Sweden. These four markets are relatively homogenous in many aspects. They are geographically close, politically stable and economically interconnected. Denmark, Finland and Sweden all belong to European union. Additionally, stock exchanges of all these three countries are operated by Nasdaq. Therefore, investors could view them as a single market. Some of the features that are characteristic for Nordic markets make them fertile ground for stock market anomaly studies. As mentioned Nordic countries are geographically closely located in northern Europe and therefore relatively distant from large European and especially American markets. European market integration is emphazised also by Fama and French \citeyear{FAMA2012457}. \par

So called periphery effect has been studied by the scholars a lot. %references)
It refers to investor behaviour where during times of a crisis investors tend to liquidate their investments first from the markets more distant to them. This increases the volatility of periphery markets and can challenge the efficient market theory. Another common feature that Nordic markets share is a high level of foreign ownership. Share of foreign investments in Nordic stock markets can reach more than 50\% \footnotemark. Given the remote location of Nordic stock markets and their high share of foreign ownership it is likely that Nordic countries could be subject to periphery effect. Which again can result abnormal returns. \par

This study contributes to the existing literature in several ways. First of all it applies machine learning framework from Gu, Kelly and Xiu \citeyear{guetal} to new market. Machine learning approach has been applied to European markets, but this study focuses on arguably even more coherent nordic submarket \cite{Drobetz, Fieberg}. Objective is to examine if investor investing only in Nordic markets could benefit from implementing the machine learning framework of Gu et al. \citeyear{guetal}. Dataset of study is unique in a sense that lot of machine learning approach studied have been conducted in more wider markets such as European stock markets, where as Nordic stock market anomaly studies mainly focuses on single markets. Pooling the four Nordic markets ensures us sufficient amount of data to train complex machine learning model, but also allows us to focus on homogenous clearly defined submarket. \par

Additionally, existing stock market anomaly literature focusing solely to Nordic markets is rather limited. Section \ref{NordicStockMarketAnomalies} introduces the existing stock market anomaly literature. Characteristic for studies in this section is that they mainly focus on one to two stock market anomalies. As this constructs 23 stock characteristic anomalies, it allows us examine anomalies that have not been applied to Nordic markets previously. Meaning that this study can not only reveal the profitability of machine learning framework in Nordic market setting, but it can also reveal existence of certain stock market anomalies in Nordic markets. As mentioned lot of Nordic stock market anomaly research focuses only up to two anomalies at once. Since this study includes 23 anomalies simultaneously, it allows us also to examine performance of already discovered Nordic anomalies while controlling for many other variables. Applying sophisticated machine learning models allows us to also to control for complex interactions. \par

Objective of this study is slightly more ambitious than in existing Nordic stock market anomaly literature. Existing literature mainly examines existence of anomalies by uni- or multivariate portfolio sorts. Studies predefine variable of interest and form portfolios based on that variable. Then the historical excess returns are investigated. This study goes one step further and attempts to predict stock level out-of-sample returns based on the predefined set of variables. This allows us to evaluate which portion of the return variability these variables are able to capture in addition to the profitability of the strategy. \par

Final contribution of this study is to expand the explanatory variable set. This study includes variable called on-balance volume. On-balance volume is a technical trading indicator which has not been studied in great extend as cross-sectional stock return predictor. Due to strong performance of momentum indicators, this study includes several momentum indicators. Extended variable set allows us to examine whether on-balance anomaly exists in Nordic stock markets or whether including on-balance volume affects performance of well established momentum indicators. \par

%all developed, but small -> liquidity

Structure of this paper goes as follows. In second chapter introduction to related literature is provided. In this chapter performance of different methods and persistence of different anomalies in different regions is discussed. Third chapter introduces the data used in this study and filters applied to the data. Fourth chapter presents the methodology. It introduces the implemented models in more detail and describes the measurements applied for the models. Fifth chapter describes the results of the empirical study. Fifth chapter is divided to discuss separately predictive accuracy, economic profitability and covariate importance for the machine learning models.Finally the last chapter provides conclusion of the empirical study. \par

\footnotetext{Butt and Hogholm \citeyear{ButtHogholm2020} calculate share of foreign ownership from IMF Coordinated Direct Investment Survey CDIS data. Foreign ownership share of Butt and Hogholm is 52\% for Denmark, 42\% for Finland, 35\% for Norway and 56\% for Sweden. %check the source
}

\section{Literature}\label{Literature}
Being the largest and most prominent stock market in the world US stock market has been subject to majority of asset pricing studies. Despite the dominance of US markets in capturing the the attraction of the academics lot of anomaly asset pricing literature has been conducted in international setting as well. Characteristic for international asset pricing literature is that instead of focusing on single countries they aggregate stock market data to a certain regional level such as Europe or Asia-Pacific. Following chapter provides an overview for pioneering asset pricing anomaly literature. Focus will mainly be on literature on US and European markets. US stock markets are chosen because of their significant impact on international stock markets and because most anomalies have been discovered there and therefore majority of the initial studies of these anomalies have been conducted there. European studies provide interesting perspective for this study since in many of them Nordic countries are included. \par

Chapter introduces the most important anomalies in these markets and how they have been exploited with different methods. This works as starting point to define set of factors that will be used in this study. It can be argued that this kind of process when the set of variables are chosen based on their performance in previous studies is one sort of forward looking information if we try to mimic investors information set. On the other hand Jacobs and Müller \citeyear{JACOBS2020213} only find reliable post-publication decline in long/short returns in US which emphasizes the practical potential of this study. \par

\subsection{US stock market anomalies}\label{USStockMarketAnomalies}

Many of the recent cross-sectional stock return studies use framework of Lewellen \citeyear{Lewellen2015} as base model. He runs 10-year rolling Fama-MacBeth regressions using lagged firm characteristics to predict out of the sample stock returns. He studies cross-sections of US stock return between 1964 and 2013 using different model settings up to 15 company characteristics. He finds strong positive correlation between expected return derived from rolling Fama-MacBeth regressions and realised returns. Additionally Lewellen shows that spread between realised return of portfolio formed from stock with lowest expected returns and portfolio with highest expected return is up to 2.36\%. In his study logarithmic market value of equity, logarithmic book-to-market value, momentum and accruals show the strongest statistical power in explaining monthly returns using lagged variables. \par

Gu, Jelly and Xiu \citeyear{guetal} contribute to the literature by applying machine learning methods to exploit stock market anomalies. By deploying sophisticated models that do not suffer from over parameterization as heavily as OLS Gu et al. are able to include 94 stock characteristics and their interactions as well as eight aggregated time series variables to their models. Gu et al. use large variety of statistical methods including linear regression, generalized linear models with penalization, dimension reduction via principal components regression and partial least squares, gradient boosted regression trees, random forest and different settings of neural networks. Out of these gradient boosted regression trees and neural networks \footnotemark explain the monthly out of sample stock return the best reaching out of sample $R^{2}$ 0.33 and 0.44 correspondingly where as three factor OLS model introduced by Lewellen \citeyear{Lewellen2015} only reaches out of sample $R^{2}$ of -3.46. \par

\footnotetext{Gu et al. \citeyear{guetal} use five different settings of neural networks differing by number of hidden layers. Neural network with three hidden layers reaches the highest $R^{2}_{oos}$ and is reported here.}

Similar to Lewellen \citeyear{Lewellen2015} Gu et al. construct portfolios based on predicted return of different models. Monthly spread in realized return between portfolio constructed from decile of companies with lowest expected return and decile of stocks with highest expected return \footnotemark is 0.94, 1.62 and 2.12 for models based on OLS, random forest and three layer neural network correspondingly. Gu et al. also show that all methods they examine show somewhat similar patterns on variable importance on return predictability. Most important factors are price trends such as momentum followed by stock liquidity, stock volatility, and valuation ratios. \par
% all stocks, confirm again
% Deeper neural networks seem to be too complex

\footnotetext{Portfolio returns are average value weighted returns.}

\subsection{European stock market anomalies}\label{EuropeanStockMarketAnomalies}

As mentioned US market environment is different in my ways compared to Nordic markets. Fortunately lot of stock market studies have been conducted in Europe and since Nordic markets are usually just a subset of European markets it can be beneficial to have a look on European studies. Tobek and Hronec \citeyear{TOBEK2021100588} study machine learning based anomaly strategies in international setting. Their study includes 153 different equity anomalies and they only include anomalies to their data after documented discovery of corresponding anomaly. This way they can mimic the information set investor would have had and avoid forward looking information. Tobek and Hronec examine five different models including weighted least squares, penalized weighted least squares, gradient boosting regression trees, random forest and neural networks. Their data set spans from 1990 to 2018. \par

Similar to Gu et al. \citeyear{guetal} in US, Tobek and Hronec find that strategy using neural networks provides highest returns on quintile long-short portfolios. Mean return for neural network long-short portfolio in Europe was 0.7. Interestingly penalized weighted least square method provided mean return of 0.651 which is higher than return of random forest based portfolio's return of 0.396. Tobek and Hronec find that Industry momentum, lagged momentum, liquidity shocks, 52 week high, book-to-market value and return on equity are the most important variables for neural networks model.\footnotemark \par

\footnotetext{Tobek and Hronec \citeyear{TOBEK2021100588} discover possibilities training models either only using historical data from US, using historical data from local markets or using international historical data. Only results for models trained using local data are reported here because that is closest to the setting of this study. Additionally, Tobek and Hronec state that difference between model trained on US data and local data are small.}

Exploiting stock market anomalies using machine learning methods is also studied by Drobetz and Otto \citeyear{Drobetz}. Their data set contains all companies listed in at least one of the 19 Eurozone countries on December 2020 and spans form 1990 to 2020 \footnotemark. Drobetz and Otto examine performance of ordinary least squares, penalized least squares, principal components regressions, partial least squares, random forests, gradient boosted regression trees and neural networks on predicting monthly stock level return exploiting a set of 22 predictions, their two-way interactions and second- and third-order polynomials. Findings of Drobetz and Otto are similar to Gu et al. \citeyear{guetal}. They show that with large number of explanatory variables simple linear regression is not able to explain out of the sample stock returns. \par

\footnotetext{Finland is the only country included in the study of Drobetz and Otto \citeyear{Drobetz} that is also included in this study, since it is the only country belonging to Eurozone.}

Findings of Drobetz and Otto \citeyear{Drobetz} are also similar to Tobek and Hronec \citeyear{TOBEK2021100588} in a sense that least squares methods where dimensionality is restricted can actually perform better than tree based methods. Like in majority of other literature, Drobetz and Otto find out that neural networks provide superior framework for stock return prediction model measured in both explanatory power and profitability. Neural network method reaches out of the sample $R^{2}$ value of 1.23 and long-short portfolio formed based on expected returns derived from neural networks model provide average value weighted monthly return of 1.94\%. Similar to Gu et al. \citeyear{guetal} Drobetz and Otto find that same variables show the most importance across the different models, most notably earnings-to-price ratio and 12 month momentum. \par
%Deeper neural networks seem to be too complex

Fieberg et al \citeyear{Fieberg} study stock market anomalies in 16 European stock markets using machine learning methods over almost the same period as Drobetz and Otto \citeyear{Drobetz} \footnotemark. Nevertheless they choose a slightly different approach where instead of including vast set of anomalies, they only consider six prominent equity factors. Their conclusion endorses findings of Drobetz and Otto \citeyear{Drobetz} and Tobek and Hronec \citeyear{TOBEK2021100588} as they shown that more complex machine learning models beat linear approach in terms of both economic and statistical performance. \par

\footnotetext{Dataset of Fieberg et al \citeyear{Fieberg} contains Denmark, Finland, Norway and Sweden.}

\subsection{Nordic stock market anomalies}\label{NordicStockMarketAnomalies}

This chapter provides an overview of observed stock market anomalies in different Nordic stock markets. Many studies in this chapter have slightly different objective than this study. Studies show the existence of the anomalies by constructing a portfolio heavily weighted on certain factor. Nevertheless, they do not describe the magnitude of the relationship between the factor and the stock returns. This study has slightly more ambitious objective and tries to derive return expectations from predefined stock market factors. This literature review serves as starting point for choosing most promising stock market factors that have already been studied. \par
%small amount of stocks

Magnitude of value and momentum anomalies in Nordic stock markets are examined in the paper by Grobys and Huhta-Halkola \citeyear{grobys}. They combine information from companies listed in main lists of Danish, Finnish, Norwegian and Swedish stock exchanges between 1991 and 2017. Grobys and Huhta-Halkola measure value with book-to-market value and momentum with past 12-month total shareholder equity%return??? % 
. Grobys and Huhta-Halkola show that momentum effect exists in Nordics markets and profitability of momentum strategy is not related to size factor. Value factor yields also significant excess return, but according to Grobys and Huhta-Halkola it could be partly driven by the size factor, since value premium reduces when accounted for the size. Among all stocks monthly average equally weighted long-short return is 1.73\% and 1.25\% for momentum and value strategies correspondingly. Both of the excess returns are statistically highly significant. Grobys and Huhta-Halkola also test combination strategies using signals from both momentum and strategy which yield even stronger results. \par

Value premium has shown consistency in Finnish stock markets. Davydov, Tikkanen and Äijö \citeyear{Davydov2017MagicFV} examine profitability of different value investing strategies between 1991 and 2013. Davydov et. al. investigate set of value indicators which included earnings to price, book to price, cash flow to price, dividends to price and earnings before income and taxes to enterprise value ratios. Additionally they test performance of investing strategy developed by Greenblatt (cite here) where portfolios are formed based on combined ranking of company's return on invested capital and earnings before income and taxes to enterprise value ratio. They show that returns of all of the value portfolios not only beat the market return, but can also not be explained by the four factor model of Carhart \citeyear{https://doi.org/10.1111/j.1540-6261.1997.tb03808.x}.  \par
%(Finnish Stock Exchange in the period 1991–2013. In total, the number of companies varies from 39 to 136)
%check the second article from patari and leivo

Similar to Grobys and Huhta-Halkola \citeyear{grobys} Leivo and Pätäri \citeyear{leivo2011} combine value anomaly with momentum anomaly in Finnish stock market for data set between 1993 and 2008. They show that two step portfolio sort that first allocates stocks to three portfolios based on their value indicators and subsequently based on momentum indicator can capture extraordinary stock returns. Leivo and Pätäri show that including momentum further increases returns of already recognised value sorting. Strategy performs even better when authors allow for long position in high value high momentum portfolio and short position on low value low momentum portfolio. Excess returns resulting from the two-fold portfolio construction can not be explained by CAP-model or two factor model including also size factor. It is not a surprise that value and momentum premium show existence in Nordic markets. Value and momentum anomalies are among the most well documented factors showing persistence in multiple cross-sectional studies (e.g, Gu, Jelly and Xiu \citeyear{guetal}, Lewellen \citeyear{Lewellen2015}, Drobetz and Otto \citeyear{Drobetz}, Tobek and Hronec \citeyear{TOBEK2021100588}). \par
% Helsinki Stock Exchange (HEX; later OMX Helsinki) during the period 1993–2008. E/P, EBITDA/EV, CF/P, dividend yield (D/P), B/P and S/P and combinations.
%re-phrase the text

Nordic stock markets have several characteristic features. One is that all Nordic stock markets are considered to be developed, but also small. Especially market capitalization of companies listed in Nordic stock exchanges are on average much smaller than in US. Therefore, it is reasonable to ask whether liquidity of the stock could be driving factor of the stock returns. Impact of illiquidity risk to stock returns in Nordic market setting has been studied by Butt and Hogholm \citeyear{ButtHogholm2020}. Butt and Hogholm test variety of different illiquidity measures and find that dollar zero returns is the most profitable illiquidity anomaly measure across all four Nordic market. Dollar zero return measurement is calculated by dividing number of days stocks return in US dollars is zero by total number of trading days. Butt and Hogholm construct five quintile portfolios based on liquidity of the stocks with data spanning from 1988 to 2013. They show that in all Nordic markets there exists large illiquidity premium as annual difference in equal weighted return of most illiquid portfolio and least illiquid portfolio is more than 18\% for Finland, Norway and Sweden. For Denmark premium is slightly smaller 8.8\%. \par

Jokipii and Vähämaa \citeyear{jokipii2006free} investigate free cash flow anomaly in Finnish stock markets between 1992 and 2002. They construct portfolios from stocks listed in Finnish stock exchange based on predefined thresholds for free cash flow ratios. These ratios include market value to free cashflow and total debt to free cashflow ratios. High free cashflow portfolio yields higher returns than market on average and the excess returns can not be completely explained by weightings in Fama and French \citeyear{FAMA19933} risk factors. \par
%Finnish companies during the period 1992-2002. net cash flow from operating activities minus capital expenditures. The net cash flow from operating activities, in turn, is defined as the sum of net income, all non-cash charges and credits (e.g.,  depreciation,  amortization  of  intangibles,  and  deferred  taxes),  extra ordinary items, and net change in working capital

\section{Data}\label{Data}

This section provides an overview of the dataset used in this study. Section starts by introducing overall market setting in Nordic stock markets. Section discusses how companies are distributed across Nordic markets and also describes the size properties of the companies in different Nordic markets. This part also describes the static and dynamic screens applied to the Datastream data in order to ensure sufficient data quality. Second part of the section describes the firm level characteristics considered in this study. This stock level excess returns as well as all independent variables. This part introduces definitions of all variables including which characteristics are included calculation of each variable. Descriptive statistics of the firm level characteristics are also presented in this part of the study. \par

\subsection{Nordic stock market data}\label{NordicStockMarketData}

Main datasource for this study is Thomson Datastream. Company fundamentals data is collected from Worldscope database. Dataset spans from 1990 January to 2022 December which is shorter than in many previous studies. Reason why time period is limited to 1990 is that the amount of publicly listed companies in Nordic markets was rather low in the 1980's and finding reliable data for period before 1990 is difficult. Dataset contains all stocks listed in primary markets of corresponding countries including also companies that went bankrupt or were de-listed for any other reason. Therefore, dataset is not subject to survivorship bias. \par

Panel A of table \ref{table:constituteLists} in Appendix shows the constituent lists used in data collection. As highlighted by Ince and Porter \citeyear{Ince2006} data in Datastream can be noisy and uncleaned data could lead to a false statistical inference. Therefore, several static and dynamic screens are applied to the data. Static screens include filtering non-equity securities, securities that are not listed in respective country and securities that are quoted in currency other than respective country's currency. Panel A from table \ref{table:StaticScreens} shows which values are accepted for type of instrument, ISIN code, code indicating the country of origin of the company, country where the security is listed, currency in which the security is noted and ISIN country code. \par

In order to filter non-common and duplicate stock affiliations. Keywords indicating non-common stock affiliations are searched from Datastream attributes NAME, ENAME and ECNAME. Panel B of table \ref{table:StaticScreens} presents the country specific keywords. These keywords are only considered searched for securities from specific countries, but among all above mentioned attributes. Keywords from table \ref{table:GeneralKeywords} are searched from name attributes of securities from all countries. If a keyword is found from any of the name attributes, the security will be removed from the dataset. Keyword deletion follows Ince and Porter \citeyear{Ince2006} and Hanauer and Windmüller \citeyear{HANAUER2023106712}. \par

Ince and Porter \citeyear{Ince2006} argue that data quality issues in Datastream could even lead to wrong conclusions. In order to avoid results to be driven by extraordinary datapoints, which could be caused by data quality issues, dynamic screens are applied to the data. Table  \ref{table:DynamicScreens} in the appendix presents the applied dynamic screens. Observations are removed from the dataset in case of extreme abnormal return. Observations are also removed in case of extremely strong strong return reversals.\par

One characteristic has to be taken into consideration when working with data from Datastream. In case company is delisted for some reason Datastream returns last available value for remaining periods in the query. In order to only include actively traded securities these observations have to be cleaned from the dataset. This could be done with variable TIME from Datastream which shows the date of last equity price data. Nevertheless, Ince and Porter \citeyear{Ince2006} argue that the TIME attribute is not reliable indicator of the delisting date, but propose to remove consecutive zero returns from the end of the dataset. Removal of zero returns at the end of the dataset could lead to removal of actual zero returns, but the effect of this is considered to be smaller than the noise caused by the usage of TIME variable. Therefore, all consecutive zero returns at the end of the dataset are removed for all companies. \par

\begin{table}[h] 
\small
\caption[Country summary statistics]{\textbf{Country summary statistics}\\ Table provides summary statistics for pooled Nordic market and separate country specific Nordic markets. Minimum number of companies tells the amount of companies included to the data set in a month that the value was lowest for respective country. Maximum number of companies tells the amount of companies included to the data set in a month that the value was highest for respective country. Mean number of companies is the time series average of monthly number of companies for each country. Total number of companies is the number of unique companies in whole data set. Time series averages for monthly mean, median and total market values are also presented. Total market value is the sum of market values of respective country in each month. All marked values are converted to USD. Only companies in the final dataset are included in calculation of the figures. Micro stocks are excluded from the dataset.}
 \label{table:CountrySummary}
\centering
\newcolumntype{Y}{>{\centering\arraybackslash}X}
\begin{tabularx}{\textwidth}{@{\extracolsep{4pt}} X Y Y Y Y Y Y Y} 
\toprule
 & \multicolumn{4}{c}{Number of companies} & \multicolumn{3}{c}{Market value} \\
\cline{2-5} \cline{6-8} 
Market		& Min 	& Max 	& Mean  	& Total	& Mean 		& Median 	& Total \\
\midrule
Denmark	 	& 42		& 106 	& 70	 	& 235	& 2399.78 	& 810.32	& 141657.9  \\
Finland	 	& 26 		& 83	 	& 62		& 186 	& 1893.43 	& 634.47	& 124389.2 \\
Norway		& 44 		& 132 	& 79	 	& 408	& 1520.75	 	& 506.17  & 124689.5 \\
Sweden		& 45 		& 256 	& 132 	& 593	& 2115.65	 	& 616.59	& 308952.1  \\
\midrule
Nordic		& 200 	& 527 	& 343 	& 1422	& 1946.46 	& 583.02	& 699688.8  \\
\bottomrule
\end{tabularx}
\end{table} 

On average number of companies with large market capitalization is more limited in nordic countries than in US or Europe. The smallest companies can be numerous, but still only account for fraction of total market capitalization. Liquidity of these companies is often also quite low. To avoid results to be driven by such a stock, approach of Hanauer and Kalsbach \citeyear{HANAUER2023} applied for emerging markets and exclude companies with smallest market value that account 3\% of the aggregated market value. On the other hand we do not want few extremely large companies to drive the results either. Therefore, market value of the companies is winsorized monthly to 99\%. If company's market value is among 1\% biggest market values in corresponding month, the market value will be replaced by the 1\% threshold. \par

Table \ref{table:CountrySummary} presents the number of companies and their sizes in separate and pooled nordic markets after above described filters. The total number of non-micro cap stocks in the dataset is 1422 whereas the monthly number of stocks in the dataset is 343 on average. Figure \ref{plot:number_of_companies_wo_micro} shows the development of the non micro-cap company count that passed the static and dynamic screens screens over time. Figure \ref{plot:number_of_companies} in appendix shows the development of company counts over time including micro-cap stocks. Comparing the two figures shows that even though micro-cap stocks account only for 3\% of the aggregated market value, they account for remarkable share of company count. Maximum number of companies including micro-cap stocks exceeds 1000 whereas maximum number of companies excluding micro-cap stocks is slightly above 500. \par

Sweden is clearly the biggest of the four markets both in regards off the number of companies and total market value of the companies. Even though Sweden is the biggest market it is not dominating. On average Sweden accounts less than half of the total market value of the pooled Nordic market. In regards of average  and median market value, Denmark has the biggest companies. Finland on the other hand is clearly the smallest of the markets included in the study measured both in number of companies and market value of the companies. \par

\begin{figure}[ht]
\centering
\caption[Number of non micro-cap companies]{\textbf{Number of non micro-cap companies}\\ Figure shows the development of total number of securities considered in the dataset from 1990 to 2022 for each Nordic country. Figures counts all non micro-cap securities that passed the static screens.}
\input{R_graphs/number_of_companies_wo_micro.tex}
\label{plot:number_of_companies_wo_micro}
\end{figure}

In this study Nordic markets are examined as one market. In the introduction it was argued that in the eyes of foreign investors Nordic markets can appear quite homogenous. There is also more practical reason why Nordic markets are pooled in this study. Table \ref{table:CountrySummary} shows that individual Nordic stock markets hold limited amount of large market capitalization stocks. This leads to situation where the performance of the whole market or portfolios formed from the market can be driven by very few large market capitalization stocks even after winsorizing the market values. Later in the study we will allocate stocks to portfolios based on their expected returns and we want to ensure that there exist reasonable amount of companies to diversify each portfolio. Unfortunately, Nordic countries have different currencies. In order to ensure comparability of the companies from different countries, we have to convert certain variables to common currency which in this case is US dollar using historical currency rates. Variables that are converted to US dollars include for example return index and market capitalization. Majority of the variables are ratios which can be calculated from local currency values. \par

\subsection{Company characteristics}\label{CompanyCharacteristics}

Total 23 characteristics are derived from stock level data. All the models use same set of explanatory variables which includes book-to-market ratio (BEME), investment (INV), earnings-to-price ratio (EP), cash-to-total assets ratio (CA), capital turnover (CTO), cashflow-to-price ratio (CFP), leverage (DEBT), sales-to-price ratio (SP), return on assets (ROA), return on equity (ROE), Tobin's Q (Q), one month momentum (MOM$_2$), momentum from $t-12$ to $t-7$ month (MOM$_7$), momentum from $t-12$ to $t-2$ (MOM$_{12}$), momentum from $t-36$ to $t-12$ (MOM$_{36}$), industry momentum (MOM.IND), log scaled market capitalization (LOG.MV), standard deviation (SD), ratio of current price and 52 week high price (HIGH52), beta coefficient (BETA), idiosyncratic volatility (IDVOL), turnover (TO) and on balance volume (OBV). \par

Data consists of variables that are available on three different frequencies. Majority of these variables are ratios calculated from accounting data. 
Usually income statement or balance sheet information are available annually and therefore majority of accounting based variables are updated only once a year. To account for possible reporting delay associated with accounting data, accounting data from year $t$ is considered to be available end of June $t+1$. Detailed descriptions how each of these variables is calculated is provided in the table \ref{table:variableDefs}. Dataset contains also variables calculated from monthly data. These include momentum variables and the market value. Even though the frequency of the return prediction will be monthly, some variables are calculated from weekly data. These include standard deviation, ratio between price and 52-week high price, beta coefficient, idiosyncratic volatility, turnover and on-balance volume. Standard deviation is calculated as rolling 52-week standard deviation of the stock returns. \par

{\small {\setstretch{1.0}
\newcolumntype{Z}{>{\centering\arraybackslash}X}
\begin{xltabular}{\textwidth}{ >{\raggedright\arraybackslash}p{0.23\textwidth} >{\raggedright\arraybackslash}p{0.2\textwidth} X}
\caption[Variable definitions]{\textbf{Variable definitions} \\ Tables provides definitions and initial authors for all anomalies used in the study. Construction of variables follows mainly Green et. al \protect\citeyear{Green2017} and Hanauer and Kalsbach \protect\citeyear{HANAUER2023} and can deviate from variable definitions of initial authors. Table also provides the direct formulas and relevant Datastream items used to calculated the variables. Abbreviatons used to indicate different variables later in the study are also displayed in the table. MV$_{t-1, Dec}$ indicates market value as of end of December in year $t -1$. Frequency of the variable is indicated after the variable name.}\
\label{table:variableDefs} \\ \toprule
Variable & Author & Definition \\ \midrule
\endfirsthead 
\multicolumn{3}{@{}l}{\ldots\ \small Continued from the previous page}\\ \midrule
Variable & Author & Definition and affected Datastream items \\ \midrule
\endhead 
\midrule
\multicolumn{3}{r@{}}{\small Table continues in the next page \ldots}\\ 
\endfoot 
\endlastfoot
Cash-to-Assets \newline Yearly		& Palazzo \citeyear{PALAZZO2012162} 			& Cash-to-Asset ratio is calculated by dividing cash and short-term investments by total assets. \newline CA = WC02001$_{t}$ / WC02999$_{t}$\\ \rule{-1ex}{3ex}
Capital Turnover \newline Yearly	& Haugen and Baker \citeyear{HAUGEN1996}		& Capital turnover is calculated by dividing total sales by one year lagged total assets. \newline CTO = WC01001$_{t}$ / WC02999$_{t-1}$\\ \rule{-1ex}{3ex}
Investment  \newline Yearly		& Cooper, Gulen and Schill \citeyear{cooper2008}	& Investments are defined as a yearly change in total assets. \newline INV = (WC02999$_{t}$ - WC02999$_{t-12}$) / WC02999$_{t-12}$\\	\rule{-1ex}{3ex}
Book-to-Market Equity \newline Yearly & Davis, Fama and French \citeyear{Davis2000}	&  Book-to-Market value is calculated by dividing company's book value of equity by company's market capitaliztion end of previous year. Book value of equity is calculated by summing common equity and deferred taxes of the company. \newline BEME = (WC03501$_{t}$ + WC03263$_{t}$) /  MV$_{t-1, Dec}$\\ \rule{-1ex}{3ex}
Cash Flow-to-Price \newline Yearly	& Lakonishok, Shleifer and Vishny  \citeyear{Lakonishok1994} 	& Cash flow to price ratio is calculated by dividing company's cash flow from operating activities by the asset's market capitalization end of previous year. \newline CFP = WC04860$_{t}$ / MV$_{t-1, Dec}$\\ \rule{-1ex}{3ex}
Debt-to-Price \newline Yearly		& Hanauer and Kalsbach \citeyear{HANAUER2023}	& Debt-to-price value is calculated as difference between total assets and common equity divided by the asset's market capitalization end of previous year. \newline DEBT = (WC02999$_{t}$ - WC03501$_{t}$) / MV$_{t-1, Dec}$\\ \rule{-1ex}{3ex}
Sales-to-Price 	\newline Yearly		& Lewellen \citeyear{Lewellen2015}				& Sales-to-price ratio is calculated by dividing total sales by asset's market capitaliztion end of previous year. \newline WC01001$_{t}$ / MV$_{t-1, Dec}$ \\ \rule{-1ex}{3ex}
Earnings-to-Price \newline Yearly	& Basu \citeyear{Basu1977}					& Earnings-to-price ratio is calculated by dividing net income before extra Items and preferred dividends by asset's market capitaliztion end of previous year. \newline EP = WC01551$_{t}$ / MV$_{t-1, Dec}$ \\ \rule{-1ex}{3ex}
Return-on-Assets \newline Yearly	& Balakrishnan, Bartov and Faurel \citeyear{BALAKRISHNAN2010}	& Return-on-assets is calculated as net income before extra items and preferred dividends divided by one year lagged total assets. \newline ROA = WC01551$_{t}$ / WC02999$_{t-12}$ \\ \rule{-1ex}{3ex}
Return-on-Equity \newline Yearly	& Haugen and Baker \citeyear{HAUGEN1996}		& Return-on-equity is calculated as net income before extra Items and preferred dividends divided by one year lagged book value of equity. See book-to-market equity for definition of book value of equity. \newline ROE = WC01551$_{t}$ / BE$_{t-12}$ \\ \rule{-1ex}{3ex}
Tobin's Q 	\newline Yearly			& Freyberger, Neuhierl and Michael Weber \citeyear{	Freyberger2020} & Tobin's Q is calculated by summing up total assets and market capitalization from previous December, then subtracting cash and short-term investments and deferred taxes. Finally result is divided by the total assets. \newline Q = (WC02999$_{t}$ + MV$_{t-1, Dec}$ - WC02001$_{t}$ - WC03263$_{t}$) / WC02999$_{t}$ \\ \rule{-1ex}{3ex}
Momentum$_{7}$ \newline Monthly	& Novy-Marx \citeyear{NOVYMARX2012}			& MOM7 is defined as cumulative return in US dollars between $t-7$ and $t-12$ months.\\ \rule{-1ex}{3ex}
Momentum$_{12}$ \newline Monthly	& Jegadeesh and Titman \citeyear{Jegadeesh1993}	& MOM12 is defined as cumulative return in US dollars between $t-2$ and $t-12$ months.\\ \rule{-1ex}{3ex}
Momentum$_{36}$ \newline Monthly	& De Bondt and Thaler \citeyear{DeBondt1985}		& MOM36 is defined as cumulative return in US dollars between $t-12$ and $t-36$ months.\\ \rule{-1ex}{3ex}
Momentum$_{2}$ \newline Monthly	& Jegadeesh \citeyear{Jegadeesh1990}			& MOM2 is defined as prior month return in US dollars.\\ \rule{-1ex}{3ex}
Industry Momentum \newline Monthly & Moskowitz and Grinblatt \citeyear{Moskowitz1999}	 & MOM.IND is defined as 12 month cumulative equal weighted industry momentum. Industry is defined using INDG attribute from Datastream. \\ \rule{-1ex}{3ex}
Standard deviation \newline Monthly	& Ang, Hodrick, Xing and Zhang \citeyear{ang2006}	& L.SD is defined as standard deviation of unadjusted weekly price for last 52 weeks. \\ \rule{-1ex}{3ex}
52-week high price \newline Monthly	& George and Hwang \citeyear{george2004}	& Calculated from weekly unadjusted prices by dividing current price by past 52-week high price. \newline  L.HIGH52.RATIO = UP$_{t}$ / UP$_{52 week high}$\\ \rule{-1ex}{3ex}
Beta	 \newline Monthly			& Fama and MacBeth \citeyear{FamaMacBeth1973}	& L.BETA is estimated by beta coefficients obtained by regressing unadjusted weekly returns noted in US dollars with equally weighted market returns. Minimum 15 observations is required.\\ \rule{-1ex}{3ex}
Idiosyncratic volatility \newline Monthly & Ali, Hwang and Trombley \citeyear{ali2003}	& L.IDVOL is estimated by standard deviation of regression residuals from regressing unadjusted weekly US dollar returns by equally weighted market return. \\ \rule{-1ex}{3ex}
Log. market value \newline Monthly 	& Banz \citeyear{BANZ1981}				& Natural logarithm of the market value of the company end of previous month. \newline L.LOG.USD.MV = $\ln$(USD.MV$_{t-1}$)\\ \rule{-1ex}{3ex}
Turnover \newline Monthly 		& Datar, Naik and Radcliffe \citeyear{datar1998} & Turnover is defines as end of previous month weekly trading volume divided by the shares outstanding. \newline L.TO = VO$_t$ / NOSH$_t$ \\ \rule{-1ex}{3ex}
On-balance volume \newline Monthly & Tsang and Chong \citeyear{tsang2009}		& OBV is calculated with following process. First on-balance volume is the weekly trading volume multiplied by corresponding week return's sign. Following on-balance volumes are calculated by adding the product of trading volume and sign of return to previous on-balance volume.\\ 
\bottomrule
\end{xltabular}
}}

Set of explanatory variables includes seven value indicators. Book-to-market value is calculated by dividing sum of common equity and deferred taxes by the market value of last December. Four of the value indicators are price ratios. Income before extraordinary items, net cash flow from operating activities and net sales are divided by market capitalization of previous year December in order to obtain earnings-to-price, cashflow-to-price ratio and sales-to-price ratios correspondingly. Leverage is calculated by first subtracting common equity from total assets and the dividing by market capitalization of previous year December. Rest two of the value indicators are normalized by total assets. Cash-to-total assets is calculated by dividing cash and short-term investments by total assets and Tobin's Q is calculated by summing up total assets and market capitalization from previous December, then subtracting cash and short-term investments and deferred taxes and finally dividing by the total assets. \par

Profitability of the companies is described with three indicators. Return on assets and return on equity divide earnings before extraordinary items by lagged total assets and lagged book equity. As described above book equity is defined as the sum of common equity and deferred taxes. Third profitability indicator is capital turnover. Capital turnover is calculated by dividing net sales by total assets. Momentum characteristics are described by five different momentum variables. Momentum variables include traditional and intermediate momentum as well as short-term and long-term reversals. Traditional momentum is defined as cumulative return from $t-12$ to $t-2$ and intermediate as cumulative return from $t-12$ to $t-7$. Short-term reversal is the return of the previous month whereas long-term reversal is defined as cumulative return from $t-36$ to $t-12$. \par

\begin{table}[ht] 
\small
\caption[Descriptive statistics]{\textbf{Descriptive statistics}\\ Table provides time series average of cross-sectional means and standard deviations of all variables used in this study. Values are reported separately for pooled Nordic market and four Nordic markets Denmark, Finland, Norway and Sweden. EXC.RET is monthly excess return calculated from total return index. Risk free rate used to calculate excess returns is US dollars one-month Treasury bill rate.}
 \label{table:DescriptiveStatistics}
\centering
\begin{tabularx}{\textwidth}{@{\extracolsep{1pt}} X r r r r r r r r r r} 
\toprule
 & \multicolumn{2}{c}{Nordic} & \multicolumn{2}{c}{Denmark} & \multicolumn{2}{c}{Finland} & \multicolumn{2}{c}{Norway}&\multicolumn{2}{c}{Sweden} \\
\cline{2-3}\cline{4-5}\cline{6-7}\cline{8-9}\cline{10-11}
Variable 		& Mean 	& Std. 	& Mean 	& Std. 	& Mean 	& Std. 	& Mean 	& Std. 	& Mean 	& Std. \\
\midrule
EXC.RET		& 0.007 	& 0.100	& 0.007 	& 0.088	& 0.009	& 0.089	& 0.006	& 0.106	& 0.009	& 0.098 \\
CA		 	& 0.117 	& 0.148	& 0.107 	& 0.159	& 0.108	& 0.119	& 0.132	& 0.159	& 0.115	& 0.144 \\
CTO		 	& 0.813  	& 0.710 	& 0.739 	& 0.670	& 0.970	& 0.647	& 0.651	& 0.674	& 0.868	& 0.741 \\
INV 			& 0.161 	& 0.422	& 0.118 	& 0.305	& 0.097	& 0.273	& 0.203	& 0.481	& 0.179	& 0.443 \\
BEME	 	& 0.699 	& 0.709	& 0.688 	& 0.558	& 0.757	& 0.661	& 0.680	& 0.630	& 0.753	& 0.827 \\
CFP	 		& 0.080 	& 0.122	& 0.086 	& 0.140	& 0.090	& 0.099	& 0.093	& 0.145	& 0.065	& 0.102 \\
DEBT	 	& 2.574 	& 5.231	& 3.025 	& 5.249	& 2.415	& 4.646	& 3.330	& 6.456	& 2.353	& 4.443 \\
SP 			& 1.585	& 2.279	& 1.356 	& 1.636	& 2.066	& 2.105	& 1.316	& 1.719	& 1.940	& 2.849 \\
EP 			& 0.045	& 0.143	& 0.044 	& 0.114	& 0.051	& 0.106	& 0.026	& 0.163	& 0.058	& 0.160 \\
ROA 		& 0.045	& 0.103	& 0.043 	& 0.096	& 0.053	& 0.072	& 0.029	& 0.110	& 0.052	& 0.112 \\
ROE 		& 0.088 	& 0.224	& 0.095 	& 0.203	& 0.103	& 0.158	& 0.058	& 0.280	& 0.093	& 0.212 \\
Q			& 0.696 	& 0.326	& 0.597 	& 0.391	& 0.735	& 0.286	& 0.655	& 0.345	& 0.759 	& 0.273 \\
MOM$_{7}$ 	& 0.080 	& 0.229	& 0.069 	& 0.198	& 0.072	& 0.201	& 0.080	& 0.245	& 0.089	& 0.230 \\
MOM$_{12}$ 	& 0.171	& 0.382	& 0.149 	& 0.329	& 0.153	& 0.320	& 0.171	& 0.409	& 0.190	& 0.387 \\
MOM$_{36}$ 	& 0.397	& 0.751	& 0.400 	& 0.679	& 0.363	& 0.625	& 0.357	& 0.797	& 0.432	& 0.754 \\
MOM$_{2}$ 	& 0.015 	& 0.093	& 0.013 	& 0.081	& 0.014	& 0.085	& 0.015	& 0.097	& 0.017	& 0.093 \\ 
MOM.IND 	& 1.144 	& 0.284	& 1.132 	& 0.248	& 1.142	& 0.285	& 1.148	& 0.293	& 1.148	& 0.279 \\
SD	 		& 0.047	& 0.030	& 0.045 	& 0.028	& 0.042	& 0.028	& 0.051	& 0.030	& 0.051	& 0.029 \\
HIGH52		& 0.684 	& 0.284	& 0.722 	& 0.266	& 0.621	& 0.294	& 0.684	& 0.271	& 0.695	& 0.261 \\
BETA	 	& 0.863 	& 0.525	& 0.703 	& 0.391	& 0.732	& 0.495	& 0.909	& 0.530	& 0.999	& 0.509 \\
IDVOL	 	& 0.046 	& 0.030	& 0.044 	& 0.025	& 0.040	& 0.028	& 0.051	& 0.032	& 0.048	& 0.029 \\ 
LOG.MV	 	& 6.331  	& 1.340	& 6.400 	& 1.328	& 6.402	& 1.272	& 6.124	& 1.246	& 6.452	& 1.414 \\ 
TO		 	& 0.043  	& 0.109	& 0.053 	& 0.126	& 0.025	& 0.065	& 0.028	& 0.073	& 0.056	& 0.119 \\
OBV		 	& 0.170 	& 0.507	& 0.151 	& 0.544	& 0.098	& 0.390	& 0.174	& 0.529	& 0.216	& 0.524 \\ 
\bottomrule
\end{tabularx}
\end{table} 

% TradingFrictions: 6
% trading: 1
% Investment: 1 
% contains all dimension of fama french framework but extended

Trading frictions are estimated by six variables. Beta coefficient and idiosyncratic volatility are calculated by regressing returns of the stocks by the excess market return. As described above, in order to pool the dataset certain variables are converted to US dollars. One of these variables is weekly unadjusted stock price which is used to calculate the weekly stock returns used in the regression. Market return is constructed as equal weighted weekly market return following Green, Hand and Zhang \citeyear{Green2017}. Because the returns are noted in US dollars one-month Treasury bill rate, which is obtained from Kenneth French's database, is used as a risk free rate proxy. The regression is run for each company separately for each month on rolling basis. For each regression up to three years of weekly historical data is considered, but minimum 15 weeks of data is required. Finally, the beta is simply the sensitivity of stock returns on the market return changes and the idiosyncratic volatility is the standard deviation of the regression residuals. \par

In addition to beta coefficient and idiosyncratic volatility, trading frictions are also measured by turnover, standard deviation, market value and 52 week high price. Turnover is calculated by dividing weekly trading volume by number of shares out standing. Standard deviation is calculated from up to 52 weeks of weekly unadjusted price data. One month lagged logarithm of market value is used as a size indicator. 52 week high indicator is also calculated from weekly data and it is defined as ratio between highest unadjusted weekly price in last 52 weeks and current price. Investment characteristic of the companies is measured by the yearly growth rate of total assets. \par

This study introduces a new explanatory variables class to the cross-sectional stock return literature. On-balance volume which is traditionally used as technical trading indicator is included to explanatory variable set. On-balance volume is often used as time series predictor for individual securities, but objective of this study is to investigate whether it could contain information about cross-section of stock returns. Calculation of on-balance volume consists of two steps and it is also calculated from weekly data. First current weekly turnover is multiplied by the sign of corresponding week's return. Then this product is added to the cumulative sum of the historical on previous on-balance volumes. On-balance volume is defined as \par

\begin{equation}
\label{eq:OBV}
OBV_t = OBV_{t-1} + 
\begin{cases}
    \text{trading volume}, 	& \text{if $r_t$} \geq \text{0}\\
    \text{- trading volume},	& \text{if $r_t$ < 0}
\end{cases}
\end{equation}

where $OBV_t$ is the on-balance volume value at time $t$, $OBV_{t-1}$ is the on-balance volume value at $t-1$, trading volume is the weekly trading volume and $r_t$ is the return of the corresponding stock at time $t$. \par

As described above covariates include to this study can be clustered to value, trading friction, momentum, investment and profitability. Since trading frictions include size and beta coefficient, all dimensions of Fama and French framework are considered. For many of the dimensions lot of alternative indicators are also considered. Whereas traditional Fama and French \citeyear{FAMA19933} model only evaluates value characteristic of a company by the book-to-market value, this this study simultaneously considers six additional value indicators. \par

Machine learning algorithms can be sensitive to outliers in the data. Therefore, all explanatory variables are winsorized between 1st and 99th percentiles. In case value of the certain variable is less than 1st percentile of the corresponding months values it will be replaced by the 1st percentile threshold value. In case value of the certain variable is above 99th percentile of the corresponding months values it will be replaced by the 99th percentile threshold value. %check how the returns are winzorized
Additionally, any missing value in explanatory variables will be replaced by zero. \par

Table \ref{table:DescriptiveStatistics} provides descriptive statistics of company characteristics. For each explanatory variable time series average and standard deviation of the cross-sectional mean is reported. Values are reported for pooled nordic markets as well as individual markets. Table shows that mean monthly excess return of Nordic markets during the studied period was 0.7\%. Mean excess return of Sweden and Finland is slightly above that and mean excess return of Denmark and Norway is slightly below that. \par

%discuss this more in detail

%introduce the explanatory variables

% excess returns like in gu et al not like Hanauer. Because we use pooled data set and want to predict comparably. Hanauer uses country balanced portfolios

%time periods

\section{Methodology} \label{Methodology}

This section provides the theoretical framework of this study. Sections starts by describing how benchmark factors are calculated, which are later used in risk adjusted evaluation of machine learning portfolios. Next chapter provides the theoretical foundation of the three machine learning models. Following section describes how stock return predictions obtained from different models are evaluated. Section describes both prediction accuracy and economic profitability metrics used to evaluate the performance of the models. \par

Variable importance section presents the approach implemented in order to evaluate comparably between models the importance of different covariates to the predictive accuracy of the models. Final part of this section introduces the sample splitting scheme applied while training the machine learning models. It also describes the hyperparameters considered as well as if they are subject to optimization. \par

\subsection{Benchmark factors}\label{BenchmarkFactors}

Benchmark factor construction follows $2 \times 3$ portfolio sort approach of Fama and French \citeyear{FAMA19933, FAMA20151} and Carhart \citeyear{Carhart1997}. Fama and French \citeyear{FAMA19933} use NYSE breakpoints for size and book-to-market value sort. Since on compared to US markets Nordic markets have less companies with high market value. Using NYSE breakpoints could lead to highly un-diversified portfolios especially among the high market value portfolios. On the other hand breakpoints should not be driven by the small stocks that are numerous, but only account for small part of the total market capitalization. Therefore approach of Fama and French \citeyear{FAMA2012457} is applied.  \par

In the end of each June stocks are first distributed to two size portfolios. Companies with biggest market value that account for $90\%$ total market value are classified as big stocks. All the rest of the stocks are considered to be small stocks. Next stocks are allocated to three value, investment, profitability and momentum portfolios. For all of above variables 30th and 70th percentiles are used to calculate breakpoints. Breakpoints are calculated using only big companies from the size allocation, but the breakpoints are used to allocate all stocks to a portfolio. \par

\begin{equation} \label{eq:FF6factors}
\begin{split}
SMB_{B/M} = & \ \frac{1}{3} (Small.High + Small.Neutral + Small.Low) \\
			& - \frac{1}{3} (Big.High + Big.Neutral + Big.Low) \\[5pt]
SMB_{OP} = & \ \frac{1}{3} (Small.Robust + Small.Neutral_{OP} + Small.Weak)\\
			& - \frac{1}{3} (Big.Robust + Big.Neutral_{OP} + Big.Weak)\\[5pt]
SMB_{INV} = & \ \frac{1}{3} (Small.Conservative + Small.Neutral_{INV} + Small.Aggressive)\\
			& - \frac{1}{3} (Big.Conservative + Big.Neutral_{INV} + Big.Aggressive)\\[5pt]
SMB_{MOM} = & \ \frac{1}{3} ((Small.Winner + Small.Neutral_{MOM} + Small.Loser)\\
		     	& - \frac{1}{3} (Big.Winner + Big.Neutral_{MOM} + Big.Loser)\\[5pt]
SMB = & \ \frac{1}{4} (SMB_{B/M} + SMB_{OP} + SMB_{INV} + SMB_{MOM})\\[20pt]
HML = & \ \frac{1}{2} (Small.High + Big.High) - \frac{1}{2} (Small.Low + Big.Low)\\[5pt]
RMW = & \ \frac{1}{2} (Small.Robust + Big.Robust) - \frac{1}{2} (Small.Weak + Big.Weak)\\[5pt]
CMA = & \ \frac{1}{2} (Small.Conservative + Big.Conservative)\\
		& - \frac{1}{2} (Small.Aggressive + Big.Aggressive)\\[5pt]
MOM = & \ \frac{1}{2} (Small.Winner + Big.Winner)\\
		& - \frac{1}{2} (Small.Loser + Big.Loser)\\
\end{split}
\end{equation}

Book-to-market value is used as indicator of value characteristic of a company. Book-to-market value is calculated as ratio between sum of common equity and deferred taxes and market capitalization on December $t-1$. Profitability is defined as net income before extra items/preferred dividends divided by the book equity of the company. Investment variable is calculated as annual change in total assets. Momentum is defined as cumulated return from $t-12$ to $t-2$. Returns are calculated using total return index that is converted to US dollars for comparability between different countries. Market value used in size allocation as well as to weight portfolio returns is also converted to US dollars. \par

Equation \ref{eq:FF6factors} shows formula for each factor. Acronym for each variable is derived from how they are calculated. Value factor is called high minus low (HML), profitability factors is called robust minus weak (RMW), investment factor is called conservative minus aggressive (CMA). Only momentum factor is exception to this rule and more intuitive naming is used. Portfolio allocation results six portfolios for value, investment, profitability and momentum factors and 24 two-fold size portfolios. After portfolio construction portfolio returns are calculated as difference on value weighted average returns on portfolios formed based on respective variable. E.g. value factor return is difference between average of value weighted returns of two high book-to-market portfolios and average of value weighted returns of two low book-to-market portfolios. Market factor is the average value weighted excess return of the whole market. Risk free rate is obtained from Kenneth French's website. \par

\subsection{Linear regression}\label{LinearRegression}
Benchmark model of this study is Fama-MacBeth \citeyear{FamaMacBeth1973} regression. First step of the method is to run rolling cross-sectional regressions with lagged variables. Second step of the method calculates means of the factor loadings obtained from the cross-sectional regressions. Finally expected stock return can be obtained by multiplying the mean factors loading with latest available stock characteristics. Below formulas show one model specification for Fama-French \citeyear{FAMA19933} three factors model. \par

%review this
\begin{equation}
f(x_{i, t}; \theta) = \theta^T x_{i, t}
\end{equation}

\begin{equation}
\overline \theta_j = \frac{1}{T} \sum^{T}_{t=1}\theta_{j, t}
\end{equation}

\begin{equation}
E_t \left[ r_{i, t} | x_{i, t-1} \right] = \overline \theta_{t-1}^T x_{i, t-1}
\end{equation}

One benefit that linear regression models have is that they do not require hyperparameter tuning. Therefore data  does not have to be split to three sub-samples for separate validation of hyperparameters and testing. To obtain the expected return mean of 120 historical regression coefficients is calculated. Due to their high computing cost machine learning models are usually trained only once a year and then used for the rest of the year. Each month recent information is just inserted to the model. Computing requirements for linear model is far lesser than for non-linear models. Nevertheless to ensure comparability between different models also the linear model is trained only once per year. That means that no more recent stock returns than $t-1$ are used to train the model to predict stock return $t$, but the gap between predicted return and last return used to train the model can grow up to 12 months. Since we use lagged variables, this means that for prediction of stock return $t$ we alway use stock characteristics from $t-1$, but some factors are only updated yearly. To mimic information set investor would have had available in historical periods we have to account for the delay in reporting balance sheet information. Therefore timeline of Fama and French \citeyear{FAMA19933} is followed and models are trained each year at end of June. \par

\subsection{Random forest}

Decision trees are one example of nonparametric machine learning algorithms. Idea of the decision trees is to split data into the most homogenous groups. Decision trees can be used for both classification and regression tasks. Starting point of the decision tree is called a root node. At each iteration of decision tree algorithm finds the optimal threshold to split the data to the nodes to minimize the objective function value. Then iteratively these nodes can be further split and the tree grows. This process is repeated until predefined tree size, set by the user, is reached or objective function cannot be improved anymore. Regression tree nodes that are not further split are called leaves. Final prediction of the regression tree leaf is the average of the dependant variable values of training set observations inside it. Gu et al. \citeyear{guetal} formulate prediction of a regression tree with $K$ leaves as \par

\begin{equation}
f(x_{i, t}; \theta, K, L) = \sum_{k=1}^K \theta_k 1 _{\{x_{i, t} \in C_K(L)\}}
\end{equation}

Where $C_k(L)$ represents one of the $K$ splits the tree consists of. $L$ is the indicator of the depth of the leaf. $\theta_k$ indicates average return within leaf $k$ and $1 _{\{x_{i, t} \in C_K(L)\}}$ indicates whether observation $x_{i, t}$ belongs to leaf $k$. Since observation can only belong to one leaf, partition $C_K(L)$ is the product of the above partitions. \par

\begin{figure}[ht]
\centering
\caption[Illustrative regression tree]{\textbf{Illustrative regression tree}\\ Tree is trained from the actual dataset for 30th of July 2004 and then pruned to show only few most important leaves. Figure serves only illustrative purposes and random forest models used in the study do not necessarily contain identical trees. }
\input{R_graphs/regr_tree.tex}
\label{plot:regre_tree}
\end{figure}

Advantage of the regression trees is that they are rather simple and intuitive, but still they are able to model even complex interactions and non-linear relationships among the predictors. One common problem with regression trees is that they easily overfit the data and would require heavy regularization. Random forest models aims to avoid this problem by deriving the predictions from ensemble of regression trees. As the name might suggest random forest consists of multiple decision trees. \par

Idea of the random forest is to randomly generate set of decision trees and then use the average outcome of the decision trees as the final output. This way the model is less likely to overfit the data. Nevertheless to avoid the overfitting trees inside random forest should not be too correlated and this is ensured including randomness in the construction of the decision trees. Randomness in the generation of the decision trees is applied by restricting the set of observations used in the training of the model. Number of the variables model considers in each split as well as maximum depth of the decision tree and number of trees in the random forest can also be limited. Setting these parameters correctly is a crucial part of the training. These are the hyperparameters which require input from the user, but which also can be optimized for different tasks. Table \ref{table:Hyperparameters} in Appendix shows which values were considered for each hyperparameter that were optimized for random forest. \par

\subsection{Neural networks}

Artificial neural networks are powerful machine learning method category. Currently neural networks are popular approach to many real world prediction issues. Due to their strong performance in multiple domains, neural networks are often considered as state of the art machine learning method. Despite their popularity, for many users neural networks are sort of black box tools because of their complexity. Compared to linear regression and tree based models, neural networks are far less interpretable. Another weakness of the neural networks is that they are highly parameterized and highly sensitive for parameter initialization. Some of the parameter, such as learning rate, are usually optimized during the training of the model while others such as architecture of the model are usually fixed.  \par

One of the first things user has to decide while training a neural network is the architecture of the model. This study focuses on feedforward neural networks which consists of input layer, hidden layers and an output layer. Input layer consists of the predictive variables whereas output layer produces the final predicition. In between thee exist 1 to N hidden layers. Hidden layers again consists of  so called neurons. Similar to number of hidden layers, user has to also decide number of neurons in each of the hidden layers. Number of the hidden layers is often referred as the deepness of the model where as number of the neurons in each hidden layer is referred as the width of the model. While lot of previous literature simultaneously examine multiple different architectural forms, due to computing capacity in this study only one architecture will be examined \cite{guetal, HANAUER2023, TOBEK2021100588}. Neural network of this study has two hidden layers. First hidden layer has 16 neurons and following common geometric pyramid rule second hidden layer has 8 neurons. Rather shallow and narrow architecture is chosen because they usually perform better with smaller datasets \cite{guetal}. In order to improve and fasten the converging of the model batch normalization is implemented between all layers. \par

\begin{figure}
\centering
\caption[Illustrative neural network]{\textbf{Illustrative neural network}\\  Figure serves only for illustrative purposes. Sole purpose of the figure is to visualize structure of a neural network. Weights and biases of the neural network are obtained by training a model from the dataset of this study. Nevertheless, this model is not used in stock return predictions. Neural networks used in predictions contain more nodes in hidden layers, but narrower architecture was chosen for better visualization.}
\input{R_graphs/NN.tex}
\label{plot:NN}
\end{figure}

Idea of the neural network is that each neuron, using weights and biases terms, aggregates information from previous layer and subsequently feeds the information to the activations function. Neural network model used in this study is fully connected, meaning that each neuron is connected to all neurons in previous layer. Output of the activation function will be the input for the next layer. Neural network model is trained by optimizing these weights and biases terms. There exists many options for the activation function, which is again one choice user has to make. Activation function used in this study is rectified linear unit \par

\begin{equation}
\label{ReLU}
ReLU(x) = max(0, x)
\end{equation}

Since model is trained for a regression task final neuron in the output layer has different activation function than the neurons in the hidden layers. Activation function for the output neuron is linear function. \par

As mentioned neural networks include numerous hyperparameters that can be optimized during the training of the model. Training neural network is computationally demanding. Due to limited computing capacity hyperparameters are not optimized in this study, but predefined values are used. Hyperparameters and their values are presented in table \ref{table:Hyperparameters}. Additionally, to further limit the computational demand and simultaneously avoid overfitting early stopping algorithm is applied. Early stopping is implemented so that training of the model is terminated after five epochs where the loss function value does not reduce for validation set. Instead of inserting whole dataset to the model at once data is inserted to the model in smaller subsamples so called batches. Epoch on the other hand measures how many times the whole dataset is run through the model. \par

Neural networks learn by adjusting weights to the direction of gradient. This is done in repetitive iterations. In each iteration size of the change is defined by hyperparameter called learning rate. Since learning rate is a hyperparameter it needs an input from the user. It can also be optimized. Setting correct learning rate is crucial, since too big learning rate might prevent algorithm from converging to optimal solution, but too small learning rate makes converging slow. For above described reasons learning rate is not optimized in this study, but using learning rate scheduler it is adjusted during the training of the model. In order to ensure efficient training learning rate is set 0.01 in the beginning of the training and after ten epochs learning rate will be multiplied by 0.9. \par

Neural networks are also sensitive to the weight initialization, where the initial weights are set which the model starts to optimize. Depending on the initialization of the weights neural networks can converge to different results. To reduce model variance caused by this, an ensemble method is applied. Ensemble is implemented by training five separate models with different initial weights. Final prediction will be then average of the predictions of the five models. \par

\subsection{Prediction performance evaluation}\label{PredictionPerformanceEvaluation}

This study will evaluate machine learning methods from two perspectives. First perspective that is evaluated is the profitability of the methods. Profitability is estimated via portfolio construction following Lewellen \citeyear{Lewellen2015}. First expected returns are derived from each model. This process is introduced in more detail in following subsections. After obtaining the expected returns, each month all stocks are distributed to ten portfolios based on the magnitude of the expected return. Allocation is univariate and does not consider other variables than the expected return of the stock for the next month. Even though models are trained only once a year, expected returns are re-calculated every month as the most recent available data is inserted to the model. Therefore, also the portfolio allocation is repeated monthly. Each month all stocks are allocated to one of the ten expected return portfolio, but to avoid result to be mainly driven by small stocks approach from Hanauer and Kalsbach is applied \citeyear{HANAUER2023}  and the breakpoints for the allocation are calculated only from the large market value stocks. Large market value stocks are the biggest stocks that account for 97\% of the aggregated total market value of respective month. \par

In addition to the ten expected return portfolios, for each method zero investment portfolio is formed. Zero investment or long-short portfolio is simply the spread between return of the highest expected return portfolio and return of the lowest expected return portfolio. Both value and equal weighted returns will be reported for each portfolio. Performance of the machine learning portfolios is backtested and evaluated in multiple ways. For all expected return portfolios historical realized mean returns are reported together with Sharpe ratios. For long-short portfolios also maximum drawdown and maximum 1 month loss will be reported. Maximum 1 month loss is simply the largest negative monthly return of each portfolio. Maximum drawdown is define as \par

\begin{equation}
\label{eq:maxDD}
MaxDD = \max\limits_{0\leq t_1\leq t_2\leq T} (Y_{t_1} - Y_{t_2})
\end{equation}

where $Y_t$ stands for cumulative return from the beginning of the period until date $t$. Turnover will be calculated only for long side of the long-short portfolios. In order to examine risk adjusted returns long-short portfolio returns will be regressed against Fama-French \citeyear{FAMA20151} six factor model factors \footnotemark. From this regression alphas, which can interpret as the excess return that the models are able to generate that cannot be explained by the loadings in the six risk factors. Also $t$-statistics for the alphas and $R^2$ values are reported. Regression formula for risk adjusted performance \par

\begin{equation}
\label{eq:FFRegFormula}
\begin{split}
\hat r_{i, t} = 	& \ \alpha+ \beta_{1} \ RMRF{t} + \beta_{2} + \ SMB_{t} + \beta_{3} \ HML_{t} + \\
		&  \beta_{4} \ CMA_{t} +  \beta_{5} \ RMW_{t} + \beta_{6} \ MOM_{t} + \epsilon_{t}
\end{split}
\end{equation}

where $RMRF$ is the excess market return, $SMB$ is the spread in the return between small market value stocks and large market value stocks, $HML$ is the spread in the return between high book-to-market value stocks and low book-to-market value stocks, $CMA$ is the spread in the return between conservatively investing stocks and aggressively investing stocks, $RMW$ is the spread in the return between stocks with robust profitability and stocks with weak profitability and $MOM$ is the between returns of stocks that had highest return in period $t-1$ and the stocks that had lowest return in period $t-1$. Factors are constructed from the same dataset as machine learning portfolios, except that the micro-cap stocks are not excluded. Construction of these factors is described in more detail in Appendix. \par

\footnotetext{Fama and French \citeyear{FAMA20151} introduced the five factor model. Factors used to regress machine learning portfolio returns include five factor model factors and momentum factor from Carhart \citeyear{Carhart1997}.}

Machine learning models are also evaluated based on their prediction accuracy. Prediction accuracy will be evaluated using out-of-sample $R^{2}_{oos}$ and Diebold-Mariano tests. Two out-of-sample $R^{2}$ figures are presented. Traditional out-of-sample $R^{2}$ uses historical mean  return as the benchmark estimation. Traditional out-of-sample $R^{2}$ is defined as \par

\begin{equation}
\label{eq:r2Trad}
R^{2}_{oos \ Trad.} = 1 - \frac{\sum^T_{t=1} \sum^N_{i=1} (r_{i, t} - \hat r_{i, t})^2}{ \sum^T_{t=1} \sum^N_{i=1} (r_{i, t} - \overline{r}_{i, t} )^2}
\end{equation}

where $r_{i, t}$ is the realized return of stock $i$ in month $t$, $\hat r_{i, t}$ is the predicted return of the same stock for month $i$ and $\overline{r}_{i, t}$ is the historical mean return of the same stock excluding month $i$. Nevertheless, Gu et al. \citeyear{guetal} argue that the historical mean return is so noisy estimator that it underperforms compared to static estimation of zero and therefore artificially improves the out-of-sample $R^{2}$. Instead they propose alternative out-of-sample $R^{2}$ measure where the squared sum of returns in denominator is not demeaned. \par

\begin{equation}
\label{eq:r2}
R^{2}_{oos} = 1 - \frac{\sum^T_{t=1} \sum^N_{i=1} (r_{i, t} - \hat r_{i, t})^2}{ \sum^T_{t=1} \sum^N_{i=1} r^2_{i, t}}
\end{equation}

$R^2$ presents the prediction accuracy as a single figure, whereas Diebold-Mariano allows for pairwise comparison of different models. Diebold-Mariano value is calculated as \par

\begin{equation}
\label{eq:Diebold-Mariano}
\begin{split}
 d_{12, t} 			& = \ \frac{1}{N_{t}}  \sum^N_{i=1}((r_{i, t} - \hat r_{i, t, 1})^2 - (r_{i, t} - \hat r_{i, t, 2})^2) \\
\overline{d}_{12} 	& = \ \frac{1}{T} \sum^T_{t=1} d_{12, t} \\
DM_{12} 			& = \ \frac{\overline{d}_{12}}{\hat \sigma_{d_{12}}} \\
\end{split}
\end{equation}

where $\hat r_{i, t, 1}$ is the return prediction of first model for company $i$ at time $t$ and $\hat r_{i, t, 1}$ is the return prediction of the second model  for company $i$ at time $t$. $N_t$ is number of observations in prediction period $t$. Therefore, $d_{12, t}$ is a time series of differences in average squared prediction errors between model $1$ and model $2$. $\overline{d}_{12}$ is the mean of $d_{12, t}$ and $\hat \sigma_{d_{12}}$ is the Newey and West (source) standard error of $d_{12, t}$. Diebold-Mariano test allows us to estimate statistical significance of the prediction accuracy of two models. Under assumption that the there does not exists difference in prediction accuracy between models Diebold-Mariano test statistic follows norma distribution with mean of $0$ and standard deviation of $1$, $DM \sim \mathcal{N}(0,\, 1)$. The significance of the difference is reported both for traditional $5\%$ level as well as for 3-way comparisons with Bonferroni adjustment. \par

In the spirit of Lewellen \citeyear{Lewellen2015} expected returns are also estimated by regressing realized returns with the expected returns. This regression follows \par

\begin{equation}
\label{eq:realizedRegression}
r_{i, t} = \alpha + \beta_1 \hat r_{i, t}
\end{equation}

where $r_{i, t}$ is the realized return of company $i$ at time $t$ and $\hat r_{i, t}$ is the expected return of corresponding model for company $i$ at time $t$. For these regressions betas, $t$-statistics for betas and $R^2$ values will be reported. Ideally the beta coefficient or the slope for the predicted return should be 1 and highly significant. Magnitude of the beta coefficients can provide information of possible over or undershooting of the models. \par

\subsection{Variable importance} \label{VariableImportance}

One challenge in dealing with various statistical methods is that they lack common metrics for explanatory inference. Many of the models have metrics for variable importance, but comparability of these metrics can be questioned. Therefore, approach of Gu et al. \citeyear{guetal} is implemented to define variable importance metrics for model applied in this study. Approach consists of following steps. First one variable at a time is set to zero. Then the reduced model is retrained and new predicted returns are derived using the reduced model. Process of training and predicting returns is identical to the reduced model as for the full model. After obtaining the predicted returns from reduced model, out-of-sample $R^2$ values are calculated for these returns. Then change compared to out-of-sample $R^2$ of full model is calculated. Finally, to obtain relative variable importance metric sum of changes in out-of-sample $R^2$s is normalized to one within model. Same process is applied to each variable and all models. \par

\subsection{Sample splitting} \label{SampleSplitting}

It is common while training machine learning models to split the data to three sets. Training set will be used as the name suggest to train the model. In case machine learning model includes hyperparameters these can be optimized with validation set. Finally the true out-of-sample predictions can be performed for testing data. Because we want to mimic situation and information set of an investor we have to take into a consideration time series nature of the data. \par

In stock return prediction literature it is common to split the data as described above, but considering the chronological order. For example Fieberg et al. \citeyear{Fieberg} use rolling 10 year rolling scheme where they first train model using the first seven years of the data and then optimize hyperparameters using last three years of the rolling window. Finally they train the model with optimal hyperparameter initalization with whole ten year window to predict returns for the next year. Gu et al. \citeyear{guetal} use slightly different approach. Instead of using rolling window they increase the training window size after each training period by one year. Common for these two approaches is that they both train the model only once a year. \par

\definecolor{Gray}{RGB}{160,160,160}
\definecolor{LightGray}{RGB}{191,191,191}
\begin{figure}[h]
\centering
\label{plot:SampleSplittingScheme}
\caption[Sample splitting scheme]{\textbf{Sample splitting}\\ Illustration sample splitting used in training of the machine learning models. Machine learning models are trained once a year and after each training model is used for next 12 months to predict the stock returns. Each year training period is extended by 12 months. Training period is further split into training data and validation data randomly allocating 80\% of the data to training set and 20\% of the data to the validation set. Validation set is used to optimize hyperparameters. Minimum length of the training period is 50 months.}
\begin{tikzpicture}
\draw[->] (0, 0) -- (12.8, 0);
\foreach \x in {0, 1.6,...,12.8}{
    \draw (\x cm, 3pt) -- (\x cm, 0pt);
} 
\fill[LightGray] (0, 0.8) rectangle (8, 0.6);
\fill[Gray] (8, 0.8) rectangle (9.6, 0.6);
\fill[LightGray] (0, 0.45) rectangle (9.6, 0.25);
\fill[Gray] (9.6, 0.45) rectangle (11.2, 0.25);

\draw [thick, decorate, decoration = {brace, amplitude = 5pt}] (0, 0.8)  -- +(8, 0) 
       node [black, midway, above = 4pt] {Training period};
\draw [thick, decorate, decoration = {brace, amplitude = 5pt}] (8, 0.8)  -- +(1.6, 0) 
       node [black, midway, above = 4pt] {Prediction period};
       
\node[anchor=north] at (8, 0) {$t$};
\node[anchor=north] at (9.6,0) {$t+12$};
\node[anchor=north] at (11.2, 0) {$t+24$};
\end{tikzpicture}
\label{plot:Turnover}
\end{figure}

The sample splitting scheme applied in this study is slightly different from above described ones. Above approaches use disjoint time periods to mimic the out-of-sample setting in the hyperparameter optimization. In this study training and validation set are separated from the testing data based on time. Approach is closer to approach of Gu et al. \citeyear{guetal} in a sense that training data window is increased each year. Nevertheless, the difference is that the data is distributed to training data and validation randomly instead of using the disjoint periods. Reason why this scheme is chosen is that we want to avoid the retraining of the model after the hyperparameter optimization which is necessary if the most recent data should be included to the model. Sample splitting scheme is illustrated in figure \ref{plot:SampleSplittingScheme}. \par

\begin{table}[h]
\small
\caption[Hyperparameters]{\textbf{Hyperparameters}\\ Table presents the hyperparameters that are either optimized or taken as fixed values. In case predefined values are used only one figure is indicated in the table. If hyperparameter is optimized set or list is displayed. FM stands for linear regression model, RF stand for random forest model and NN stands for neural networks model.}
\label{table:Hyperparameters}
\centering
\newcolumntype{Y}{>{\centering\arraybackslash}X}
\begin{tabularx}{\textwidth}{@{\extracolsep{4pt}} X Y Y Y} 
\toprule
& FM & RF & NN \\
\midrule
\multirow[t]{5}{*}{Hyperparameter} & \multirow[t]{5}{*}{-} & ntree$ \ =  300$ & Learning rate$ \ = 0.001$  \\
			&	& mtry$ \ = \ \in (2, 3, 5, 7)$ 		& Batch size$ \ = 502$ \\
			&	& max.depth$ \ = 2 \sim 6$ 		& Epochs$ \ = 100$ \\
			&	& sample.fraction$ \ = 0.5$ 		& Patience$\  = 5$\\
			&	& 							& Ensemble$ \ = 5$\\
\bottomrule
\end{tabularx}
\end{table}

Since linear regression does not require any hyperparameter optimization there is no need for validation set and all data can be used to train the model. For random forest model we actually optimize the hyperparameters. Therefore, training window is split to training and validation data so that $80\%$ of the data is used in the training and $20\%$ is assigned to the validation set. For neural network model we do not optimize any actual hyperparameters, but we still need a validation set for the early stopping algorithm. Therefore, for neural network $15\%$ of the training window is assigned to the validation set.  Approach of this study follows the common approach to only train the models once a year. \par

\section{Empirical results on Nordic equities}

Performance of the machine learning models will be evaluated from two aspects. First profitability of the models will be evaluated by investigating performance of univariate expected return sort portfolios. Construction of these portfolios is explained in more detail in section \ref{Methodology}. Second aspect that is examined is the prediction accuracy of the machine learning models. Prediction accuracy is evaluated by out-out sample $R^2$. Additionally, in the spirit of Lewellen \citeyear{Lewellen2015} relationship between expected and realized excess returns are examined by regressing the realized excess returns with the individual stock level predicted returns. Finally, the variable importance for different methods are calculated to see effect of each explanatory variable to the prediction accuracy of the model. Process to define variable importance is described in section \ref{VariableImportance}. \par

\subsection{Benchmark factor performance}\label{BenchmarkFactorPerformance}

This section works as prequisite for the machine learning portfolios as it shows the performance of Fama and French five factor \citeyear{FAMA20151} model factors extended by the momentum factor.  Later these factors are used to evaluate the risk adjusted performance of the machine learning portfolios, but prior to that it is interesting to observe whether simpler factor construction shows persistence in Nordic markets. \par

\begin{table}[h]
\small
\caption[Benchmark factor summary statistics]{\textbf{Benchmark factor summary statistics} \\ Table presents the mean returns and standard deviations of the benchmark factors together with two-sided t-statistics and corresponding p-values. For each factor minimum and maximum monthly return is reported.  RMRF is the average value weighted excess return of the pooled Nordic market. Portfolio returns are calculated based on $2 \times 3$ sorts on size and one other factor. HML is the difference in average of value weighted return of two high value portfolios and average of value weighted return of two low value portfolios. RMW, CMA and MOM are calculated in similar manner, but portfolio sort are done based on investment, profitability momentum factors. SMB is the average of the value weighted returns of the 12 portfolios of small stocks minus the average of the value weighted returns of the 12 portfolios of big stocks. Returns are calculated in US dollars.}
\label{table:variableFFfactors}
\centering
\newcolumntype{Y}{>{\centering\arraybackslash}X}
\begin{tabularx}{\textwidth}{@{\extracolsep{4pt}} X Y Y Y Y Y Y} 
\toprule
& Mean & Std. & $t$-stat. & p-value & Min & Max \\
\midrule
HML & 0.0014 & 0.0022 & 0.6299 & 0.5291 & -0.2662 & 0.2508 \\
RMW & 0.0013 & 0.0015 & 0.8711 & 0.3842 & -0.1251 & 0.1640 \\
CMA & 0.0014 & 0.0015 & 0.9102 & 0.3633 & -0.1077 & 0.1704 \\
MOM & 0.0090 & 0.0021 & 4.2990 & 0.0000 & -0.1501 & 0.1828 \\
SMB & -0.0001 & 0.0014 & -0.1091 & 0.9132 & -0.1204 & 0.1042 \\
RMRF & 0.0074 & 0.0032 & 2.3051 & 0.0217 & -0.2576 & 0.2072 \\
\bottomrule
\end{tabularx}
\end{table}

Table \ref{table:variableFFfactors} provides time series average factor returns, standard deviation of the factor returns, corresponding $t$ and p-values as well as monthly minimum and maximum returns for all six factors. It is clear that momentum factor shows strongest performance measured both by the magnitude of the return as well as the statistical significance. Monthly momentum factor return is 0.9\% with $t$-statistic of 4.3. Table \ref{table:FFfactorsCorrelations} in appendix shows the correlations between factor returns. In Nordic markets correlation of momentum factor with other factors is only minor. Interestingly in Nordic markets momentum factor seems to negatively correlate with market factor. \par

Strong performance of the momentum factor is inline with previous literature. Many previous studies have documented excess momentum returns either in pooled or individual Nordic markets (e.g. Grobys and Huhta-Halkola \citeyear{grobys} and Leivo and Pätäri \citeyear{leivo2011}). Slightly more surprising is the poor performance of the value factor. Average return of the value factor is 0.14\% and it is not statistically significant. Some of the previous studies document value premium in Nordic markets. Grobys and Huhta-Halkola \citeyear{grobys} find statistically significant value premium in Nordic markets, but Grobys and Huhta-Halkola construct equal weighted portfolios where as benchmark factors reported here are formed from value weighted portfolios. \par
% check if huhta-halkola reports value weughted factors

\begin{figure}[h]
\centering
\caption[Benchmark factor performance]{\textbf{Benchmark factor performance}\\ Plot presents the cumulative return of the benchmark factors. RMRF is average value weighted excess return of pooled Nordic market. Portfolio returns are calculated based on $2 \times 3$ sorts on size and one other factor. HML is the difference in average of value weighted return of two high value portfolios and average of value weighted return of two low value portfolios. RMW, CMA and MOM are calculated in similar manner, but portfolio sort are done based on investment, profitability momentum factors. SMB is the average of the value weighted returns of the 12 portfolios of small stocks minus the average of the value weighted returns of the 12 portfolios of big stocks. Returns are calculated in US dollars.}
\input{R_graphs/factor_performance.tex}
\label{plot:factor_performance}
\end{figure}

Figure \ref{plot:factor_performance} shows the historical performance of the benchmark factors. It shows that until 2008 even momentum factor could not exceed the market return, but still seemed to be less volatile. In post financial crisis period momentum factor has been clearly the strongest performing factor. Interestingly late nineties, which was strong period for market return, was extremely difficult period for value factor. Contrary in the early 2000's value factor performed strongly when market factor was decreasing steeply. Until 2003 performance of value factor is almost opposite to the market factor. Figure \ref{plot:factor_performance} also shows that the performance of the size, investment and profitability factors has been poor through out the examined period. \par

\subsection{Predictive performance}\label{PredictivePerformance}

Panel A of table \ref{table:PredictionAccuracy} presents the out-of-sample predictive accuracies for all models. In panel B it presents the pairwise Diebold-Mariano statistics. It can clearly be seen from table \ref{table:PredictionAccuracy} that random forest model produces the most accurate out-of-sample predictions. Out of the three models it is the only one that produces positive out-of-sample $R^2$ value even with the more strict version where the benchmark model is prediction of zero. While linear and neural network models both produce negative out-of-sample $R^2$ of -0.0191 and -0.0039, it can be seen that neural network model performs better than the linear model. \par

Another interesting insight table \ref{table:PredictionAccuracy} provides is the relationship between traditional out-of-sample $R^2$ and the modified out-of-sample $R^2$. It confirms the hypothesis of Gu et al. \citeyear{guetal} about traditional out-of-sample $R^2$ metric being too loose and showing unrealistically strong results. The  traditional out-of-sample $R^2$ metric is positive for all three models, which means that compared to the more strict out-of-sample $R^2$ metric the sign of the metric changes for linear and neural network models. Nevertheless, the order between models does not change based on the definition of the out-of-sample $R^2$ metric. In this regards results are in line with findings of Fieberg et al. \citeyear{Fieberg}. \par

\begin{table}[h]
\small
\caption[Prediction accuracy]{\textbf{Prediction accuracy}\\ Table presents the prediction accuracy metrics for different machine learning models. Panel A presents two out-of-sample $R^2$ values. First one uses zero prediction as a benchmark model. This means that the denominator in the calculation of the metric is squared excess return. Second out-of-sample $R^2$ figure follows the traditional definition and the realized excess return is demeaned by the historical mean return. Panel B of table presents the pairwise Diebold-Mariano statistics for all the methods. Bolded figure indicated significance at 5\% level, whereas asterisk indicates significance at 5\% level after three-way Bonferroni adjustment. FM stands for linear regression model, RF stand for random forest model and NN stands for neural networks model.}
\label{table:PredictionAccuracy}
\centering
\newcolumntype{Y}{>{\centering\arraybackslash}X}
\begin{tabularx}{\textwidth}{@{\extracolsep{4pt}} X Y Y Y} 
\toprule
\multicolumn{4}{l}{\textit{Panel A: Out-of-sample $R^2$}}\\
\midrule
& FM & RF & NN \\
\midrule
$R^2_{oos}$ & -0.0191 & 0.0051 & -0.0039\\
$R^2_{oos \ Trad.}$  & 0.0299 & 0.0517 & 0.0431\\
\midrule
\multicolumn{4}{l}{\textit{Panel B: Diebold-Mariano statistics}}\\
\midrule
& FM & RF & NN \\
\midrule
FM 	& 	& \textbf{8.4462*} (0.0000)& \textbf{2.1495} (0.0316)\\
RF	& 	& 					& \textbf{-10.0946*} (0.0000)\\
\bottomrule
\end{tabularx}
\end{table}

%mentions also in the table that diebold-mariano statistics are calculated using newey west standard errors

Figure \ref{plot:OOSR2_ts} in Appendix shows the out-of-sample $R^2$ values as a time series of prediction periods. Figure shows that the same trends can be seen from all of the methods. It also reveals that in prediction period starting from July 2011 neural network model produced extremely bad out-of-sample predictions. This period is difficult to other methods as well, but not in the same scale as for neural network model. Modified out-of-sample $R^2$ of neural network model reaches -0.14\% during this period. \par

Inspecting the time series of the out-of-sample $R^2$ produced by the two definitions further the argumentation that the traditional out-of-sample $R^2$ is too optimistic metric to evaluate goodness of the stock return prediction model. Figure \ref{plot:OOSR2_ts} shows how the traditional out-of-sample $R^2$ are not only sifted upwards, but also the variation is smaller. This supports the argument of Gu et al. \citeyear{guetal} that the historical mean as an estimator of future stock return contains so much noise that it actually artificially improves the out-of-sample $R^2$ values. \par

Results of the predictive performace of different model measured by out-of-sample $R^2$ is partially inline with previous literature. Results are inline with findings of Drobetz and Otto's \citeyear{Drobetz} study in European stock markets in a sense that the linear model offers worst out of sample preditcive power when measured by the out-of-sample $R^2$. They also find negative out-of-sample $R^2$ for linear model. Results are also inline in a sense that random forest model shows strong predictive performance. Results of Fieberg et al. \citeyear{Fieberg} are slightly more contradictory, since they show positive out-of-sample $R^2$ also for linear model \footnotemark. Both studies of Drobetz and Otto and Fieberg et al. are conducted in European stock markets, which partially overlap with markets of this study, but the difference is that Drobetz and Otto use twenty-two characteristics as well as their  second- and third order polynomials and two-way interactions whereas Fieberg et al. only use six characteristics. Variable selection of this study is in between of these two since we include more variables than Fieberg et al., but we do not include second- and third order polynomials or two-way interactions like Drobetz and Otto. \par

\footnotetext{Fieberg et al. \citeyear{Fieberg} report result for multiple subsets where companies are filtered based on their market capitaalization. Linear model produces negative out-of-sample $R^2$ values when only biggest 20\% of the stocks are included, but this is not the setting of this study.}

Where the results clearly deviation from previous literature is the predictive performance of the neural network model. In studies of Drobetz and Otto \citeyear{Drobetz} and Fieberg et al. \citeyear{Fieberg} neural network model produces highest, clearly positive, out-of-sample $R^2$ values. Naturally studies of Drobetz and Otto and Fieberg et al. are not directly comparable to this study since variable set differs and the datasets of  Drobetz and Otto and Fieberg et al. are much wider since they include more countries. The size of the dataset could also partially explain the relative poor performance of the neural network model, since usually neural network models require lot of data. \par

Panel B of table \ref{table:PredictionAccuracy} presents the pairwise Diebold-Mariano statistics for all the models. Calculation of the statistics is described in section \ref{Methodology}. Table \ref{table:PredictionAccuracy} reports the Diebold-Mariano statistics together with corresponding p-values. Bolding of the Diebold-Mariano figure imply significance in normal 5\% level where as asterisk implies more conservative 5\% level which is Bonferroni adjusted for three-way comparisons. The three-way Bonferroni adjusted critical one-sided Diebold-Mariano value is 2.13. \par

Discuss the results.

Next the prediction accuracy is examined by following approach of Lewellen \citeyear{Lewellen2015} by regressing the realized excess returns by the return predictions from different models. Table \ref{table:expRetRegressions} presents the summary statistics for these regressions. Left side of the table presents the univariate properties of expected returns and the right side of the table presents the regression statistics. Comparing univariate properties of the expected returns from table \ref{table:expRetRegressions} to descriptive statistics in table \ref{table:DescriptiveStatistics} shows that the mean expected return is really close to actual realized mean excess return for neural network and random forest model. Both mean expected and realized return are calculated as time series average of cross-sectional means. Linear model on the other hand seems to predict larger returns on average than what is actually realized. Another remark from the table \ref{table:expRetRegressions} is that the standard deviation for the return predictions produced by the linear model and neural network model is higher than the standard deviation of the realized excess returns. This indicates that the variation in expected returns from these models is larger than the variation in realized excess returns. \par

\begin{table}[h]
\small
\caption[Expected return regression summaries]{\textbf{Expected return regression summaries} \\ Table provides univariate properties of the return predictions for all models and summary statistics for regression where realized excess returns are regressed with expected returns. Mean and standard deviation are reported for expected returns. Mean value reported is the time series average of the cross-sectional means and standard deviation is the time series average of cross-sectional standard deviations. Right side of the table reports the regression coefficients, standard errors of the coefficients, corresponding $t$-statistics and the $R^2$ values. FM stands for linear regression model, RF stand for random forest model and NN stands for neural networks model. }
\label{table:expRetRegressions}
\centering
\newcolumntype{Y}{>{\centering\arraybackslash}X}
\begin{tabularx}{\textwidth}{@{\extracolsep{4pt}} X Y Y Y Y Y Y} 
\toprule
& \multicolumn{2}{c}{Univariate properties} & \multicolumn{4}{c}{Predictive ability}\\
\cline{2-3}\cline{4-7}
& Mean & Std. & Slope. & SE & $t$-stat & $R^2$ \\
\midrule
FM & 0.0124 & 0.0123 & 0.1638 & 0.0217 & 7.324 & 0.0005 \\
RF & 0.0074 & 0.0093 & 0.5767 & 0.0306 & 18.831 & 0.0029 \\
NN & 0.0070 & 0.0131 & 0.2949 & 0.0210 & 14.05 & 0.0016 \\
\bottomrule
\end{tabularx}
\end{table}

Results from right side of the table \ref{table:expRetRegressions} support the remarks from out-of-sample predictive performance and univariate properties. For all of the models there exists statistically highly significant positive relationship between expected returns and realized returns. If the expected returns would reflect realized excess returns perfectly regression slope shown in table \ref{table:expRetRegressions} should be one. Random forest model has the highest predictive slope of 0.58, which means that 1\% change in expected return respond to 0.58\% change in realized return. Neural network and linear models have slightly smaller slopes of 0.29 and 0.16 correspondingly. $R^2$ values from the regressions are also presented as third alternative out-of-sample prediction accuracy metric in addition to two previously introduced out-of-sample $R^2$ metrics. The third $R^2$ metric further confirms the message of first two as the ordering of the methods is the same for the their metric as for the two previous. \par

Given that the mean predicted return matches quite well mean realized excess return, but the standard deviation is higher and the slope is slower, it seems that models seem to overshoot in their predictions. It seems that, especially neural network and random forest models, are able to predict the returns correctly on average, but exaggerate the extreme returns. This could at least partially explain rather low out-of-sample $R^2$ values discovered in table \ref{table:PredictionAccuracy}. Further insight for this will be provided in next section where performance of expected return sorted portfolios is examined. \par

\subsection{Portfolio performance}\label{Portfolio performance}

This section focuses on backtesting the machine learning portfolios, which are formed based on the expected returns produced by different models. Idea is to mimic of information set of a historical investor. Section describes the historical realized return for an investment strategy build on the machine learning models. Section mainly focuses on evaluating performance of the expected return portfolios. Formation of expected return portfolios is described in section \ref{Methodology}. Results are discussed separately for value and equal weighted portfolios. \par

\begin{table}[p]
\small
\caption[Machine learning portfolio performance]{\textbf{Machine learning portfolio performance} \\ Table reports performance metrics for portfolios formed based on univariate expected return sort. Each month all stocks are allocated to ten portfolios based on their expected returns. Breakpoints for the allocation are calculated only from big stocks, which are the biggest stocks that in current month account for 90 percent of cumulative market value of all stocks in the dataset. H-L is zero investment portfolio which consist of short position in portfolio formed from stocks with lowest expected return and long position in portfolio formed from stocks with highest expected return. Time series average of predicted return and realized return of each portfolio is reported for each portfolio together with standard error of realized return. Additionally, Sharpe ratios are reported. Left side of the table reports result for equally weighted portfolios and right side reports results for portfolios where each stock in portfolio is weighted by its lagged market value.}
\label{table:PortfolioPerformance}
\centering
\newcolumntype{Y}{>{\centering\arraybackslash}X}
\begin{tabularx}{\textwidth}{@{\extracolsep{1pt}} l c c c c c c Y Y Y Y} 
\toprule
\multicolumn{11}{l}{\textit{Panel A: Linear regression}}\\
\midrule
& \multicolumn{5}{c}{Equal weighted} & \multicolumn{5}{c}{Value weighted}\\
\cline{2-6}\cline{7-11}
			& Pred. 	& Avg. 	& Std. 	& $t$-stat	 & SR 	& Pred. 	& Avg. 	& Std. 	& $t$-stat	& SR \\
\midrule
Low			& -0.0028 	& 0.0049 	& 0.0773	& 1.1621	& 0.0630 	& -0.0016 & 0.0081	& 0.0796	& 1.8704	& 0.1014\\
2			& 0.0048 	& 0.0035 	& 0.0668 	& 0.9552	& 0.0518 	& 0.0048 	& 0.0063	& 0.0666	& 1.7495	& 0.0949\\
3		 	& 0.0078 	& 0.0051 	& 0.0645	& 1.4703	& 0.0797 	& 0.0078 	& 0.0052	& 0.0638	& 1.4971	& 0.0812\\
4 			& 0.0098 	& 0.0083 	& 0.0603 	& 2.5384	& 0.1377 	& 0.0098 	& 0.0089	& 0.0614	& 2.6690	& 0.1447\\
5 			& 0.0115 	& 0.0097 	& 0.0640 	& 2.7957	& 0.1516 	& 0.0115 	& 0.0080	& 0.0662	& 2.2228	& 0.1205\\
6			& 0.0115 	& 0.0088 	& 0.0609 	& 2.6740	& 0.1450 	& 0.0131 	& 0.0080	& 0.0640	& 2.2946	& 0.1244\\
7			& 0.0148 	& 0.0086 	& 0.0619 	& 2.5691	& 0.1393 	& 0.0148 	& 0.0080	& 0.0635	& 2.3258	& 0.1261\\
8			& 0.0170 	& 0.0094 	& 0.0623 	& 2.7933	& 0.1515 	& 0.0170 	& 0.0077	& 0.0646	& 2.1977	& 0.1192\\
9			& 0.0202 	& 0.0130 	& 0.0636	& 3.7658	& 0.2042 	& 0.0203 	& 0.0110	& 0.0632	& 3.2209	& 0.1747\\
High			& 0.0328 	& 0.0173 	& 0.0682 	& 4.6742	& 0.2535 	& 0.0346 	& 0.0127	& 0.0687	& 3.4027	& 0.1845\\
H-L			& 0.0356 	& 0.0124 	& 0.0495 	& 4.6193	& 0.2505 	& 0.0362 	& 0.0046	& 0.0584	& 1.4510	& 0.0787\\
\midrule
\multicolumn{11}{l}{\textit{Panel B: Random forest}}\\
\midrule
& \multicolumn{5}{c}{Equal weighted} & \multicolumn{5}{c}{Value weighted}\\
\cline{2-6}\cline{7-11}
			& Pred. 	& Avg. 	& Std. 	& $t$-stat	 & SR 	& Pred. 	& Avg. 	& Std. 	& $t$-stat	& SR \\
\midrule
Low			&  -0.0061	& 0.0011	 & 0.0750	& 0.2680	 & 0.0146	 & -0.0049	& 0.0038	& 0.0762	& 0.9213	& 0.0500 \\
2			& 0.0010	& 0.0036	 & 0.0643 	& 1.0384	 & 0.0563	 & 0.0010	& 0.0019	& 0.0688	& 0.5202	& 0.0282\\
3		 	& 0.0035	& 0.0073	 & 0.0633	& 2.1190	 & 0.1149	 & 0.0036	& 0.0077	& 0.0665	& 2.1398	& 0.1160\\
4 			& 0.0057	& 0.0095	 & 0.0622 	& 2.8198	 & 0.1529	 & 0.0058	& 0.0113	& 0.0635	& 3.2695	& 0.1773\\
5 			& 0.0078	& 0.0090	 & 0.0601 	& 2.7572	 & 0.1495	 & 0.0078	& 0.0076	& 0.0637	& 2.1903	& 0.1188\\
6			& 0.0097	& 0.0090	 & 0.0587 	& 2.8400	 & 0.1540	 & 0.0097	& 0.0065	& 0.0626	& 1.9227	& 0.1043\\
7			& 0.0115	& 0.0098	 & 0.0644 	& 2.7914	 & 0.1514	 & 0.0115	& 0.0089	& 0.0677	& 2.4314	& 0.1319\\
8			& 0.0133	& 0.0121	 & 0.0618 	& 3.6032	 & 0.1954	 & 0.0133	& 0.0096	& 0.0658	& 2.6900	& 0.1459\\
9			& 0.0155 	& 0.0137	 & 0.0606 	& 4.1542	 & 0.2253	 & 0.0155	& 0.0112	& 0.0647	& 3.1953	& 0.1733\\
High			& 0.0230 	& 0.0170	 & 0.0701 	& 4.4627	 & 0.2420	 & 0.0213	& 0.0133	& 0.0744	& 3.3056	& 0.1793\\
H-L			& 0.0291	& 0.0159	 & 0.0396 	& 7.3965	 & 0.4011	 & 0.0262	& 0.0095	& 0.0515	& 3.4125	& 0.1851\\
\midrule
\multicolumn{11}{l}{\textit{Panel C: Neural network}}\\
\midrule
& \multicolumn{5}{c}{Equal weighted} & \multicolumn{5}{c}{Value weighted}\\
\cline{2-6}\cline{7-11}
			& Pred. 	& Avg. 	 & Std. 	& $t$-stat	 & SR 	& Pred. 	& Avg. 	& Std. 	& $t$-stat	& SR \\
\midrule
Low			& -0.0149 	& 0.0010	 & 0.0797 	& 0.2241	 & 0.0122	 & -0.0135	& 0.0044	& 0.0844	& 0.9699	& 0.0526\\
2			& -0.0018	& 0.0032	 & 0.0665 	& 0.8949	 & 0.0485	 & -0.0018	& 0.0033	& 0.0705	& 0.8635	& 0.0468\\
3		 	& 0.0018	& 0.0065	 & 0.0602 	& 1.9795	 & 0.1074	 & 0.0018	& 0.0064	& 0.0645	& 1.8393	& 0.0997\\
4 			& 0.0045  	& 0.0089	 & 0.0607 	& 2.6986	 & 0.1464	 & 0.0045	& 0.0084	& 0.0687	& 2.2606	& 0.1226\\
5 			& 0.0068	& 0.0078	 & 0.0624 	& 2.3127	 & 0.1254	 & 0.0068	& 0.0071	& 0.0647	& 2.0264	& 0.1099\\
6			& 0.0089 	& 0.0097	 & 0.0584  & 3.0771	 & 0.1669	 & 0.0089 	& 0.0087	& 0.0614	& 2.6226	& 0.1422\\
7			& 0.0111 	& 0.0097	 & 0.0603 	& 2.9531	 & 0.1602	 & 0.0111	& 0.0092	& 0.0625	& 2.7144	& 0.1472\\
8			& 0.0135 	& 0.0114	 & 0.0609 	& 3.4533	 & 0.1873	 & 0.0135	& 0.0114	& 0.0593	& 3.5454	& 0.1923\\
9			& 0.0169 	& 0.0123	 & 0.0640 	& 3.5319	 & 0.1915	 & 0.0168	& 0.0121	& 0.0669	& 3.3223	& 0.1802\\
High			& 0.0256 	& 0.0165	 & 0.0695 	& 4.3830	 & 0.2377	 & 0.0248	& 0.0123	& 0.0742	& 3.0669	& 0.1663\\
H-L			& 0.0405	& 0.0156	 & 0.0417 	& 6.8742	 & 0.3728	 & 0.0377	& 0.0079	& 0.0565	& 2.5785	& 0.1398\\
\bottomrule
\end{tabularx}
\end{table}

Table \ref{table:PortfolioPerformance} presents the performance statistics for all ten expected return portfolios for all three models. Average predicted return, average realized excess return, standard deviation of the realized excess return, corresponding $t$-statistic and Sharpe ratio is reported for each portfolio. Left side of the table presents the values for equal weighted portfolios, whereas right of the table presents the values for the value weighted portfolios. Panel A of the table shows results for linear regression model, panel B shows the results for random forest model and panel C shows the results for neural network model. Numbers on the left side of the table indicate the decile of the expected return of the corresponding portfolio. H-L is a portfolio formed from short position on lowest expected return portfolio and long position in highest expected return portfolio. \par

Looking at the equal weighted part of the table \ref{table:PortfolioPerformance} message is quite clear. Even though out-of-sample $R^2$ remained rather low, especially for linear and neural network model, examined variable set seems to contain information about cross-section of stock returns. For all models there is clear rising trend of realized excess returns together across expected return portfolios. Linear model has couple of outliers where the return of lower expected return portfolio actually exceeds the return of higher expected return portfolio. Random forest and neural network models both only have one such an outlier. The spread of average realized excess return between minimum expected return portfolio and maximum expected return portfolio is more than one percent for all models.  \par

Models struggle more with value weighted portfolios. Especially linear and random forest models fail to create the increasing trend of realized returns among the expected return portfolio. Nevertheless, for both models on average five smallest expected return portfolios generate lower realized returns than five largest expected return portfolios. For both models portfolio with highest expected return also has the highest realized return. Slightly surprisingly neural network model perform relatively strongly also among value weighted portfolios. Even though there exists some outliers trend of increasing realized returns is much more clear for neural network model than other two models. Also for neural network model highest expected return portfolio produces highest realized return, but also five smallest portfolios have on average smaller realized excess return than average market return from table \ref{table:DescriptiveStatistics}. On the other hand five portfolios with highest expected return have clearly higher realized return than the average market return. \par

\begin{figure}[h]
\centering
\caption[Cumulative return of equal weighted machine learning portfolios]{\textbf{Cumulative return of equal weighted machine learning portfolios}\\ Figure plots the realized historical cumulative excess return of the out-of-sample predictions. Figure shows performance of portfolios that are formed allocating all except micro-cap stocks to ten portfolios based on their expected returns. Re-allocation is done monthly. Section \ref{Methodology} describes how expected returns are derived for different models. FM stands for linear regression model, RF stands for random forest model and NN stands for neural network model. Solid line plots the cumulative performance of the highest expected return portfolio whereas dashed line plots the cumulative excess return for the lowest expected return portfolio. All portfolios are equal weighted. Solid black line shows the value weighted marked return. All returns are converted to US dollars.}
\input{R_graphs/cumul_ew_portf_return.tex}
\label{plot:cumul_ew_portf_return}
\end{figure}

It is quite expectable that predictability of the value weighted portfolios is lower than the equal weighted portfolios. One reason for this is that is that stocks are divided to ten expected return portfolios.  From table \ref{table:CountrySummary} it can be seen that on average month dataset contains approximately 340 stocks. This would mean that even if stocks were allocated to portfolios evenly each portfolio would on average contain 34 stocks. This can be considered sufficient diversification for equal weighted portfolio. Nevertheless, typical to stock markets also Nordic stock markets have few extremely large market capitalization companies. Performance of these companies can even after winsorizing the market value drive the performance of the whole portfolio if number of stock inside the portfolio is limited. \par

Additionally, it is not guaranteed that each expected return portfolio would consists of same amount of stocks. This is because breakpoint expected returns for the portfolio allocation are calculated from expected returns of big market value stocks. Distribution of the expected returns of the small market value companies does not necessarily follow the expected return distribution of the large market value companies, which could lead to unbalanced portfolios. This can further  lower the diversification of the portfolios. One alternative to ensure diversification of the machine learning portfolios would be to allocate stocks to only five expected return portfolios instead of ten. \par

\begin{figure}[ht]
\centering
\caption[Cumulative return of value weighted machine learning portfolios]{\textbf{Cumulative return of value weighted machine learning portfolios}\\ Figure plots the realized historical cumulative excess return of the out-of-sample predictions. Figure shows performance of portfolios that are formed allocating all except micro-cap stocks to ten portfolios based on their expected returns. Re-allocation is done monthly. Section \ref{Methodology} describes how expected returns are derived for different models. FM stands for linear regression model, RF stands for random forest model and NN stands for neural network model. Solid line plots the cumulative performance of the highest expected return portfolio whereas dashed line plots the cumulative excess return for the lowest expected return portfolio. All portfolios are value weighted. Solid black line shows the value weighted marked return. All returns are converted to US dollars.}
\input{R_graphs/cumul_vw_portf_return.tex}
\label{plot:cumul_vw_portf_return}
\end{figure}

Another interesting remark from table \ref{table:PortfolioPerformance} is that standard deviation of the realized excess portfolio returns doesn't increase together average realized returns. For this reason Sharpe ratios increase together with expected returns. For all models and also for both equal and value weighted portfolios portfolio with highest expected return has also the highest Sharpe ratio if high-low portfolios are not considered. Since the volatility of the returns does not increase together with magnitude of the returns, it means that machine learning models are able to generate without simply investing simply more volatile stocks. Naturally correlation of the prices of the stocks inside the portfolio also affects the volatility of the portfolio returns, but this is the first indication of risk adjusted performance of the machine learning portfolios. Risk adjusted performance is discussed in more detail for high-low portfolios later. \par

Results from \ref{table:PortfolioPerformance} further support the findings of section \ref{PredictivePerformance} that models overshoot in their predictions. Table shows that average predicted returns for middle expected returns portfolios are close to mean return from \ref{table:DescriptiveStatistics}. Realized excess return of the middle predicted return portfolios also lands close to the markets mean return. This is true especially for random forest and neural network models. On the other hand expected returns for minimum and maximum expected return portfolios are rather extreme. For example for neural network model spread between average expected returns of the two extreme portfolios is more than four percent. \par

Given that there is clear trend of increasing realized excess returns among the predicted returns while the expected and realized return of the middle predicted return portfolios match mean return of the market, it seems that the models do pretty good job on allocating companies to return clusters, but produce too extreme predictions for lowest and highest predicted return portfolios. On average models are able to find which stocks produce the highest returns, but are too optimistic in their predictions. There seems to be similar situation with lowest expected return portfolios as well. Realized average excess return of the lowest expected return portfolios from neural network model are clearly below average market return, but still positive. As the average predicted return for these portfolios is below minus one percent, the spread between realized and expected return is rather large. This phenomena could at least partially explain the low out-of-sample $R^2$ values seen in section \ref{PredictivePerformance}. \par  

Figures \ref{plot:cumul_ew_portf_return} and \ref{plot:cumul_vw_portf_return} show the historical cumulative return of the highest and lowest expected return portfolios for all models for equal value weighted portfolios correspondingly. Solid line shows the cumulative excess return for the highest expected return portfolio, whereas dashed line indicates the cumulative excess return of the lowest expected return portfolio for each model. Figures are in line with the results from the table \ref{table:PortfolioPerformance}. Overall market trends can be seen from all portfolios, but there is clear spread between low and high expected return portfolios. Compared to table \ref{table:PortfolioPerformance} figures \ref{plot:cumul_ew_portf_return} and \ref{plot:cumul_vw_portf_return} provide the time series dimension to the numbers. For equal weighted portfolios figure \ref{plot:cumul_ew_portf_return} reveals rather constant spread between high and low expected return portfolio, which results divergent cumulative returns. Despite the average realized return being positive for lowest expected return portfolios for random forest and neural network models, the cumulative return of these models is negative. \par

Same overall market trends can be seen from figure \ref{plot:cumul_vw_portf_return} for value weighted portfolios. Compared to equal weighted portfolios, value weighted portfolios show more seasonality. Especially value weighted high expected return portfolios from neural network model seems to be sensitive to overall market distress. Both in early 2000's and during the financial crisis value of this portfolio decreases close to the market return. Among market value weighted portfolios both performance of the highest expected return portfolio as well as performance of the lowest expected return portfolio are more modest than among equal weighted portfolios. Even for neural network and random forest model cumulative return of the lowest expected return portfolio is positive. \par

Next part of the paper focuses on evaluating the performance of the long-short portfolios. Table \ref{table:PortfolioPerformanceMetrics} reports set of performance metrics for both equal and value weighted portfolios. Riskiness of the portfolios are evaluated by maximum drawdown and maximum one month loss. Additionally, risk adjusted performance is evaluated by regressing realized returns of each of the portfolios by the benchmark factors. From these regressions alphas, $t$-statistics for alphas and $R^2$ values are reported. Also Sharpe values are reported. Finally, table reports the turnover for long side of the long-short portfolios. \par

% cumulative performance -> are able to smoothen overall market changes

\begin{table}[ht]
\small
\caption[Zero investment portfolio performance metrics]{\textbf{Zero investment portfolio performance metrics} \\ Table shows different performance metrics for the spread in realized excess return of highest and lowest expected return portfolios for each model. Left side of the table shows the results for equal weighted portfolios and right side for market value weighted. Loss metrics reported in the table include maximum drawdown and maximum one month loss. Table also reports risk adjusted performance metrics. These include excess return that can not be explained by regressing realized returns of the portfolios by benchmark factors indicated by alpha. Additionally $t$-statistic for the alpha and $R^2$ values are reported. Table also shows Sharpe ratios for each of the long-short portfolios. Last row of the table shows the turnovers of long side of the long-short portfolios. FM stands for linear regression model, RF stand for random forest model and NN stands for neural networks model}
\label{table:PortfolioPerformanceMetrics}
\centering
\newcolumntype{Y}{>{\centering\arraybackslash}X}
\begin{tabularx}{\textwidth}{@{\extracolsep{4pt}} l Y Y Y Y Y Y} 
\toprule
& \multicolumn{3}{c}{Equal weighted} & \multicolumn{3}{c}{Value weighted} \\
\cline{2-4}\cline{5-7}\cline{6-7}
					& FM 	& RF 	& NN 	& FM 	& RF 	& NN \\
\midrule
Max DD(\%) 			& -0.3804	& -0.2708 	& -0.2627	& -0.6062 	& -0.5462 	& -0.4795\\
Max 1month Loss(\%) 	& -0.1744	& -0.1883	& -0.1926	& -0.3374 	& -0.3542 & -0.3003\\
FF Alpha 				& 0.0034 & 0.0136 & 0.0110 & -0.0048 & 0.0052 & 0.0030\\
t-stats 				& 1.7530 & 6.8282 & 4.8320 & -1.9326  & 1.8517 & 1.0487\\
$R^2$ 				& 0.5482 & 0.2638 & 0.2063 & 0.4547 & 0.1772 & 0.2523\\
Sharpe ratio			& 0.2505 & 0.4305 & 0.3287 & 0.0787 & 0.1894 & 0.1429\\
Turnover (\%)		 	& 0.1089 & 0.1140 & 0.1258 & 0.0995 & 0.1492 & 0.1417\\
\bottomrule
\end{tabularx}
\end{table}

\begin{figure}[ht]
\centering
\caption[Cumulative return of equal weighted zero investment portfolios]{\textbf{Cumulative return of equal weighted zero investment portfolios}\\ Figure presents the realized cumulative spread return between highest expected return portfolio and lowest expected return portfolio. Re-allocation of stocks to portfolios is done monthly. Section \ref{Methodology} describes how expected returns are derived for different models. Both high and low expected return portfolios are equal weighted. FM stands for linear regression model, RF stands for random forest model and NN stands for neural network model. Solid black line shows the value weighted marked return. All returns are converted to US dollars.}
\input{R_graphs/cumul_ew_LS_return.tex}
\label{plot:cumul_ew_LS_portf_return}
\end{figure}

\begin{figure}[ht]
\centering
\caption[Cumulative return of value weighted zero investment portfolios]{\textbf{Cumulative return of value weighted zero investment portfolios}\\ Figure presents the realized cumulative spread return between highest expected return portfolio and lowest expected return portfolio. Re-allocation of stocks to portfolios is done monthly. Section \ref{Methodology} describes how expected returns are derived for different models. Both high and low expected return portfolios are value weighted. FM stands for linear regression model, RF stands for random forest model and NN stands for neural network model. Solid black line shows the value weighted marked return. All returns are converted to US dollars.}
\input{R_graphs/cumul_vw_LS_return.tex}
\label{plot:cumul_vw_LS_portf_return}
\end{figure}

\subsection{Variable importance}\label{VariableImportance}

\begin{figure}[ht]
\centering
\caption[Variable importance]{\textbf{Variable importance}\\ Figure plots the importance of the explanatory variables to the predictive performance of the three machine learning models. Variable importance is defined as reduction in out-of-sample $R^2$ when corresponding variable is replaced by zero before the training process. Definition of the out-of-sample $R^2$ is described in section \ref{Methodology}. Darker colour indicates higher variable importance. FM stands for linear regression model, RF stand for random forest model and NN stands for neural networks model.}
\input{R_graphs/combined_VI.tex}
\label{plot:combined_VI}
\end{figure}

\begin{figure}[ht]
\centering
\caption[Relative variable importance]{\textbf{Relative variable importance}\\ Figure plots the relative importance of the explanatory variables to the predictive performance of the three machine learning models. Variable importance is defined as reduction in out-of-sample $R^2$ when corresponding variable is replaced by zero before the training process. Definition of the out-of-sample $R^2$ is described in section \ref{Methodology}. In order to obtain relative variable importance measures, reductions in out-of-sample $R^2$ compared to full model are normalized to sum to one within one model. FM stands for linear regression model, RF stand for random forest model and NN stands for neural networks model.}
\input{R_graphs/relative_VI.tex}
\label{plot:relative_VI}
\end{figure}

\section{Conclusion}\label{Conclusion}
More trading variables.

\clearpage

\appendix
\section{Data collection}
\renewcommand{\thefigure}{A.\arabic{figure}}
\setcounter{figure}{0}
\renewcommand{\thetable}{A.\arabic{table}}
\setcounter{table}{0}

\begin{table}[ht] 
\small
\caption[Constituent lists and keywords]{\textbf{Constituent lists and keywords}\\ Table provides the constituent lists used in data collection.}
 \label{table:constituteLists}
\centering
\begin{tabularx}{\textwidth}{X X X X}
\toprule
Denmark & Finland & Norway & Sweden \\
\midrule
FDEN 		&  FFIN		& FNOR		& FSWD\\
WSCOPEDK & WSCOPEFN & WSCOPENW& WSCOPESD\\
DEADDK 	&   DEADFN 	& DEADNW 	& DEADSD\\
& & & FAKTSWD\\
 \bottomrule
 \end{tabularx}
 \end{table} 
 
\begin{table}[ht] 
\small
\caption[Dynamic screens]{\textbf{Dynamic screens}\\ Table provides the dynamics screens used in the data cleaning.}
 \label{table:DynamicScreens}
\centering
\begin{tabularx}{\textwidth}{l X}
\toprule
Attribute & Applied screen \\
\midrule
RI 		& Observations where one month return is larger than 990\% are removed.\\
RI		& Observation is removed if return in $r_t$ or $r_{t-1}$ exceed 300\% and $(1+r_{t})(1+r_{t-1}) -1$ is less than 0.5. \\
RI 		& For periods after the delisting of a security Datastream returns last available value. Therefore, by removing all consecutive zero returns at the end of the dataset for all securities.\\
 \bottomrule
 \end{tabularx}
 \end{table} 

\begin{table}[ht]
\small
\caption[Static screens]{\textbf{Static screens}\\ It also provides the country specific keywords that are used to deleted entries from the dataset. Panel B provides keywords that were used to delete entries from each market separately. Keyword deletion follows Ince and Porter \protect\citeyear{Ince2006} and Hanauer and Windmüller \protect\citeyear{HANAUER2023106712}. Same logic is applied to remove both country specific and generic keywords. Keyword is searched from Datastream attributes NAME, ENAME and ECNAME. In case if at least one of these attributes contains the keyword security is deleted from the dataset. To avoid deleting proper entries, security is only deleted if keyword occurs at the beginning of the name, at the end of the name or as separate word in the name.}
\label{table:StaticScreens}
\begin{tabularx}{\textwidth}{X X X X l}
\toprule
\multicolumn{5}{l}{\textit{Panel A: Static screens.}} \\
\midrule
 & Denmark & Finland & Norway & Sweden\\
\midrule
MAJOR & Y & Y & Y & Y\\
TYPE & EQ & EQ & EQ & EQ\\
ISINID & P & P & P & P\\
GEOGN & DENMARK & FINLAND & NORWAY & SWEDEN\\
GEOLN & DENMARK & FINLAND & NORWAY & SWEDEN\\
PCUR & DK & FI, MK & NK & SK\\
GGSIN & DK & FI & NO &SE\\
\toprule
\multicolumn{5}{l}{\textit{Panel B: Country specific keywords.}} \\
\midrule
 & Denmark & Finland & Norway & Sweden\\
 \midrule
NAME & \multirow[m]{3}{*}{ \textbackslash \textbackslash)CSE \textbackslash \textbackslash} & \multirow[m]{3}{*}{USE} & & \multirow[m]{3}{13em}{CONVERTED INTO, USE, CONVERTED-, CONVERTED - SEE}\\
ENAME & & \\
ECNAME & &\\
\bottomrule
\end{tabularx}
\end{table}


\begin{table}[ht] 
\small
\caption[Common keywords]{\textbf{Common keywords}\\ Table shows the general keywords that were used to delete entries from all markets. Keyword deletion follows Ince and Porter \protect\citeyear{Ince2006} and Hanauer and Windmüller \protect\citeyear{HANAUER2023106712}. Same logic is applied to remove both country specific and generic keywords. Keyword is searched from Datastream attributes NAME, ENAME and ECNAME. In case if at least one of these attributes contains the keyword security is deleted from the dataset. To avoid deleting proper entries, security is only deleted if keyword occurs at the beginning of the name, at the end of the name or as separate word in the name.}
 \label{table:GeneralKeywords}
\centering
\begin{tabularx}{\textwidth}{l X}
\toprule
Security class 	& Keywords \\
\midrule
Duplicates 		& 1000DUPL, DULP, DUP, DUPE, DUPL, DUPLI, DUPLICATE, XSQ, XETa  \\[1ex]
Depository receipts	& ADR, GDR \\[1ex]
Preferred stock 	&  PF, ’PF’, PFD, PREF, PREFERRED, PRF\\ [1ex]
Warrants 			&  WARR, WARRANT, WARRANTS, WARRT, WTS, WTS2\\[1ex]
Debt 			& \%, DB, DCB, DEB, DEBENTURE, DEBENTURES, DEBT\\[1ex]
Unit trusts 		& .IT, .ITb, TST, INVESTMENTTRUST, RLSTIT, TRUST, TRUSTUNIT, TRUSTUNITS, TST, TSTUNIT, TST UNITS, UNIT, UNITTRUST, UNITS, UNT, UNTTST, UT\\[1ex]
ETFs 			& AMUNDI, ETF, INAV, ISHARES, JUNGE, LYXOR, X-TR\\[1ex]
Expired securities 	& EXPD, EXPIRED, EXPIRY, EXPY\\[1ex]
Miscellaneous 		& ADS, BOND, CAP.SHS, CONV, DEFER, DEP, DEPY, ELKS, FD, FUND, GW.FD, HI.YIELD, HIGHINCOME, IDX, INC.								\&GROWTH, INC.\&GW, INDEX, LP, MITS, MITT, MPS, NIKKEI, OPCVM, ORTF, PERQS, 												PFC, PFCL, PINES, PRTF, PTNS, PTSHP, QUIBS, QUIDS, RATE, RCPTS, REALEST, RECEIPTS, REIT, RESPT, 								RETUR, RIGHTS, RST, RTN.INC, RTS, SBVTG, SCORE, SPDR, STRYPES, TOPRS, UTS, VCT, VTG.SAS, 									XXXXX, YIELD,YLD, PF.SHS.\\
 \bottomrule
 \end{tabularx}
 \end{table} 

\begin{figure}[ht]
\centering
\caption[Number of companies]{\textbf{Number of companies}\\ Figure shows the development of total number of securities considered in the dataset from 1990 to 2022 for each Nordic country. Figures counts all securities that passed the static screens.}
\input{R_graphs/number_of_companies.tex}
\label{plot:number_of_companies}
\end{figure}

\begin{figure}[ht]
\centering
\caption[Time series of the mean cross-sectional properties]{\textbf{Time series of the mean cross-sectional properties}\\ ....}
\input{R_graphs/variable_ts.tex}
\label{plot:variableTS}
\end{figure}

\clearpage

\section{Benchmark factors}
\renewcommand{\thefigure}{B.\arabic{figure}}
\setcounter{figure}{0}
\renewcommand{\thetable}{B.\arabic{table}}
\setcounter{table}{0}

\begin{table}[ht]
\small
\caption[Benchmark factor correlation matrix]{\textbf{Benchmark factor correlation matrix}\\ Table shows the correlations among the benchmark factors. RMRF is the average value return of the pooled Nordic market. Portfolio returns are calculated based on 2 × 3 sorts on size and one other factor. HML is the difference in average of value weighted return of two high value portfolios and average of value weighted return of two low value portfolios. RMW, CMA and MOM are calculated in similar manner, but portfolio sorts are done based on investment, profitability momentum factors. SMB is the average of the value weighted returns of the 12 portfolios of small stocks minus the average of the value weighted returns of the 12 portfolios of big stocks. Returns are calculated in US dollars.}
\label{table:FFfactorsCorrelations}
\centering
\newcolumntype{Y}{>{\raggedleft\arraybackslash}X}
\begin{tabularx}{\textwidth}{@{\extracolsep{4pt}} X Y Y Y Y Y Y} 
\toprule
& HML & RMW & CMA & MOM & SMB & RMRF \\
\midrule
HML & 1 & -0.5707 & 0.5542 & 0.1122 & 0.2998 & -0.2740 \\
RMW & -0.5707 & 1 & -0.5899 & 0.0857 & -0.2568 & 0.0639 \\
CMA & 0.5542 & -0.5899 & 1 & -0.0703 & 0.1777 & -0.2078 \\
MOM & 0.1122 & 0.0857 & -0.0703 & 1 & 0.1544 & -0.2040 \\
SMB & 0.2998 & -0.2568 & 0.1777 & 0.1544 & 1 & -0.2695 \\
RMRF & -0.2740 & 0.0639 & -0.2078 & -0.2040 & -0.2695 & 1 \\
\bottomrule
\end{tabularx}
\end{table}

\begin{figure}[ht]
\centering
\caption[Factor autocorrelation]{\textbf{Factor autocorrelation}\\ .}
\input{R_graphs/tes.tex}
\label{plot:factor_autocorrelation}
\end{figure}

\clearpage

\section{Additional information}
\renewcommand{\thefigure}{C.\arabic{figure}}
\setcounter{figure}{0}
\renewcommand{\thetable}{C.\arabic{table}}
\setcounter{table}{0}

\begin{figure}[ht]
\centering
\caption[Time series of out-of-sample $R^2$]{\textbf{Time series of out-of-sample \boldmath$R^2$s}\\ Figures present the out-of-sample predictive performance of different machine learning models. Left side graphs show the out-of-sample $R^2$ values with benchmark prediction of zero. This method is described in section \ref{Methodology}. Additionally, traditional out-of-sample $R^2$s are displayed. In traditional out-of-sample $R^2$ benchmark prediction is the historical mean of corresponding stocks return. $R^2$s are calculated for each re-traning period.}
\input{R_graphs/R2_ts.tex}
\label{plot:OOSR2_ts}
\end{figure}

\begin{figure}[ht]
\centering
\caption[Turnover of highest epected return portfolios]{\textbf{Turnover of highest epected return portfolios}\\ .}
\input{R_graphs/turnover.tex}
\label{plot:Turnover}
\end{figure}

\begin{figure}[ht]
\centering
\caption[Optimized random forest hyperparameters]{\textbf{Random forest optimized hyper parameters}\\ .}
\input{R_graphs/hyper_param_rf.tex}
\label{plot:RFHyperParams}
\end{figure}

\clearpage
\bibliographystyle{apacite}
\bibliography{References}

\end{document}