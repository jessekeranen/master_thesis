---
title: "Random Forest"
author: "Jesse Ker√§nen"
date: "12/20/2023"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
rm(list = ls())

library(data.table)
library(ranger)
library(ggplot2)
library(tikzDevice)

set.seed(42)
```

# Preliminary steps

Process is started by loading the data file generated in Data_collection file. In case data file consists old expected returns, they will be removed. Seed is also set to ensure reproducibility of the results. Then set of explanatory variables is defined and formula random forest algorithm is formed from then. Additionally, data is sorted to chronological order and finally array of dates when the model will be retrained is formed. Model will be retrained each July. First training consists data from 50 first months. 
```{r}
load(file = "Data/data.Rdata")
data[, RF.EXP.RET := NULL]
setorder(data, Date, Company)

independend_variables <- c("CA", "CTO", "INV", "BEME", "CFP", "DEBT", "SP", "EP",
                           "ROA", "ROE", "Q", "MOM7", "MOM12", "MOM36", "MOM2",
                           "MOM.IND", "L.SD", "L.HIGH52.RATIO", "L.BETA",
                           "L.IDVOL", "L.LOG.USD.MV", "L.TO", "L.OBV")
# Regression formula
formula <- as.formula(paste("EXC.USD.RET", "~", paste(independend_variables,
                                                     collapse = " + "))) 

dates <- sort(data[, unique(Date)]) # Sort the data

# Dates when the model will be re trained.
trainings <- data[Date > dates[50] & lubridate::month(Date) == 7, unique(Date)]
trainings <- c(trainings, max(data$Date)) # Add last date
trainings <- sort(trainings) # Sort the training days
```

# Visualization

For visualization purposes one decisision tree is trained using rpart library. Tree is purely for visualization and is not any way related to actual model. Rpart libary is used because it offers nice visualization functionality for the decision trees which is also compatible with tikz library that is used to generate Latex file from the plot.
```{r, fig.dim = c(8, 8), message=FALSE, echo=FALSE, include=FALSE, results='hide'}
'library(rpart)

# Example Random Forest tree, only for illustration purposes
example_tree <- rpart(formula, method = "anova", data = na.omit(data[Date == as.POSIXct("2004-07-30", tz="UTC")]))
example_tree <- prune.rpart(example_tree, cp = 0.02)


tikz(file = "Latex/R_graphs/regr_tree.tex", width = 4, height = 4)
plot(example_tree, uniform = T)
text(example_tree, use.n = TRUE, cex = .7, xpd=T)
dev.off()'
```

# Expected returns

For each of the retraining date random_forest function is called. Inside the random_forest function data is limited to dates earlier to responding retraining date. Then the data is split to training and testing data using random 0.8/0.2 split. In each retraining date all hyperparameter combinations that will be optimized are looped through. Then based if the mean squared error on the test data improved, trained model will be used to predict the returns. 
```{r, warning=FALSE}
# Create hyperparameter grid
rf_grid <- expand.grid(dates = trainings, ntree = 300, mtry = c(2, 3, 5, 7),
                          max.depth = seq(2, 6), sample.fraction = 0.5, MSE = 0)
rf_grid <- setorder(as.data.table(rf_grid), dates)

# data[, RF.EXP.RET := runif(length(FM.EXP.RET))]

random_forest <- function(dt, grid, date_index, formula, independend_variables) {
  # browser()
  print(trainings[date_index])
  mse_lowest <- .Machine$integer.max
  
  data <- dt[Date < trainings[date_index]]
  
  # Split the sample to training and testing data
  sample <- sample(c(TRUE, FALSE), nrow(data), replace = TRUE, prob=c(0.8, 0.2))
  train  <- data[sample, ]
  test   <- data[!sample, ]
  
  grid_size <- length(grid$dates) / uniqueN(grid$dates)
  
  # Loop through the hyperparameter combinations
  for (j in 1:grid_size) {
    # browser()
    index <- (date_index - 1) * grid_size + j # Update the index
    
    # Train the random forest model with training data
    rf <- ranger(formula, data = train,
                 num.tree = grid$ntree[index], mtry = grid$mtry[index],
                 max.depth = grid$max.depth[index], 
                 sample.fraction = grid$sample.fraction[index])
    
    # Calculate the MSE for test data
    grid$MSE[index] <- test[, mean((EXC.USD.RET - predict(rf,
                                .SD[, ..independend_variables])$predictions)^2, na.rm = T)]
    
    # Check if the new hyperparameter initialization improved the model   
    if (grid$MSE[index] < mse_lowest) {
      mse_lowest <- grid$MSE[index] # Update the lowes MSE
      
      # Predict the returns for next 12 months
      dt[Date >= trainings[date_index] & Date < trainings[date_index + 1],
         RF.EXP.RET := predict(rf, .SD[, ..independend_variables])$predictions]
    }
    print(index) 
  }
  return(list(dt, grid))
}

for (i in 1:(length(trainings) - 1)) {
  lista <- random_forest(data, rf_grid, i, formula, independend_variables)
  data <- lista[[1]]
  rf_grid <- lista[[2]]
}
# summary(data[RF.EXP.RET != 0, lm(EXC.USD.RET ~ RF.EXP.RET)])

# Extract the optimal hyperparameter values
optimal_grid <- rf_grid[dates < as.POSIXct("2022-12-30", tz = "UTC"),
                        .SD[which.min(MSE), .(ntree, mtry, max.depth, MSE,
                                                sample.fraction)], by = dates]
optimal_grid_long <- melt(optimal_grid[, -c("ntree", "sample.fraction", "MSE")],
                          id.vars = "dates")

tikz(file = "Latex/R_graphs/hyper_param_rf.tex", width = 6, height = 2)
plot <- ggplot(optimal_grid_long, aes(dates, value)) + geom_line() + 
  facet_wrap(~variable, scales = "free") + 
  scale_y_continuous(expand = c(0, 0), limits = c(0, NA)) +
  theme(axis.title = element_blank(), text = element_text(size = 9))
print(plot)
dev.off()

plot
```
Finally the data will be saved for later usage.
```{r}
save(data, file = "Data/data.Rdata")
save(optimal_grid, file = "Data/RF_Optimal_Grid.Rdata")
```

# Variable importance

Variable importance for random forest approach is calculated replacing one variable at a time by 0 and then repeating above steps. Once the predicted returns are obtained for the reduced model, they are saved one by one to the RData files in Data folder for later usage.
```{r, warning=FALSE}
invisible(lapply(independend_variables, function(x) {
  set.seed(42)
  r2 <- copy(data)
  r2[, (x) := 0]
  
  for (i in 1:(length(trainings) - 1)) {
    lista <- random_forest(r2, optimal_grid, i, formula, independend_variables)
    r2 <- lista[[1]]
  }
  print(x)
  r2 <- r2[, .(Date, Company, EXC.USD.RET, RF.EXP.RET)][, Variable := x]
  file_name <- paste("Data/RF/", x, ".Rdata", sep = "")
  save(r2, file = file_name)
}))
```

