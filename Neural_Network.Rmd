---
title: "Neural network"
author: "Jesse Ker√§nen"
date: "12/21/2023"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
rm(list = ls(all.names = TRUE))
gc()


library(keras)
library(tensorflow)
tensorflow::set_random_seed(42, disable_gpu = TRUE)
library(data.table)
library(ggplot2)
library(neuralnet)
library(tikzDevice)
library(zoo)
```

# Preliminary steps

Process is started by loading the data file generated in Data_collection file. In case data file consists old expected returns, they will be removed. Seed is also set to ensure reproducibility of the results. Funciton set_random_seed from tensorflo library should set all necesary seeds including the seeds for python, in order to make the result reproducable. Then set of explanatory variables is defined. Additionally, data is sorted to chronological order and finally array of dates when the model will be retrained is formed. Model will be retrained each July. First training consists data from 50 first months. 
```{r}
load(file = "Data/data.Rdata")
data[, NN.EXP.RET := NULL]
setorder(data, Date, Company)
```

```{r}
independend_variables <- c("CA", "CTO", "INV", "BEME", "CFP", "DEBT", "SP", "EP",
                           "ROA", "ROE", "Q", "MOM7", "MOM12", "MOM36", "MOM2",
                           "MOM.IND", "L.SD", "L.HIGH52.RATIO", "L.BETA",
                           "L.IDVOL", "L.LOG.USD.MV", "L.TO", "L.OBV")

dates <- sort(data[, unique(Date)])

trainings <- data[Date > dates[50] & lubridate::month(Date) == 7, unique(Date)]
trainings <- c(trainings, max(data$Date)) # Add last date
trainings <- sort(trainings)
```

Neural network for illustration purposes.
```{r, fig.dim = c(8, 8), message=FALSE, echo=FALSE, include=FALSE}
model <- neuralnet(EXC.USD.RET ~ MOM12 + L.IDVOL + L.LOG.USD.MV + CA,
                  data = data[Date == as.POSIXct("2010-09-30", tz = "UTC")],
                  hidden = c(4,2),
                  linear.output = T)

tikz(file = "Latex/R_graphs/NN.tex", width = 5.5, height = 4)
invisible(plot(model, rep = "best", information = F, cex=.7))
dev.off()
```

Function for learning rate scheduler. After each epoch scheduler function is called. After ten epoch function multiplies the learning rate of the model by 0.9.
```{r}
scheduler <- function(epoch, lr) {
  if (epoch < 10) {
    return(lr)    
  }
  else {
    return(lr * 0.9)
  }
}
```

# Expected returns

For each of the retraining date neural_network function is called. Inside the neural_network function data is limited to dates earlier to responding retraining date. Then the data is split to training and testing data using random 0.8/0.2 split. Total 5 neural network models are trained and the final prediction is the average of the models.
```{r}
# Grid to keep track of MSE
NN_grid <- expand.grid(dates = trainings, MSE = 0)
NN_grid <- setorder(as.data.table(NN_grid), dates)
NN_grid_size <- length(NN_grid$dates) / uniqueN(NN_grid$dates)

data[, `:=` (NN.EXP.RET = NULL, NN1.EXP.RET = NULL, NN2.EXP.RET = NULL,
             NN3.EXP.RET = NULL, NN4.EXP.RET = NULL, NN5.EXP.RET = NULL)]

neural_network <- function(dt, grid, date_index, independend_variables) {
  # Define callbacks for early stopping and learning rate scheduler
  callback_1 <- callback_early_stopping(monitor = 'val_loss', patience = 5)
  #callback_2 <- callback_reduce_lr_on_plateau(monitor = 'val_loss', factor = 0.5,
  #                            patience = 1)
  callback_3 <- callback_learning_rate_scheduler(scheduler)
  
  # Initialize the models
  model_1 <- keras_model_sequential()
  model_2 <- keras_model_sequential()
  model_3 <- keras_model_sequential()
  model_4 <- keras_model_sequential()
  model_5 <- keras_model_sequential()
  
  model_1 %>%
        layer_batch_normalization() %>%
        layer_dense(units = 10, activation = 'relu',
                    kernel_initializer = initializer_random_normal(seed = 10),
                    bias_initializer = initializer_constant(0.0)) %>%
        layer_batch_normalization() %>%
        layer_dense(units = 5, activation = 'relu',
                    kernel_initializer = initializer_random_normal(seed = 20),
                    bias_initializer = initializer_constant(0.0)) %>%
        layer_batch_normalization() %>%
        layer_dense(units = 1, activation = "linear",
                    kernel_initializer = initializer_random_normal(seed = 1),
                    bias_initializer = initializer_constant(0.0))
  model_2 %>%
        layer_batch_normalization() %>%
        layer_dense(units = 10, activation = 'relu',
                    kernel_initializer = initializer_random_normal(seed = 11),
                    bias_initializer = initializer_constant(0.0)) %>%
        layer_batch_normalization() %>%
        layer_dense(units = 5, activation = 'relu',
                    kernel_initializer = initializer_random_normal(seed = 21),
                    bias_initializer = initializer_constant(0.0)) %>%
        layer_batch_normalization() %>%
        layer_dense(units = 1, activation = "linear",
                    kernel_initializer = initializer_random_normal(seed = 2),
                    bias_initializer = initializer_constant(0.0))
  model_3 %>%
        layer_batch_normalization() %>%
        layer_dense(units = 10, activation = 'relu',
                    kernel_initializer = initializer_random_normal(seed = 12),
                    bias_initializer = initializer_constant(0.0)) %>%
        layer_batch_normalization() %>%
        layer_dense(units = 5, activation = 'relu',
                    kernel_initializer = initializer_random_normal(seed = 22),
                    bias_initializer = initializer_constant(0.0)) %>%
        layer_batch_normalization() %>%
        layer_dense(units = 1, activation = "linear",
                    kernel_initializer = initializer_random_normal(seed = 3),
                    bias_initializer = initializer_constant(0.0))
  model_4 %>%
        layer_batch_normalization() %>%
        layer_dense(units = 10, activation = 'relu',
                    kernel_initializer = initializer_random_normal(seed = 13),
                    bias_initializer = initializer_constant(0.0)) %>%
        layer_batch_normalization() %>%
        layer_dense(units = 5, activation = 'relu',
                    kernel_initializer = initializer_random_normal(seed = 23),
                    bias_initializer = initializer_constant(0.0)) %>%
        layer_batch_normalization() %>%
        layer_dense(units = 1, activation = "linear",
                    kernel_initializer = initializer_random_normal(seed = 4),
                    bias_initializer = initializer_constant(0.0))
  model_5 %>%
        layer_batch_normalization() %>%
        layer_dense(units = 10, activation = 'relu',
                    kernel_initializer = initializer_random_normal(seed = 14),
                    bias_initializer = initializer_constant(0.0)) %>%
        layer_batch_normalization() %>%
        layer_dense(units = 5, activation = 'relu',
                    kernel_initializer = initializer_random_normal(seed = 24),
                    bias_initializer = initializer_constant(0.0)) %>%
        layer_batch_normalization() %>%
        layer_dense(units = 1, activation = "linear",
                    kernel_initializer = initializer_random_normal(seed = 5),
                    bias_initializer = initializer_constant(0.0))
  
  # Compile the models
  model_1 %>% compile(loss = 'mse', optimizer = optimizer_adam(learning_rate = 1e-3),
                      metrics = 'mae') 
  model_2 %>% compile(loss = 'mse', optimizer = optimizer_adam(learning_rate = 1e-3),
                      metrics = 'mae')
  model_3 %>% compile(loss = 'mse', optimizer = optimizer_adam(learning_rate = 1e-3),
                      metrics = 'mae')
  model_4 %>% compile(loss = 'mse', optimizer = optimizer_adam(learning_rate = 1e-3),
                      metrics = 'mae')
  model_5 %>% compile(loss = 'mse', optimizer = optimizer_adam(learning_rate = 1e-3),
                      metrics = 'mae')
  
  # browser()
  # Split the data
  data <- dt[Date < trainings[date_index]]
  sample <- sample(c(TRUE, FALSE), nrow(data), replace = TRUE, prob = c(0.8, 0.2))
  
  x_train <- as.matrix(data[sample, ..independend_variables])
  y_train <- as.matrix(data[sample, EXC.USD.RET])
  x_validation <- as.matrix(data[!sample, ..independend_variables])
  y_validation <- as.matrix(data[!sample, EXC.USD.RET])
  

    index <- (date_index - 1) * NN_grid_size
    # Fit the models
    model_1 %>% fit(x_train, y_train, epochs = 100, batch_size = 502, 
                  callbacks = list(callback_1, callback_3), 
                  validation_data = list(x_validation, y_validation), 
                  verbose = 0)
    model_2 %>% fit(x_train, y_train, epochs = 100, batch_size = 502, 
                  callbacks = list(callback_1, callback_3), 
                  validation_data = list(x_validation, y_validation), 
                  verbose = 0)
    model_3 %>% fit(x_train, y_train, epochs = 100, batch_size = 502, 
                  callbacks = list(callback_1, callback_3), 
                  validation_data = list(x_validation, y_validation), 
                  verbose = 0)
    model_4 %>% fit(x_train, y_train, epochs = 100, batch_size = 502, 
                  callbacks = list(callback_1, callback_3), 
                  validation_data = list(x_validation, y_validation), 
                  verbose = 0)
    model_5 %>% fit(x_train, y_train, epochs = 100, batch_size = 502, 
                  callbacks = list(callback_1, callback_3), 
                  validation_data = list(x_validation, y_validation), 
                  verbose = 0)
    
    # Evaluate the models
    mse_1 <- model_1 %>% evaluate(x_validation, y_validation, verbose = 0)
    mse_2 <- model_2 %>% evaluate(x_validation, y_validation, verbose = 0)
    mse_3 <- model_3 %>% evaluate(x_validation, y_validation, verbose = 0)
    mse_4 <- model_4 %>% evaluate(x_validation, y_validation, verbose = 0)
    mse_5 <- model_5 %>% evaluate(x_validation, y_validation, verbose = 0)
    
    grid$MSE[index] <- (mse_1[1] + mse_2[1] + mse_3[1] + mse_4[1] + mse_5[1]) / 5
    dt[Date >= trainings[date_index] & Date < trainings[date_index + 1], `:=`
        (NN1.EXP.RET = model_1 %>% predict(as.matrix(.SD[, ..independend_variables])),
        NN2.EXP.RET = model_2 %>% predict(as.matrix(.SD[, ..independend_variables])),
        NN3.EXP.RET = model_3 %>% predict(as.matrix(.SD[, ..independend_variables])),
        NN4.EXP.RET = model_4 %>% predict(as.matrix(.SD[, ..independend_variables])),
        NN5.EXP.RET = model_5 %>% predict(as.matrix(.SD[, ..independend_variables])))]
    print(index)
    
  return(list(dt, grid))
}

# Call neural network model for all retraining dates
for (i in 1:(length(trainings) - 1)) {
  lista <- neural_network(data, NN_grid, i, independend_variables)
  data <- lista[[1]]
  NN_grid <- lista[[2]]
}
# Final expected return
data[, NN.EXP.RET := (NN1.EXP.RET + NN2.EXP.RET +  NN3.EXP.RET + NN4.EXP.RET + 
                        NN5.EXP.RET) / 5, by = c("Date", "Company")]

ggplot(data[, mean((NN.EXP.RET - EXC.USD.RET)^2), by = Date], aes(Date, V1)) + geom_line() + scale_y_log10()

summary(data[NN.EXP.RET != 0, lm(EXC.USD.RET ~ NN.EXP.RET)])

#NN_optimal_grid <- NN_grid[, .SD[which.min(MSE), .(mtry, max.depth, MSE)], by = dates]
NN_optimal_grid <- melt(NN_grid, id.vars = "dates")

data[!is.na(NN.EXP.RET), cor(NN.EXP.RET, EXC.USD.RET)]

# Plot the MSE
ggplot(NN_optimal_grid, aes(dates, value)) + geom_line() + 
  scale_y_log10()
```
Finally the data will be saved for later usage.
```{r}
save(data, file = "Data/data.Rdata")
```

# Variable importance

Variable importance for neural network approach is calculated replacing one variable at a time by 0 and then repeating above steps. Once the predicted returns are obtained for the reduced model, they are saved one by one to the RData files in Data folder for later usage.
```{r}
NN_grid2 <- expand.grid(dates = trainings, MSE = 0)
NN_grid2 <- setorder(as.data.table(NN_grid2), dates)

invisible(lapply(independend_variables, function(x) {
  tensorflow::set_random_seed(42, disable_gpu = TRUE)
  r2 <- copy(data)
  setorder(r2, Date, Company)
  
  r2[, (x) := 0]
  r2[, `:=` (NN.EXP.RET = NULL, NN1.EXP.RET = NULL, NN2.EXP.RET = NULL,
             NN3.EXP.RET = NULL, NN4.EXP.RET = NULL, NN5.EXP.RET = NULL)]
  
  #for (i in 1:(length(trainings) - 1)) {
  for (i in 1:(length(trainings) - 1)) {
    lista <- neural_network(r2, NN_grid2, i, independend_variables)
    r2 <- lista[[1]]
  }
  print(x)
  r2[, NN.EXP.RET := (NN1.EXP.RET + NN2.EXP.RET +  NN3.EXP.RET + NN4.EXP.RET + 
                        NN5.EXP.RET) / 5, by = c("Date", "Company")]
  r2 <- r2[, .(Date, Company, EXC.USD.RET, NN.EXP.RET)][, Variable := x]
  file_name <- paste("Data/NN/", x, ".Rdata", sep = "")
  save(r2, file = file_name)
}))
```


