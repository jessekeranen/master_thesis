scheduler <- function(epoch, lr) {
if (epoch < 10) {
return(lr)
}
else {
return(lr)
}
}
NN_grid <- expand.grid(dates = trainings, MSE = 0)
NN_grid <- setorder(as.data.table(NN_grid), dates)
NN_grid_size <- length(NN_grid$dates) / uniqueN(NN_grid$dates)
data[, `:=` (NN.EXP.RET = NULL, NN1.EXP.RET = NULL, NN2.EXP.RET = NULL, NN3.EXP.RET = NULL,
NN4.EXP.RET = NULL, NN5.EXP.RET = NULL)]
neural_network <- function(dt, grid, date_index, independend_variables) {
set.seed(42)
callback_1 <- callback_early_stopping(monitor = 'val_loss', patience = 5)
#callback_2 <- callback_reduce_lr_on_plateau(monitor = 'val_loss', factor = 0.2,
#                            patience = 2)
callback_3 <- callback_learning_rate_scheduler(scheduler)
csv_logger_1 <- callback_csv_logger("model_1_logs.csv", append = T)
csv_logger_2 <- callback_csv_logger("model_2_logs.csv", append = T)
csv_logger_3 <- callback_csv_logger("model_3_logs.csv", append = T)
csv_logger_4 <- callback_csv_logger("model_4_logs.csv", append = T)
csv_logger_5 <- callback_csv_logger("model_5_logs.csv", append = T)
model_1 <- keras_model_sequential()
model_2 <- keras_model_sequential()
model_3 <- keras_model_sequential()
model_4 <- keras_model_sequential()
model_5 <- keras_model_sequential()
model_1 %>%
layer_batch_normalization() %>%
layer_dense(units = 16, activation = 'relu',
kernel_initializer = initializer_random_normal(seed = 10),
bias_initializer = initializer_constant(0.0)) %>%
layer_batch_normalization() %>%
layer_dense(units = 1, activation = "linear",
kernel_initializer = initializer_random_normal(seed = 1),
bias_initializer = initializer_constant(0.0))
model_2 %>%
layer_batch_normalization() %>%
layer_dense(units = 16, activation = 'relu',
kernel_initializer = initializer_random_normal(seed = 11),
bias_initializer = initializer_constant(0.0)) %>%
layer_batch_normalization() %>%
layer_dense(units = 1, activation = "linear",
kernel_initializer = initializer_random_normal(seed = 2),
bias_initializer = initializer_constant(0.0))
model_3 %>%
layer_batch_normalization() %>%
layer_dense(units = 16, activation = 'relu',
kernel_initializer = initializer_random_normal(seed = 12),
bias_initializer = initializer_constant(0.0)) %>%
layer_batch_normalization() %>%
layer_dense(units = 1, activation = "linear",
kernel_initializer = initializer_random_normal(seed = 3),
bias_initializer = initializer_constant(0.0))
model_4 %>%
layer_batch_normalization() %>%
layer_dense(units = 16, activation = 'relu',
kernel_initializer = initializer_random_normal(seed = 13),
bias_initializer = initializer_constant(0.0)) %>%
layer_batch_normalization() %>%
layer_dense(units = 1, activation = "linear",
kernel_initializer = initializer_random_normal(seed = 4),
bias_initializer = initializer_constant(0.0))
model_5 %>%
layer_batch_normalization() %>%
layer_dense(units = 16, activation = 'relu',
kernel_initializer = initializer_random_normal(seed = 14),
bias_initializer = initializer_constant(0.0)) %>%
layer_batch_normalization() %>%
layer_dense(units = 1, activation = "linear",
kernel_initializer = initializer_random_normal(seed = 5),
bias_initializer = initializer_constant(0.0))
model_1 %>% compile(loss = 'mse', optimizer = optimizer_adam(learning_rate = 1e-3),
metrics = 'mae')
model_2 %>% compile(loss = 'mse', optimizer = optimizer_adam(learning_rate = 1e-3),
metrics = 'mae')
model_3 %>% compile(loss = 'mse', optimizer = optimizer_adam(learning_rate = 1e-3),
metrics = 'mae')
model_4 %>% compile(loss = 'mse', optimizer = optimizer_adam(learning_rate = 1e-3),
metrics = 'mae')
model_5 %>% compile(loss = 'mse', optimizer = optimizer_adam(learning_rate = 1e-3),
metrics = 'mae')
data <- dt[Date < trainings[date_index]]
data <- data[sample(1:nrow(data))]
sample <- sample(c(TRUE, FALSE), nrow(data), replace = TRUE, prob = c(0.8, 0.2))
x_train <- as.matrix(data[sample, ..independend_variables])
y_train <- as.matrix(data[sample, EXC.USD.RET])
x_validation <- as.matrix(data[!sample, ..independend_variables])
y_validation <- as.matrix(data[!sample, EXC.USD.RET])
index <- (date_index - 1) * NN_grid_size
model_1 %>% fit(x_train, y_train, epochs = 100, batch_size = 502,
callbacks = list(callback_1, csv_logger_1, callback_3),
validation_data = list(x_validation, y_validation),
verbose = 0)
model_2 %>% fit(x_train, y_train, epochs = 100, batch_size = 502,
callbacks = list(callback_1, csv_logger_2, callback_3),
validation_data = list(x_validation, y_validation),
verbose = 0)
model_3 %>% fit(x_train, y_train, epochs = 100, batch_size = 502,
callbacks = list(callback_1, csv_logger_3, callback_3),
validation_data = list(x_validation, y_validation),
verbose = 0)
model_4 %>% fit(x_train, y_train, epochs = 100, batch_size = 502,
callbacks = list(callback_1, csv_logger_4, callback_3),
validation_data = list(x_validation, y_validation),
verbose = 0)
model_5 %>% fit(x_train, y_train, epochs = 100, batch_size = 502,
callbacks = list(callback_1, csv_logger_5, callback_3),
validation_data = list(x_validation, y_validation),
verbose = 0)
mse_1 <- model_1 %>% evaluate(x_validation, y_validation, verbose = 0)
mse_2 <- model_2 %>% evaluate(x_validation, y_validation, verbose = 0)
mse_3 <- model_3 %>% evaluate(x_validation, y_validation, verbose = 0)
mse_4 <- model_4 %>% evaluate(x_validation, y_validation, verbose = 0)
mse_5 <- model_5 %>% evaluate(x_validation, y_validation, verbose = 0)
grid$MSE[index] <- (mse_1[1] + mse_2[1] + mse_3[1] + mse_4[1] + mse_5[1]) / 5
dt[Date >= trainings[date_index] & Date < trainings[date_index + 1], `:=`
(NN1.EXP.RET = model_1 %>% predict(as.matrix(.SD[, ..independend_variables])),
NN2.EXP.RET = model_2 %>% predict(as.matrix(.SD[, ..independend_variables])),
NN3.EXP.RET = model_3 %>% predict(as.matrix(.SD[, ..independend_variables])),
NN4.EXP.RET = model_4 %>% predict(as.matrix(.SD[, ..independend_variables])),
NN5.EXP.RET = model_5 %>% predict(as.matrix(.SD[, ..independend_variables])))]
print(index)
return(list(dt, grid))
}
for (i in 1:(length(trainings) - 1)) {
lista <- neural_network(data, NN_grid, i, independend_variables)
data <- lista[[1]]
NN_grid <- lista[[2]]
}
data[, NN.EXP.RET := (NN1.EXP.RET + NN2.EXP.RET +  NN3.EXP.RET + NN4.EXP.RET +
NN5.EXP.RET) / 5, by = c("Date", "Company")]
ggplot(data[, mean((NN.EXP.RET - EXC.USD.RET)^2), by = Date], aes(Date, V1)) + geom_line() + scale_y_log10()
summary(data[NN.EXP.RET != 0, lm(EXC.USD.RET ~ NN.EXP.RET)])
#NN_optimal_grid <- NN_grid[, .SD[which.min(MSE), .(mtry, max.depth, MSE)], by = dates]
NN_optimal_grid <- melt(NN_grid, id.vars = "dates")
data[!is.na(NN.EXP.RET), cor(NN.EXP.RET, EXC.USD.RET)]
ggplot(NN_optimal_grid, aes(dates, value)) + geom_line() +
scale_y_log10()
save(data, file = "Data/data.Rdata")
knitr::opts_chunk$set(echo = TRUE)
library(data.table)
library(ggplot2)
library(zoo)
library(writexl)
library(gridExtra)
load(file = "Data/data.Rdata")
independend_variables <- c("CA", "CTO", "CFP", "BEME", "DEBT", "SP", "EP",
"ROA", "Q", "MOM7", "MOM12", "MOM36",
"MOM.IND", "L.SD", "L.HIGH52.RATIO", "L.BETA",
"L.IDVOL", "L.LOG.USD.MV", "L.OBV", "L.TO")
data2 <- copy(data[, .(Date, pf.size, year, Company, USD.RET, EXC.USD.RET,
L.USD.MV, FM.EXP.RET, RF.EXP.RET, NN.EXP.RET)])
setorder(data2)
quantiles <- seq(0.1, 0.9, by = 0.1)
factor <- function(dt, variable, name, labels, quantiles) {
# browser()
dt[, temp := cut(.SD[, get(variable)], breaks = c(-Inf, quantile(.SD[pf.size == "Big",
get(variable)], quantiles, na.rm = T), Inf), labels = labels), by = Date]
temp2 <- dt[is.na(temp)]
setnames(dt, "temp", name)
return(dt)
}
data2 <- factor(data2[Date > as.POSIXct("1994-7-31", tz = "UTC") &
Date < as.POSIXct("2022-12-30", tz = "UTC")],
"FM.EXP.RET", "FM.POSITION", c("FM.Short", "FM.2", "FM.3", "FM.4",
"FM.4", "FM.6", "FM.7", "FM.8", "FM.9", "FM.Long"), quantiles)
data2 <- factor(data2, "RF.EXP.RET", "RF.POSITION", c("RF.Short", "RF.2", "RF.3",
"RF.4", "RF.5", "RF.6", "RF.7", "RF.8", "RF.9", "RF.Long"),
quantiles)
data2 <- factor(data2, "NN.EXP.RET", "NN.POSITION", c("NN.Short", "NN.2", "NN.3",
"NN.4", "NN.5", "NN.6", "NN.7", "NN.8", "NN.9", "NN.Long"),
quantiles)
portfolioreturns <- function(dt, portfolio, return){
#browser()
portfolio_returns <- dt[, .(EW.RET = mean(EXC.USD.RET, na.rm = T),
VW.RET = weighted.mean(EXC.USD.RET, L.USD.MV, na.rm = T),
EW.EXP.RET = mean(get(return), na.rm = T),
VW.EXP.RET = weighted.mean(get(return), L.USD.MV, na.rm = T)),
by = c("Date", portfolio)]
portfolio_returns <- melt(portfolio_returns, id.vars = c("Date", portfolio))
return(dcast(portfolio_returns, paste("... ~ ", portfolio), value.var = "value"))
}
portfolio_returns1 <- portfolioreturns(data2, "FM.POSITION", "FM.EXP.RET")
portfolio_returns2 <- portfolioreturns(data2, "RF.POSITION", "RF.EXP.RET")
portfolio_returns3 <- portfolioreturns(data2, "NN.POSITION", "NN.EXP.RET")
portfolio_returns <- merge(portfolio_returns1, portfolio_returns2, by = c("Date", "variable"))
portfolio_returns <- merge(portfolio_returns, portfolio_returns3, by = c("Date", "variable"))
portfolio_returns[, `:=` (RF = RF.Long - RF.Short, FM = FM.Long - FM.Short,
NN = NN.Long - NN.Short), by = c("Date", "variable")]
portfolio_returns_long <- melt(portfolio_returns, id.vars = c("Date", "variable"),
variable.name = "Portfolio", value.name = "Return")
portfolio_returns_long <- portfolio_returns_long[, cum_prod := cumprod(Return + 1),
by = c("Portfolio", "variable")]
portfolio_returns_long[, `:=` (Method = substr(Portfolio, 1, 2),
Position = as.factor(substr(Portfolio, 4, length(Portfolio))))]
market_return <- data[Date >= min(portfolio_returns$Date),
.(VW.RET = weighted.mean(USD.RET, L.USD.MV), RF = mean(RF)),
by = Date]
setorder(market_return, Date)
market_return[, MKT.RET := VW.RET - RF][, cum_prod := cumprod(MKT.RET + 1)]
ggplot() + geom_line(data = portfolio_returns_long[Portfolio %in% c("FM", "RF", "NN")
& variable == "VW.RET"], aes(x = Date, y = cum_prod, color = Portfolio)) +
scale_y_log10() + theme(axis.title = element_blank(), legend.position = "bottom") +
geom_line(data = market_return, aes(x = Date, y = cum_prod))
ggplot() + geom_line(data = portfolio_returns_long[Portfolio %in% c("FM", "RF", "NN")
& variable == "EW.RET"], aes(x = Date, y = cum_prod, color = Portfolio)) +
scale_y_log10() + theme(axis.title = element_blank(), legend.position = "bottom") +
geom_line(data = market_return, aes(x = Date, y = cum_prod))
ggplot() + geom_line(data = portfolio_returns_long[Position %in% c("Long", "Short") &
variable == "VW.RET"], aes(Date, cum_prod, color = Method, linetype = Position)) +
scale_y_log10() + geom_line(data = market_return, aes(Date, cum_prod)) +
theme(axis.title = element_blank(), legend.position = "bottom")
ggplot() + geom_line(data = portfolio_returns_long[Position %in% c("Long", "Short") &
variable == "EW.RET"], aes(Date, cum_prod, color = Method, linetype = Position)) +
scale_y_log10() + geom_line(data = market_return, aes(Date, cum_prod)) +
theme(axis.title = element_blank(), legend.position = "bottom")
exp_portfolio_returns_long <- portfolio_returns_long[variable %in% c("EW.EXP.RET", "VW.EXP.RET")]
exp_portfolio_returns_long <- exp_portfolio_returns_long[, .(EXP.RET = mean(Return)),
by = c("variable", "Portfolio")]
exp_portfolio_returns_long[, variable := gsub(".EXP", "", variable)]
portfolio_returns_long2 <- portfolio_returns_long[variable %in% c("EW.RET", "VW.RET")]
# Check the portfolio statistics
min_max <- portfolio_returns_long2[, .(Min = min(Return), Max = max(Return)),
by = c("variable", "Portfolio")]
portfolio_statistics <- portfolio_returns_long2[, t.test(Return, na.rm = T),
by = c("variable", "Portfolio")]
portfolio_statistics[, Count := seq(.N), by = c("variable",
"Portfolio")][, Count := ifelse(Count == 1,
"Lower Bound", "Upper Bound")]
portfolio_statistics <- dcast(portfolio_statistics, ... ~ Count, value.var = "conf.int")
portfolio_statistics <- merge(portfolio_statistics, min_max,
by = c("variable", "Portfolio"))
portfolio_statistics <- merge(portfolio_statistics, exp_portfolio_returns_long,
by = c("variable", "Portfolio"))
round <- colnames(portfolio_statistics[, -c("variable", "Portfolio", "parameter",
"alternative", "method", "data.name")])
portfolio_statistics[, (round) := round(.SD, 4), .SDcols = round]
portfolio_statistics
oos_r2 <- function(dt, variable) {
# browser()
temp <- dt[, .(Date, EXC.USD.RET, Company, get(variable))]
temp <- temp[!is.na(V4) & Date > "1994-07-29", .(method = variable,
R2OOS = 1 - (sum((EXC.USD.RET - V4)^2) / (sum(EXC.USD.RET^2)))),
by = Date]
return(temp)
}
scale_oosr2 <- function(dt, exp_ret) {
# browser()
dt <- dt[, mean(R2OOS), by = method]
dt <- dt[, V1 := dt[method == exp_ret, V1] - V1][method != exp_ret]
dt[, V1 := V1 / sum(V1)]
return(dt)
}
load_exp_ret <- function(method, variable) {
# browser()
path <- paste("Data/", method, "/", variable , ".Rdata", sep = "")
load(file = path)
return(r2)
}
rf_exp_rets <- rbindlist(lapply(independend_variables, function(x) {
return(load_exp_ret("RF", x))
}))
rf_exp_rets <- dcast(rf_exp_rets, Date + Company + EXC.USD.RET ~ Variable, value.var = "RF.EXP.RET")
RF_r2s_ts <- rbindlist(lapply(independend_variables, function(x) {
return(oos_r2(rf_exp_rets, x))
}))
RF_r2s_ts <- rbind(RF_r2s_ts, oos_r2(data, "RF.EXP.RET"))
RF_r2s_scaled <- scale_oosr2(RF_r2s_ts, "RF.EXP.RET")
fm_exp_rets <- rbindlist(lapply(independend_variables, function(x) {
return(load_exp_ret("FM", x))
}))
fm_exp_rets <- dcast(fm_exp_rets, Date + Company + EXC.USD.RET ~ Variable, value.var = "FM.EXP.RET")
FM_r2s_ts <- rbindlist(lapply(independend_variables, function(x) {
return(oos_r2(fm_exp_rets, x))
}))
FM_r2s_ts <- rbind(FM_r2s_ts, oos_r2(data, "FM.EXP.RET"))
FM_r2s_scaled <- scale_oosr2(FM_r2s_ts, "FM.EXP.RET")
nn_exp_rets <- rbindlist(lapply(independend_variables, function(x) {
return(load_exp_ret("NN", x))
}))
nn_exp_rets <- dcast(nn_exp_rets, Date + Company + EXC.USD.RET ~ Variable, value.var = "NN.EXP.RET")
NN_r2s_ts <- rbindlist(lapply(independend_variables, function(x) {
return(oos_r2(nn_exp_rets, x))
}))
NN_r2s_ts <- rbind(NN_r2s_ts, oos_r2(data, "NN.EXP.RET"))
NN_r2s_scaled <- scale_oosr2(NN_r2s_ts, "NN.EXP.RET")
options(scipen = 999999)
RF_r2_plot <- ggplot(RF_r2s_scaled, aes(y = reorder(method, V1), x = V1)) +
geom_bar(stat='identity', fill = "#13bfc4") + theme(axis.title = element_blank())
FM_r2_plot <- ggplot(FM_r2s_scaled, aes(y = reorder(method, V1), x = V1)) +
geom_bar(stat='identity', fill = "#13bfc4") + theme(axis.title = element_blank())
NN_r2_plot <- ggplot(NN_r2s_scaled, aes(y = reorder(method, V1), x = V1)) +
geom_bar(stat='identity', fill = "#13bfc4") + theme(axis.title = element_blank())
grid.arrange(RF_r2_plot, FM_r2_plot, NN_r2_plot, nrow = 2)
drawdown <- function(dt) {
# browser()
cum_profit <- dt[, .(Date, CUM.RET = cumprod(RET + 1)),
by = c("variable", "Method")]
drawdown <- cum_profit[, .(DRAWDOWN = (CUM.RET - cummax(CUM.RET))/
cummax(CUM.RET)), by = c("variable", "Method")]
return(drawdown[, .(MDD = min(DRAWDOWN)), by = c("variable", "Method")])
}
high_low <- portfolio_returns[variable %in% c("EW.RET", "VW.RET"),
.(Date, variable, RF, FM, NN)]
high_low_long <- melt(high_low, id.vars = c("Date", "variable"),
variable.name = "Method", value.name = "RET")
max_drawdown <- drawdown(high_low_long)
min_one_month <- high_low_long[, min(RET), by = c("variable", "Method")]
load("Data/factors.Rdata")
factors_wide <- dcast(factors, Date ~ ..., value.var = "USD.RET")
factors_wide <- merge(high_low_long, factors_wide, by = "Date")
alphas <- factors_wide[, .(ALPHA = summary(lm(RET ~ RMRF + SMB + HML + RMW + CMA + MOM,
data = .SD))$coefficients["(Intercept)", "Estimate"]),
by = c("variable", "Method")]
t_value <- factors_wide[, .(T.VAL = summary(lm(RET ~ RMRF + SMB + HML + RMW + CMA + MOM,
data = .SD))$coefficients["(Intercept)", "t value"]),
by = c("variable", "Method")]
r_squared <- factors_wide[, .(R.SQUARED = summary(lm(RET ~ RMRF + SMB + HML + RMW + CMA + MOM,
data = .SD))$r.squared), by = c("variable", "Method")]
high_low_long[, mean(RET) / sd(RET), by = c("variable", "Method")]
turnover_long <- copy(data2)
turnover_long <- melt(turnover_long[, -c("pf.size", "year", "EXC.USD.RET",
"FM.EXP.RET", "NN.EXP.RET", "RF.EXP.RET")],
id.vars = c("Date", "Company", "USD.RET", "L.USD.MV"),
value.name = "Portfolio", variable.name = "Method")
# turnover_long[, c("Method", "Portfolio") := tstrsplit(value, "\\.")]
turnover_long <- turnover_long[Portfolio %in% c("FM.Long", "RF.Long", "NN.Long")]
turnover_long[, PORT.RET := weighted.mean(USD.RET, L.USD.MV, na.rm = T),
by = c("Date", "Portfolio")]
turnover_long[, V.WEIGHT := L.USD.MV / sum(L.USD.MV), by = c("Date", "Portfolio")]
turnover_long[, SCALED.RET := (USD.RET + 1)/(PORT.RET + 1)]
turnover_long[, SCALED.V.WEIGHT := V.WEIGHT * SCALED.RET]
# Create data.table with all possible date/id combinations
combinations <- turnover_long[, CJ(Date = Date, Company = Company, unique=TRUE)]
# Merge combinations and opt_port2. Then we have a observation for each stock for each date. When stock wasn't included in portfolio it will get V.WEIGHT 0. This way we can also account for stocks leaving portfolio.
turnover_long <- merge(combinations, turnover_long, all.x = T, by = c("Date", "Company"))
turnover_long[is.na(V.WEIGHT), V.WEIGHT := 0]
turnover_long[, BOP := shift(SCALED.V.WEIGHT, 1), by = c("Company", "Portfolio")]
turnover_long[, BOP := ifelse(is.na(BOP), 0, BOP)]
turnover_long[, CHANGE := V.WEIGHT - BOP]
#turnover_long[Date > "1991-07-31", 0.5*sum(abs(Change)), by = Date] %>% ggplot(aes(x = Date, y = V1)) + geom_bar(stat="identity") + labs(y = "Turnover")
turnover <- turnover_long[, 0.5 * sum(abs(CHANGE)), by = c("Date", "Portfolio")]
ggplot(turnover, aes(x = Date, y = V1)) +
geom_bar(stat="identity") + labs(y = "Turnover") +
facet_wrap(~Portfolio, scales = "free")
turnover[, mean(V1), by = Portfolio]
knitr::opts_chunk$set(echo = TRUE)
rm(list = ls(all.names = TRUE))
gc()
library(data.table)
library(ggplot2)
library(neuralnet)
library(zoo)
library(keras)
library(tensorflow)
tensorflow::set_random_seed(42)
load(file = "Data/data.Rdata")
data[, NN.EXP.RET := NULL]
independend_variables <- c("CA", "CTO", "CFP", "BEME", "DEBT", "SP", "EP",
"ROA", "Q", "MOM7", "MOM12", "MOM36",
"MOM.IND", "L.SD", "L.HIGH52.RATIO", "L.BETA",
"L.IDVOL", "L.LOG.USD.MV", "L.OBV", "L.TO")
#independend_variables <- c("CA", "CTO", "BEME", "DEBT", "SP", "EP",
#                           "ROE", "Q", "MOM2", "MOM7", "MOM12", "MOM36",
#                           "MOM.IND", "L.SD", "L.HIGH52.RATIO", "L.BETA",
#                           "L.IDVOL", "L.LOG.USD.MV", "L.OBV", "L.TO")
dates <- sort(data[, unique(Date)])
trainings <- data[Date > dates[50] & lubridate::month(Date) == 7, unique(Date)]
trainings <- c(trainings, max(data$Date)) # Add last date
trainings <- sort(trainings)
'model <- neuralnet(EXC.USD.RET ~ MOM12 + L.IDVOL + L.LOG.USD.MV + CA,
data = data,
hidden = c(4,2),
linear.output = T)
tikz(file = "NN.tex", width = 6, height = 4)
invisible(plot(model, rep = "best", information = F, cex=.7))
dev.off()'
scheduler <- function(epoch, lr) {
if (epoch < 10) {
return(lr)
}
else {
return(lr)
}
}
NN_grid <- expand.grid(dates = trainings, MSE = 0)
NN_grid <- setorder(as.data.table(NN_grid), dates)
NN_grid_size <- length(NN_grid$dates) / uniqueN(NN_grid$dates)
data[, `:=` (NN.EXP.RET = NULL, NN1.EXP.RET = NULL, NN2.EXP.RET = NULL, NN3.EXP.RET = NULL,
NN4.EXP.RET = NULL, NN5.EXP.RET = NULL)]
neural_network <- function(dt, grid, date_index, independend_variables) {
set.seed(42)
callback_1 <- callback_early_stopping(monitor = 'val_loss', patience = 5)
#callback_2 <- callback_reduce_lr_on_plateau(monitor = 'val_loss', factor = 0.2,
#                            patience = 2)
callback_3 <- callback_learning_rate_scheduler(scheduler)
csv_logger_1 <- callback_csv_logger("model_1_logs.csv", append = T)
csv_logger_2 <- callback_csv_logger("model_2_logs.csv", append = T)
csv_logger_3 <- callback_csv_logger("model_3_logs.csv", append = T)
csv_logger_4 <- callback_csv_logger("model_4_logs.csv", append = T)
csv_logger_5 <- callback_csv_logger("model_5_logs.csv", append = T)
model_1 <- keras_model_sequential()
model_2 <- keras_model_sequential()
model_3 <- keras_model_sequential()
model_4 <- keras_model_sequential()
model_5 <- keras_model_sequential()
model_1 %>%
layer_batch_normalization() %>%
layer_dense(units = 16, activation = 'relu',
kernel_initializer = initializer_random_normal(seed = 10),
bias_initializer = initializer_constant(0.0)) %>%
layer_batch_normalization() %>%
layer_dense(units = 1, activation = "linear",
kernel_initializer = initializer_random_normal(seed = 1),
bias_initializer = initializer_constant(0.0))
model_2 %>%
layer_batch_normalization() %>%
layer_dense(units = 16, activation = 'relu',
kernel_initializer = initializer_random_normal(seed = 11),
bias_initializer = initializer_constant(0.0)) %>%
layer_batch_normalization() %>%
layer_dense(units = 1, activation = "linear",
kernel_initializer = initializer_random_normal(seed = 2),
bias_initializer = initializer_constant(0.0))
model_3 %>%
layer_batch_normalization() %>%
layer_dense(units = 16, activation = 'relu',
kernel_initializer = initializer_random_normal(seed = 12),
bias_initializer = initializer_constant(0.0)) %>%
layer_batch_normalization() %>%
layer_dense(units = 1, activation = "linear",
kernel_initializer = initializer_random_normal(seed = 3),
bias_initializer = initializer_constant(0.0))
model_4 %>%
layer_batch_normalization() %>%
layer_dense(units = 16, activation = 'relu',
kernel_initializer = initializer_random_normal(seed = 13),
bias_initializer = initializer_constant(0.0)) %>%
layer_batch_normalization() %>%
layer_dense(units = 1, activation = "linear",
kernel_initializer = initializer_random_normal(seed = 4),
bias_initializer = initializer_constant(0.0))
model_5 %>%
layer_batch_normalization() %>%
layer_dense(units = 16, activation = 'relu',
kernel_initializer = initializer_random_normal(seed = 14),
bias_initializer = initializer_constant(0.0)) %>%
layer_batch_normalization() %>%
layer_dense(units = 1, activation = "linear",
kernel_initializer = initializer_random_normal(seed = 5),
bias_initializer = initializer_constant(0.0))
model_1 %>% compile(loss = 'mse', optimizer = optimizer_adam(learning_rate = 1e-3),
metrics = 'mae')
model_2 %>% compile(loss = 'mse', optimizer = optimizer_adam(learning_rate = 1e-3),
metrics = 'mae')
model_3 %>% compile(loss = 'mse', optimizer = optimizer_adam(learning_rate = 1e-3),
metrics = 'mae')
model_4 %>% compile(loss = 'mse', optimizer = optimizer_adam(learning_rate = 1e-3),
metrics = 'mae')
model_5 %>% compile(loss = 'mse', optimizer = optimizer_adam(learning_rate = 1e-3),
metrics = 'mae')
data <- dt[Date < trainings[date_index]]
data <- data[sample(1:nrow(data))]
sample <- sample(c(TRUE, FALSE), nrow(data), replace = TRUE, prob = c(0.8, 0.2))
x_train <- as.matrix(data[sample, ..independend_variables])
y_train <- as.matrix(data[sample, EXC.USD.RET])
x_validation <- as.matrix(data[!sample, ..independend_variables])
y_validation <- as.matrix(data[!sample, EXC.USD.RET])
index <- (date_index - 1) * NN_grid_size
model_1 %>% fit(x_train, y_train, epochs = 100, batch_size = 502,
callbacks = list(callback_1, csv_logger_1, callback_3),
validation_data = list(x_validation, y_validation),
verbose = 0)
model_2 %>% fit(x_train, y_train, epochs = 100, batch_size = 502,
callbacks = list(callback_1, csv_logger_2, callback_3),
validation_data = list(x_validation, y_validation),
verbose = 0)
model_3 %>% fit(x_train, y_train, epochs = 100, batch_size = 502,
callbacks = list(callback_1, csv_logger_3, callback_3),
validation_data = list(x_validation, y_validation),
verbose = 0)
model_4 %>% fit(x_train, y_train, epochs = 100, batch_size = 502,
callbacks = list(callback_1, csv_logger_4, callback_3),
validation_data = list(x_validation, y_validation),
verbose = 0)
model_5 %>% fit(x_train, y_train, epochs = 100, batch_size = 502,
callbacks = list(callback_1, csv_logger_5, callback_3),
validation_data = list(x_validation, y_validation),
verbose = 0)
mse_1 <- model_1 %>% evaluate(x_validation, y_validation, verbose = 0)
mse_2 <- model_2 %>% evaluate(x_validation, y_validation, verbose = 0)
mse_3 <- model_3 %>% evaluate(x_validation, y_validation, verbose = 0)
mse_4 <- model_4 %>% evaluate(x_validation, y_validation, verbose = 0)
mse_5 <- model_5 %>% evaluate(x_validation, y_validation, verbose = 0)
grid$MSE[index] <- (mse_1[1] + mse_2[1] + mse_3[1] + mse_4[1] + mse_5[1]) / 5
dt[Date >= trainings[date_index] & Date < trainings[date_index + 1], `:=`
(NN1.EXP.RET = model_1 %>% predict(as.matrix(.SD[, ..independend_variables])),
NN2.EXP.RET = model_2 %>% predict(as.matrix(.SD[, ..independend_variables])),
NN3.EXP.RET = model_3 %>% predict(as.matrix(.SD[, ..independend_variables])),
NN4.EXP.RET = model_4 %>% predict(as.matrix(.SD[, ..independend_variables])),
NN5.EXP.RET = model_5 %>% predict(as.matrix(.SD[, ..independend_variables])))]
print(index)
return(list(dt, grid))
}
for (i in 1:(length(trainings) - 1)) {
lista <- neural_network(data, NN_grid, i, independend_variables)
data <- lista[[1]]
NN_grid <- lista[[2]]
}
