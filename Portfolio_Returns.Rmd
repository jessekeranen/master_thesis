---
title: "Portofolio returns"
author: "Jesse Ker√§nen"
date: "12/21/2023"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(data.table)
library(ggplot2)
library(zoo)
library(writexl)
library(gridExtra)
library(tikzDevice)
library(forecast)
library(sandwich)

theme_set(theme_grey())
```

# Model performance

This script finally calculates the performance statistics for the machine learning models. Prequisite for this file it to run Data_collection_v2.Rmd, Fama_French_Factors.Rmd, Fama_MacBeth.Rmd, Random_Forest.Rmd and Neural_Network.Rmd files. First file constructs firm level characteristics from the raw data and second file constructs the benchmark factors. Last three files focus on the stock return predictions. These files add full model predictions to the "data" file. Additionally they will save reduced model predictions to specific folders, which will be then loaded in this file in order to evaluate variable importance. Each of the previously mentioned files has to be run before running this file.

First step is to load the dataset which contains all characteristics and return predictions. Additionally independent variables are defined and the dataset is ordered.
```{r}
load(file = "Data/data.Rdata")
independend_variables <- c("CA", "CTO", "BEME", "CFP", "INV", "DEBT", "SP", "EP",
                           "ROA", "ROE", "Q", "MOM7", "MOM12", "MOM36", "MOM2",
                           "MOM.IND", "L.SD", "L.HIGH52.RATIO", "L.BETA",
                           "L.IDVOL", "L.LOG.USD.MV", "L.TO", "L.OBV")
setorder(data, Date, Company)
```

# Portfolio returns

This chunk forms expected return portfolios. Based on return predictions from each models all stocks are divide to ten expected return portfolios each month. Breakpoints for the allocation are calculated from expected returns of large market value stocks. Large market capitalization stocks are the biggest stocks that count for 90% if the aggregated market value in each month. Even though the breakpoints are defined only by the big stocks, all stocks are allocated to the portfolios. First companies are allocated to portfolios and then equal and market value weighted portfolios returns are calculated. Additionally long-short portfolio returns are calculated for each method as difference between highest and lowest expected return portfolio.

Code contains two functions. Function "factor" does the allocation as described above. It takes five arguments:
*dt = dataset
*variable = name of the expected return column for corresponding model
*name = name that will be used for created portfolio allocation column
*labels = array of expected return portfolio names
*quantiles = quantiles used to calculate expected return breakpoints monthly
This function is called separately for all models. Function uses variable pf.size to define which companies are considered big in order to calculate breakpoint expected returns each month. This variable was calculated in Data_collection_v2.Rmd file.

Second function is "portfolioreturns". It is used to calculate equal and weighted realized and predicted returns for portfolios formed in function "factor". Function takes three arguments:
*dt = dataset
*portfolio = name of the column where portfolio allocation for corresponding model is indicated
*return = name of the expected return column for corresponding model
Also this function is called separately for all models.

Finally long-short portfolios and market returns are calculated and cumulative returns for different portfolios are plotted.
```{r}
data2 <- copy(data[, .(Date, pf.size, year, Company, USD.RET, EXC.USD.RET,
                       L.USD.MV, FM.EXP.RET, RF.EXP.RET, NN.EXP.RET)])
setorder(data2)
quantiles <- seq(0.1, 0.9, by = 0.1) # Expected return quantiles

# Function to allocate companies to portfolios
factor <- function(dt, variable, name, labels, quantiles) {
  # browser()
  dt[, temp := cut(.SD[, get(variable)], 
                   breaks = c(-Inf, quantile(.SD[pf.size == "Big", get(variable)],
                        quantiles, na.rm = T), Inf), labels = labels), by = Date]
  temp2 <- dt[is.na(temp)]
  setnames(dt, "temp", name)
  return(dt)
}
data2 <- factor(data2[Date >= as.POSIXct("1994-7-31", tz = "UTC") &
                      Date < as.POSIXct("2022-12-30", tz = "UTC")],
                "FM.EXP.RET", "FM.POSITION", c("FM.Low", "FM.2", "FM.3", "FM.4",
                 "FM.5", "FM.6", "FM.7", "FM.8", "FM.9", "FM.High"), quantiles)
data2 <- factor(data2, "RF.EXP.RET", "RF.POSITION", c("RF.Low", "RF.2", "RF.3",
            "RF.4", "RF.5", "RF.6", "RF.7", "RF.8", "RF.9", "RF.High"), quantiles)
data2 <- factor(data2, "NN.EXP.RET", "NN.POSITION", c("NN.Low", "NN.2", "NN.3",
            "NN.4", "NN.5", "NN.6", "NN.7", "NN.8", "NN.9", "NN.High"), quantiles)

# Function to calculate portfolio returns
portfolioreturns <- function(dt, portfolio, return){
  # browser()
  portfolio_returns <- dt[, .(EW.RET = mean(EXC.USD.RET, na.rm = T), 
                      VW.RET = weighted.mean(EXC.USD.RET, L.USD.MV, na.rm = T),
                      EW.EXP.RET = mean(get(return), na.rm = T),
                      VW.EXP.RET = weighted.mean(get(return), L.USD.MV, na.rm = T)),
                      by = c("Date", portfolio)]
  portfolio_returns <- melt(portfolio_returns, id.vars = c("Date", portfolio))
  return(dcast(portfolio_returns, paste("... ~ ", portfolio), value.var = "value"))
}
portfolio_returns1 <- portfolioreturns(data2, "FM.POSITION", "FM.EXP.RET")
portfolio_returns2 <- portfolioreturns(data2, "RF.POSITION", "RF.EXP.RET")
portfolio_returns3 <- portfolioreturns(data2, "NN.POSITION", "NN.EXP.RET")

# Merge portfolio return
portfolio_returns <- merge(portfolio_returns1, portfolio_returns2,
                           by = c("Date", "variable"))
portfolio_returns <- merge(portfolio_returns, portfolio_returns3,
                           by = c("Date", "variable"))

# Calculate the long-short portfolio returns
portfolio_returns[, `:=` (RF = RF.High - RF.Low, FM = FM.High - FM.Low,
                          NN = NN.High - NN.Low), by = c("Date", "variable")]

# Long format for more convenient plotting
portfolio_returns_long <- melt(portfolio_returns, id.vars = c("Date", "variable"),
                               variable.name = "Portfolio", value.name = "Return")
portfolio_returns_long <- portfolio_returns_long[, cum_prod := cumprod(Return + 1),
                                                 by = c("Portfolio", "variable")]
portfolio_returns_long[, `:=` (Method = substr(Portfolio, 1, 2),
                    Position = as.factor(substr(Portfolio, 4, length(Portfolio))))]

# Value weighted market return
market_return <- data[Date >= min(portfolio_returns$Date), .(VW.RET = 
                      weighted.mean(USD.RET, L.USD.MV), RF = mean(RF)), by = Date]
setorder(market_return, Date)
# Excess market return and cumulative market return
market_return[, MKT.RET := VW.RET - RF][, cum_prod := cumprod(MKT.RET + 1)]

# Value weighted cumulative return of long-short portfolios
tikz(file = "Latex/R_graphs/cumul_vw_LS_return.tex", width = 5.5, height = 4)
plot1 <- ggplot() + geom_line(data = portfolio_returns_long[Portfolio %in% 
        c("FM", "RF", "NN") & variable == "VW.RET"], aes(x = Date, y = cum_prod, color = Method)) + scale_y_log10() + geom_line(data = market_return, aes(x = Date, y = cum_prod)) + theme(axis.title = element_blank(), legend.position = "bottom", text = element_text(size = 9))
print(plot1)
dev.off()

# Equal weighted cumulative return of long-short portfolios
tikz(file = "Latex/R_graphs/cumul_ew_LS_return.tex", width = 5.5, height = 4)
plot2 <- ggplot() + geom_line(data = portfolio_returns_long[Portfolio %in% c("FM", "RF", "NN") & variable == "EW.RET"], aes(x = Date, y = cum_prod, color = Method)) +
  scale_y_log10() + geom_line(data = market_return, aes(x = Date, y = cum_prod)) +
  theme(axis.title = element_blank(), legend.position = "bottom", 
        text = element_text(size = 9))
print(plot2)
dev.off()

# Value weighted cumulative return of highest and lowest expected return portfolio
tikz(file = "Latex/R_graphs/cumul_vw_portf_return.tex", width = 5.5, height = 4)
plot3 <- ggplot() + geom_line(data = portfolio_returns_long[Position %in% c("Low", "High") & variable == "VW.RET"], aes(Date, cum_prod, color = Method, linetype = Position)) + scale_y_log10() + geom_line(data = market_return, aes(Date, cum_prod)) +
  theme(axis.title = element_blank(), legend.position = "bottom", text =
          element_text(size = 9))
print(plot3)
dev.off()

# Equal weighted cumulative return of highest and lowest expected return portfolio
tikz(file = "Latex/R_graphs/cumul_ew_portf_return.tex", width = 5.5, height = 4)
plot4 <- ggplot() + geom_line(data = portfolio_returns_long[Position %in% c("Low", "High") & variable == "EW.RET"], aes(Date, cum_prod, color = Method, linetype = Position)) + scale_y_log10() + geom_line(data = market_return, aes(Date, cum_prod)) +
  theme(axis.title = element_blank(), legend.position = "bottom", text =
          element_text(size = 9))
print(plot4)
dev.off()
plot1
plot2
plot3
plot4
```

# Expected return portfolio statistics

Next performance metrics for expected return portfolios are calculated. First mean expected returns for all expected return portfolios for all models are calculated. Average expected returns are calculated both for equal and value weighted expected return portfolios. Later these means are merged to the portfolio statistics datatable.
```{r}
exp_portfolio_returns_long <- portfolio_returns_long[variable %in% c("EW.EXP.RET",
                                                                     "VW.EXP.RET")]
exp_portfolio_returns_long <- exp_portfolio_returns_long[, .(EXP.RET = mean(Return)),
                                                    by = c("variable", "Portfolio")]
exp_portfolio_returns_long[, variable := gsub(".EXP", "", variable)]
```

Next Sharpe ratio, standard deviation, minumum and maximum return for all portfolios are calculated. Values are calculated separately for each portfolio and equal and value-weighted portfolios. Also this datatable will be later merged to the portfolio statistics datatable.
```{r}
portfolio_returns_long2 <- portfolio_returns_long[variable %in% c("EW.RET", "VW.RET")]
sharpe <- portfolio_returns_long2[, .(SR = mean(Return) / sd(Return),
                              SD = sd(Return), Min = min(Return), Max = max(Return)),
                              by = c("variable", "Portfolio")]
```

"portfolio_statistics" contains the final performance statistics for the expected return portfolios. First mean returns, t-statistics, p-values and confidence intervals are obtained by calling t.test function. Then average expected returns, Sharpe ratios, standard deviations, minimum and maximum returns are merged to the "portfolio_statistics" datatable from above described tables. Finally, figures are rounded to four decimals.
```{r}
portfolio_statistics <- portfolio_returns_long2[, t.test(Return, na.rm = T),
                                                by = c("variable", "Portfolio")]
# Wrangle confidence interval to two columns
portfolio_statistics[, Count := seq(.N), by = c("variable","Portfolio")][,
                      Count := ifelse(Count == 1, "Lower Bound", "Upper Bound")]
portfolio_statistics <- dcast(portfolio_statistics, ... ~ Count, value.var = "conf.int")

# Merge above datatables
portfolio_statistics <- merge(portfolio_statistics, exp_portfolio_returns_long,
                              by = c("variable", "Portfolio"))
portfolio_statistics <- merge(portfolio_statistics, sharpe, 
                              by = c("variable", "Portfolio"))

# Round values to four decimals
round <- colnames(portfolio_statistics[, -c("variable", "Portfolio", "parameter",
                                        "alternative", "method", "data.name")])
portfolio_statistics[, (round) := round(.SD, 4), .SDcols = round]

portfolio_statistics[variable == "VW.RET" & substr(Portfolio, 1, 2) == "NN"]
```

# Characteristics importance

Three functions are used to calculate characteristics importance for different models. Same functions are called separately for all models. First function is called "load_exp_ret". This function takes two arguments:
*method = corresponding method
*variable = variable for which characteristic importance will be calculated
Together "method" and "variable" arguments will used to generate path from where presaved datatable will be obtained. This datatable contains expected returns from model (defined by "method") "variable" is excluded from explanatory variable set.

Second function is called "oos_r2". This function takes also two arguments:
*dt = dataset
*variable = variable for which characteristic importance will be calculated
Function calculates the out-of-sample r squared values for expected returns that are obtained from model where variable indicated by "variable" argument is excluded from explanatory variable set. Out-of-sample r squared values are calculated for each retraining period. Function calculates two out-of-sample r squared values. One where benchmark model is static prediction of zero and one where benchmark model is the historical mean return of the company. Function will return time series of out-of-sample r squared values.

Third function is called "scale_oosr2". This function is used to calculate reductions in out-of-sample r squared values compared to the full model. Function takes two arguments:
*dt = dataset
*exp_ret = name indicating out-of-sample r squared values for full model
First function calculates time series averages of the out-of-sample r squared values. This is done separately for two definitions of out-of-sample r squared returned by "oos_r2" function. Then average r squared values are subtracted from average r squared value of full model to get the change in r squared value. Finally within r squred definition changes are normalized to sum to one. 
```{r}
load_exp_ret <- function(method, variable) {
  # browser()
  path <- paste("Data/", method, "/", variable , ".Rdata", sep = "")
  load(file = path)
  return(r2)
}

oos_r2 <- function(dt, variable) {
  # browser()
  temp <- dt[, .(Date, EXC.USD.RET, MEAN.RET, Company, var = get(variable))]
  # Variable indicating retraining periods
  temp[, Date := ifelse(month(Date) >= 7, year(Date), year(Date) - 1)]
  # Calculate out-of-sample r squared values
  temp <- temp[!is.na(var) & Date > "1994", .(method = variable,
  R2OOS = 1 - (sum((EXC.USD.RET - var)^2) / (sum(EXC.USD.RET^2))),
  TRAD.R2OOS = 1 - (sum((EXC.USD.RET - var)^2) /
  (sum((EXC.USD.RET - MEAN.RET)^2)))), by = Date]
  temp <- melt(temp, measure.vars = c("R2OOS", "TRAD.R2OOS"), 
               variable.name = "R2.Method")
  return(temp)
}

scale_oosr2 <- function(dt, exp_ret) {
  # browser()
  dt <- dt[, mean(value), by = c("method", "R2.Method")]
  dt <- dt[, V1 := .SD[method == exp_ret, V1] - V1, R2.Method][method != exp_ret]
  dt[, V1 := V1 / abs(sum(V1)), by = R2.Method]
  return(dt)
}
```

As a preliminary step historical rolling mean stock returns are calculated for all companies. This is later used as a benchmark prediction for one of the out-of-sample r squared definitions.
```{r}
setorder(data, Date)
data[, MEAN.RET := shift(rollapplyr(EXC.USD.RET, .SD[, .N], mean, partial = T,
                                    na.rm = T), fill = 0), by = Company]
```

Variable importance for random forest model. First the reduced model expected returns are loaded for all variables using function "load_exp_ret". Then time series of out-of-sample r squared values are calculated for all reduced models and full model using function "oos_r2". Then r squared values are scaled using function "scale_oosr2". Additionally r squared values are calculated for whole dataset and for first half of the dataset.
```{r}
# Load the reduced model expected returns
rf_exp_rets <- rbindlist(lapply(independend_variables, function(x) {
  return(load_exp_ret("RF", x))
}))

rf_exp_rets <- dcast(rf_exp_rets, Date + Company + EXC.USD.RET ~ Variable,
                     value.var = "RF.EXP.RET")
# Merge the mean returns to reduced expected returns
rf_exp_rets <- merge(rf_exp_rets, data[, .(Date, Company, MEAN.RET)],
                     by = c("Date", "Company"))

# Calculate time series of r squared values for each reduced model
RF_VI_ts <- rbindlist(lapply(independend_variables, function(x) {
  return(oos_r2(rf_exp_rets, x))
}))
RF_VI_ts <- rbind(RF_VI_ts, oos_r2(data, "RF.EXP.RET"))
RF_r2s_scaled <- scale_oosr2(RF_VI_ts, "RF.EXP.RET")
# Variable importance ranking
RF_r2s_scaled[, Rank := rank(.SD$V1), by = R2.Method]

# Save r squared time series for later plotting
RF_r2_ts <- oos_r2(data, "RF.EXP.RET")

# R squared values for whole dataset
RF_r2 <- data[!is.na(RF.EXP.RET), 
        .(R2OOS = 1 - (sum((EXC.USD.RET - RF.EXP.RET)^2) / (sum(EXC.USD.RET^2))),
         TRAD.R2OOS = 1 - (sum((EXC.USD.RET - RF.EXP.RET)^2) / 
                             (sum((EXC.USD.RET - MEAN.RET)^2))))]

# R squared values for first half of the dataset
RF_r2_2008 <- data[!is.na(RF.EXP.RET) & Date < as.POSIXct("2008-12-31", tz = "UTC"), 
        .(R2OOS = 1 - (sum((EXC.USD.RET - RF.EXP.RET)^2) / (sum(EXC.USD.RET^2))),
         TRAD.R2OOS = 1 - (sum((EXC.USD.RET - RF.EXP.RET)^2) / 
                             (sum((EXC.USD.RET - MEAN.RET)^2))))]
```

Variable importance for linear regression model. First the reduced model expected returns are loaded for all variables using function "load_exp_ret". Then time series of out-of-sample r squared values are calculated for all reduced models and full model using function "oos_r2". Then r squared values are scaled using function "scale_oosr2". Additionally r squared values are calculated for whole dataset and for first half of the dataset.
```{r}
# Load the reduced model expected returns
fm_exp_rets <- rbindlist(lapply(independend_variables, function(x) {
  return(load_exp_ret("FM", x))
}))
fm_exp_rets <- dcast(fm_exp_rets, Date + Company + EXC.USD.RET ~ Variable,
                     value.var = "FM.EXP.RET")
# Merge the mean returns to reduced expected returns
fm_exp_rets <- merge(fm_exp_rets, data[, .(Date, Company, MEAN.RET)], 
                     by = c("Date", "Company"))

# Calculate time series of r squared values for each reduced model
FM_VI_ts <- rbindlist(lapply(independend_variables, function(x) {
  return(oos_r2(fm_exp_rets, x))
}))
FM_VI_ts <- rbind(FM_VI_ts, oos_r2(data, "FM.EXP.RET"))
FM_r2s_scaled <- scale_oosr2(FM_VI_ts, "FM.EXP.RET")
# Variable importance ranking
FM_r2s_scaled[, Rank := rank(.SD$V1), by = R2.Method]

# Save r squared time series for later plotting
FM_r2_ts <- oos_r2(data, "FM.EXP.RET")

# R squared values for whole dataset
FM_r2 <- data[!is.na(FM.EXP.RET), 
        .(R2OOS = 1 - (sum((EXC.USD.RET - FM.EXP.RET)^2) / (sum(EXC.USD.RET^2))),
          TRAD.R2OOS = 1 - (sum((EXC.USD.RET - FM.EXP.RET)^2) / 
                              (sum((EXC.USD.RET - MEAN.RET)^2))))]

# R squared values for first half of the dataset
FM_r2_2008 <- data[!is.na(FM.EXP.RET) & Date < as.POSIXct("2008-12-31", tz = "UTC"), 
        .(R2OOS = 1 - (sum((EXC.USD.RET - FM.EXP.RET)^2) / (sum(EXC.USD.RET^2))),
          TRAD.R2OOS = 1 - (sum((EXC.USD.RET - FM.EXP.RET)^2) / 
                              (sum((EXC.USD.RET - MEAN.RET)^2))))]
```

Variable importance for neural network model. First the reduced model expected returns are loaded for all variables using function "load_exp_ret". Then time series of out-of-sample r squared values are calculated for all reduced models and full model using function "oos_r2". Then r squared values are scaled using function "scale_oosr2". Additionally r squared values are calculated for whole dataset and for first half of the dataset.
```{r}
# Load the reduced model expected returns
nn_exp_rets <- rbindlist(lapply(independend_variables, function(x) {
  return(load_exp_ret("NN", x))
}))
nn_exp_rets <- dcast(nn_exp_rets, Date + Company + EXC.USD.RET ~ Variable,
                     value.var = "NN.EXP.RET")
# write_xlsx(nn_exp_rets, "test.xlsx")
# Merge the mean returns to reduced expected returns
nn_exp_rets <- merge(nn_exp_rets, data[, .(Date, Company, MEAN.RET)],
                     by = c("Date", "Company"))

# Calculate time series of r squared values for each reduced model
NN_VI_ts <- rbindlist(lapply(independend_variables, function(x) {
  return(oos_r2(nn_exp_rets, x))
}))
NN_VI_ts <- rbind(NN_VI_ts, oos_r2(data, "NN.EXP.RET"))
NN_r2s_scaled <- scale_oosr2(NN_VI_ts, "NN.EXP.RET")
# Variable importance ranking
NN_r2s_scaled[, Rank := rank(.SD$V1), by = R2.Method]

# Save r squared time series for later plotting
NN_r2_ts <- oos_r2(data, "NN.EXP.RET")

# R squared values for whole dataset
NN_r2 <- data[!is.na(NN.EXP.RET), 
        .(R2OOS = 1 - (sum((EXC.USD.RET - NN.EXP.RET)^2) / (sum(EXC.USD.RET^2))),
          TRAD.R2OOS = 1 - (sum((EXC.USD.RET - NN.EXP.RET)^2) /
                              (sum((EXC.USD.RET - MEAN.RET)^2))))]

# R squared values for first half of the dataset
NN_r2_2008 <- data[!is.na(NN.EXP.RET) & Date < as.POSIXct("2008-12-31", tz = "UTC"), 
        .(R2OOS = 1 - (sum((EXC.USD.RET - NN.EXP.RET)^2) / (sum(EXC.USD.RET^2))),
          TRAD.R2OOS = 1 - (sum((EXC.USD.RET - NN.EXP.RET)^2) / 
                              (sum((EXC.USD.RET - MEAN.RET)^2))))]

options(scipen = 999999)
```

Plotting the variable importance and the prediction accuracy. First time series development of the r squared values are plotted by merging the time series from three models.

Next FM_r2_plot, trad_FM_r2_plot, RF_r2_plot, trad_RF_r2_plot, NN_r2_plot, trad_NN_r2_plot are generated to plot the relative variable importance. These plots are arranged to one plot using grid.arrange. 

Finally combined variable importance is plotted, where variable importance ranks for all variables for all methods are plotted.
```{r}
r2_ts <- merge(FM_r2_ts[, .(Date, R2.Method, FM = value)],
               RF_r2_ts[, .(Date, R2.Method, RF = value)],
               by = c("Date", "R2.Method"))
r2_ts <- merge(r2_ts, NN_r2_ts[, .(Date, R2.Method, NN = value)], 
               by = c("Date", "R2.Method"))
r2_ts <- melt(r2_ts, measure.vars = c("FM", "RF", "NN"), value.name = "R2",
              variable.name = "Method")
r2_ts[, R2.Method := ifelse(R2.Method == "R2OOS", "$R^2_{oos}$",
                            "$R^2_{oos \ Trad.}$")]
r2_ts[, temp := paste(Method, R2.Method, sep = " ")]

tikz(file = "Latex/R_graphs/R2_ts.tex", width = 5.5, height = 5)
plot5 <- ggplot(r2_ts, aes(Date, R2)) + geom_line() + 
  facet_wrap(~temp, ncol = 2, scales = "free_x") +
  theme(axis.title = element_blank(), text = element_text(size = 9))
print(plot5)
dev.off()
plot5

FM_r2_plot <- ggplot(FM_r2s_scaled[R2.Method == "R2OOS"], aes(y = reorder(method, V1), x = V1)) +
  geom_bar(stat='identity', fill = "#13bfc4") +
  theme(axis.title = element_blank(), text = element_text(size = 8))

trad_FM_r2_plot <- ggplot(FM_r2s_scaled[R2.Method == "TRAD.R2OOS"], aes(y = reorder(method, V1), x = V1)) + geom_bar(stat='identity', fill = "#13bfc4") + theme(axis.title = element_blank(), text = element_text(size = 8))

RF_r2_plot <- ggplot(RF_r2s_scaled[R2.Method == "R2OOS"], aes(y = reorder(method, V1), x = V1)) + geom_bar(stat='identity', fill = "#13bfc4") + theme(axis.title = element_blank(), text = element_text(size = 8))

trad_RF_r2_plot <- ggplot(RF_r2s_scaled[R2.Method == "TRAD.R2OOS"], aes(y = reorder(method, V1), x = V1)) + geom_bar(stat='identity', fill = "#13bfc4") + theme(axis.title = element_blank(), text = element_text(size = 8))

NN_r2_plot <- ggplot(NN_r2s_scaled[R2.Method == "R2OOS"], aes(y = reorder(method, V1), x = V1)) + geom_bar(stat='identity', fill = "#13bfc4") + theme(axis.title = element_blank(), text = element_text(size = 8))

trad_NN_r2_plot <- ggplot(NN_r2s_scaled[R2.Method == "TRAD.R2OOS"], aes(y = reorder(method, V1), x = V1)) + geom_bar(stat='identity', fill = "#13bfc4") + theme(axis.title = element_blank(), text = element_text(size = 8))

grid.arrange(FM_r2_plot, trad_FM_r2_plot, RF_r2_plot, trad_RF_r2_plot, NN_r2_plot,
             trad_NN_r2_plot, ncol = 2)

tikz(file = "Latex/R_graphs/relative_VI.tex", width = 5.5)
plot6 <- grid.arrange(FM_r2_plot + 
                        facet_wrap(facets = as.factor('FM $R^2_{oos}$')) +
                ggtitle(NULL) + theme(axis.title = element_blank()),
trad_FM_r2_plot + facet_wrap(facets = as.factor("FM $R^2_{oos \\ Trad.}$")) +
                ggtitle(NULL) + theme(axis.title = element_blank()),
RF_r2_plot + facet_wrap(facets = as.factor("RF $R^2_{oos}$")) + 
                ggtitle(NULL) + theme(axis.title = element_blank()),
trad_RF_r2_plot + facet_wrap(facets = as.factor("RF $R^2_{oos \\ Trad.}$")) +
                ggtitle(NULL) +  theme(axis.title = element_blank()),
NN_r2_plot + facet_wrap(facets = as.factor("NN $R^2_{oos}$")) + 
                ggtitle(NULL) + theme(axis.title = element_blank()),
trad_NN_r2_plot + facet_wrap(facets = as.factor("NN $R^2_{oos \\ Trad.}$")) +
                ggtitle(NULL) + theme(axis.title = element_blank()), nrow = 3)
print(plot6)
dev.off()

comb_vi <- merge(RF_r2s_scaled[R2.Method == "R2OOS", .(method, RF = Rank)],
               FM_r2s_scaled[R2.Method == "R2OOS", .(method, FM = Rank)],
               by = "method")
comb_vi <- merge(comb_vi, NN_r2s_scaled[R2.Method == "R2OOS", .(method, NN = Rank)],
               by = "method")
comb_vi <- melt(comb_vi, id.vars = "method")
comb_vi$variable <- with(comb_vi, base::factor(variable, levels = c("FM", "RF", "NN")))

tikz(file = "Latex/R_graphs/combined_VI.tex", width = 5.8, height = 4)
plot7 <- ggplot(comb_vi, aes(variable, reorder(method, value))) + 
  geom_tile(aes(fill = value), show.legend = F) +
  scale_fill_distiller(palette = "Blues", direction = 1) + 
  scale_x_discrete(expand = c(0, 0)) + scale_y_discrete(expand = c(0, 0)) + 
  theme(axis.title = element_blank(), text = element_text(size = 9))
print(plot7)
dev.off()
plot7
```

# Long-short portfolio performance metrics

Next part will focus on calculating different kind of performance metrics for long-short portfolios. This is started by creating new datatable called "high_low" which only contains long-short portfolios. First metric that is calculated is the maximum drawdown that is calculated using function "drawdown". "drawdown" takes one argument:
*dt = dataset
It returns maximum drawdown rounded to four decimals for all methods and for equal and value weighting. Additionally, one month maximum loss and Sharpe ratio is calculated for all methods and for equal and value weighting as well.
```{r}
# Filter for long-short portfolios
high_low <- portfolio_returns[variable %in% c("EW.RET", "VW.RET"), 
                              .(Date, variable, RF, FM, NN)]
high_low_long <- melt(high_low, id.vars = c("Date", "variable"),
                      variable.name = "Method", value.name = "RET")

# Function to calculate maximum drawdown
drawdown <- function(dt) {
  # browser()
  cum_profit <- dt[, .(Date, CUM.RET = cumprod(RET + 1)), by = c("variable", "Method")]
  drawdown <- cum_profit[, .(DRAWDOWN = (CUM.RET - cummax(CUM.RET))/ cummax(CUM.RET)),
                         by = c("variable", "Method")]
  return(drawdown[, .(MDD = round(min(DRAWDOWN), 4)), by = c("variable", "Method")])
}
max_drawdown <- drawdown(high_low_long)

# Maximum one month loss
min_one_month <- high_low_long[, .(MIN.1MONTH = round(min(RET), 4)), 
                  by = c("variable", "Method")]

# Sharpe ratios for long-short portfolios 
sharpe_high_low <- high_low_long[, .(SHARPE = round(mean(RET) / sd(RET), 4)),
                     by = c("variable", "Method")]
```

Next risk adjusted returns of the long-short portfolios are examined by regressing the portfolios returns by the benchmark factors. Benchmark factors are constructed in Fama_French_Factors.Rmd file. From the time series regressions alphas, t-statistics and r squared values are saved. All values are rounded to four decimals and reported separately for all methods and for equal and value weighting.
```{r}
load("Data/factors.Rdata")
factors_wide <- dcast(factors, Date ~ ..., value.var = "USD.RET")
factors_wide <- merge(high_low_long, factors_wide, by = "Date")

FF_stats <- factors_wide[, {reg <- summary(lm(RET ~ RMRF + SMB + HML + RMW + 
                                CMA + MOM, data = .SD, na.action = na.omit))
  list(ALPHA = round(reg$coefficients["(Intercept)", "Estimate"], 4),
  T.STAT = round(reg$coefficients["(Intercept)", "t value"], 4),
  R.SQUARED = round(reg$r.squared, 4))}, by = c("variable", "Method")]
```

This part calculates the turnover for long-short portfolios. Calculation of turnover is started by getting rid of unnecessary columns and melting data to long format. Next value weighted weight of each company in their portfolio in each month is defined as their relative market value compared to total market value of corresponding portfolio. Equal weight is defined as one divided by number of companies in the portfolio. These are the weights immediately after portfolio formation.

Weights right before re-allocation are calculated by multiplying the start weight of a company by its scaled return. Scaled return is calculated by dividing company's return by portfolio's return. In order to account for situations where company is included or excluded to the portfolio "combinations" datatable is created, which is then merged to "turnover_long" datatable. Now we have datatable with all date company combinations for all portfolios. If weight is NA , this means that company was not part of respective portfolio in corresponding month and therefore NAs in weight column can be replaced by zero.

In order to have weight right before re-allocation and right after re-allocation in the same row, we shift scaled weights by one. Now we can calculate change in weight in given asset simply by calculating the difference between weight and shifted scaled weight. This process also accounts for situations where company is included or excluded completely from the portfolio. Monthly turnover is sum of absolute changes in the weights.  
```{r}
# Copy data and remove additional variables
turnover_long <- copy(data2)
turnover_long <- melt(turnover_long[, -c("pf.size", "year", "EXC.USD.RET",
                           "FM.EXP.RET", "NN.EXP.RET", "RF.EXP.RET")],
                      id.vars = c("Date", "Company", "USD.RET", "L.USD.MV"),
                      value.name = "Portfolio", variable.name = "Method")
turnover_long <- turnover_long[Portfolio %in% c("FM.High", "RF.High", "NN.High",
                                                "FM.Low", "RF.Low", "NN.Low")]

# Starting weights
turnover_long[, `:=` (V.WEIGHT = L.USD.MV / sum(L.USD.MV), E.WEIGHT = 1/.N),
              by = c("Date", "Portfolio")]
turnover_long <- melt(turnover_long, measure.vars = c("V.WEIGHT", "E.WEIGHT"),
                      value.name = "WEIGHT", variable.name = "TYPE")
turnover_long[, (c("Method", "Position")) := tstrsplit(Portfolio, ".", fixed = TRUE)]

# Account for position in short side of the portfolio
turnover_long[, USD.RET := ifelse(Position == "High", USD.RET, USD.RET * -1)]

# Portfolio return
turnover_long[, PORT.RET := weighted.mean(USD.RET, WEIGHT, na.rm = T),
              by = c("Date", "Portfolio", "TYPE")]
# Scaling the return
turnover_long[, SCALED.RET := (USD.RET + 1)/(PORT.RET + 1)]
turnover_long[, SCALED.WEIGHT := WEIGHT * SCALED.RET]

# Create data.table with all possible date/id combinations
combinations <- turnover_long[, CJ(Date = Date, Company = Company, unique = TRUE)]
turnover_long <- merge(combinations, turnover_long, all.x = T, by = c("Date", "Company"))
turnover_long[is.na(WEIGHT), WEIGHT := 0]
turnover_long[, BOP := shift(SCALED.WEIGHT, 1), by = c("Company", "Portfolio", "TYPE")]
turnover_long[, BOP := ifelse(is.na(BOP), 0, BOP)]
# Change in the weight after re-allocation
turnover_long[, CHANGE := WEIGHT - BOP]

# Sum of changes in the weights
turnover_long <- turnover_long[!is.na(Portfolio), .(sum(abs(CHANGE))),
                               by = c("Date", "Method", "TYPE")]
turnover_long[, temp := paste("$", Method, "_{", TYPE, "}", "$", sep = " ")]

# For visualization purposes mean turnovers for each re-training period
turnover_long[, Year := ifelse(month(Date) >= 7, year(Date), year(Date) - 1)]
turnover_long_plot <- turnover_long[, mean(V1), by = c("Year", "temp")]

tikz(file = "Latex/R_graphs/turnover.tex", width = 5.5, height = 5)
plot8 <- ggplot(turnover_long_plot, aes(x = Year, y = V1)) + geom_line() +
  labs(y = "Turnover") + facet_wrap(~temp, scales = "free", ncol = 2) +
  theme(axis.title = element_blank(), text = element_text(size = 9))
print(plot8)
dev.off()

plot8

# Average turnover
turnover <- turnover_long[, .(TO = round(mean(V1), 4)), by = c("Method", "TYPE")]
```

# Expected return regressions

Next the relationship between expected and realized returns is evaluated by regressing realized excess returns by the stock level out-of-sample return predictions. Regression is run separetely for each method. In addition to regressions univariate properties of the expected returns are also saved to "univariate" datatable. Final univariate properties are timeseries averages of cross-sectional means and standard deviations of expected returns for all models.
```{r}
# Subset only data that is needed
data3 <- melt(data2[, .(Date, Company, EXC.USD.RET, RF.EXP.RET, FM.EXP.RET, NN.EXP.RET)],
              id.vars = c("Date", "Company", "EXC.USD.RET"), variable.name = "Method",
              value.name = "EXP.RET")
# Run the regressions
data3[, print(list(.BY, summary(lm(data = .SD, formula = EXC.USD.RET ~ EXP.RET))),
              digits = 2), by = Method]

# Univariate properties
univariate <- data3[, .(Mean = mean(EXP.RET), SD = sd(EXP.RET)),
                    by = c("Method", "Date")]
univariate <- univariate[, .(Mean = round(mean(Mean), 4),
                             SD = round(mean(SD), 4)), by = Method]
```

# Diebold Mariano test

Pairwise model comparisons are done using Diebold-Mariano tests. First for all models prediction residuals are calculated. Then for each pair of residuals Diebold-Mariano statistics are calculated by calling "NeweyWest" function. Product of "NeweyWest" is first converted to standard errors and then Diebold Mariano statistics. Additionally corresponding p-values are returned.
```{r}
# Subset only data that is needed
diebold_mariano <- copy(data[, .(Date, Company, EXC.USD.RET,
                              FM.EXP.RET, RF.EXP.RET, NN.EXP.RET, hcol)])
diebold_mariano[, hcol := hcol + 1]
# Calculate the residuals
diebold_mariano <- diebold_mariano[, .(FM = mean((EXC.USD.RET - FM.EXP.RET)^2),
                            RF = mean((EXC.USD.RET - RF.EXP.RET)^2),
                            NN = mean((EXC.USD.RET - NN.EXP.RET)^2)), by = hcol]
setorder(diebold_mariano, hcol)

# Diebold Mariano test
diebold_mariano <- combn(diebold_mariano[!is.na(NN), 2:4], 2, function(x) {
  # browser()
  d <- as.matrix(x[, 1] - x[, 2])
  se <- sqrt(NeweyWest(lm(d ~ 1), lag = 4, prewhite = FALSE)) / sqrt(length(d))
  dm <- mean(d) / se
  return(list(dm, colnames(x), pt(-abs(dm), length(d) - 1)))
})
```

# Descriptive statistics

Calculation of descriptive statistics. First I calculate number of unique companies in whole dataset for each country and for the dataset as whole. Then number of companies in each country in each month is calculated. From this time series minimum, mamixum and mean number of companies is calculated again for each country and the pooled dataset. After company counts we repeat the same process for market values. First I calculate time series mean, median and aggregated market values for each country and pooled market for each month. Then the final values are just time series averages of respective variables.
```{r}
# Total number of unique companies
data[year >= 1991, uniqueN(Company), by = Country]
data[year >= 1991, uniqueN(Company)]

# Country level company counts
company_count <- data[year >= 1991, .(number_of_companies = uniqueN(Company)), 
                      by = c("Country", "Date")]
company_count <- company_count[, .(MIN = min(number_of_companies), 
                                   MAX = max(number_of_companies), 
                 MEAN = mean(number_of_companies)), by = Country]

# Company counts for whole Nordic market
company_count_tot <- data[year >= 1991, .(Country = "Nordic",
                           number_of_companies = uniqueN(Company)), by =  Date]
company_count_tot <- company_count_tot[, .(Country = "Nordic", 
                                           MIN = min(number_of_companies), 
                                           MAX = max(number_of_companies),
                                           MEAN = mean(number_of_companies))]
company_count <- rbind(company_count, company_count_tot)

# Country level market values
market_value <- data[, .(MEAN.MV = mean(L.USD.MV, na.rm = T), 
                         MEDIAN.MV = median(L.USD.MV, na.rm = T),
                         SUM.MV = sum(L.USD.MV)), by = c("Country", "Date")]
market_value <- market_value[, .(MEAN.MV = mean(MEAN.MV), 
                                 MEDIAN.MV = mean(MEDIAN.MV),
                                 SUM.MV = mean(SUM.MV)), by = Country]

# Market value statistics for whole nordic market
market_value_tot <-  data[, .(Country = "Nordic", 
                              MEAN.MV = mean(L.USD.MV, na.rm = T), 
                              MEDIAN.MV = median(L.USD.MV, na.rm = T), 
                              SUM.MV = sum(L.USD.MV)), by = c("Date")]
market_value_tot <- market_value_tot[, .(Country = "Nordic", 
                                         MEAN.MV = mean(MEAN.MV), 
                                         MEDIAN.MV = mean(MEDIAN.MV), 
                                         SUM.MV = mean(SUM.MV))]
market_value <- rbind(market_value, market_value_tot)

country_chacteristics <- merge(company_count, market_value, by = "Country")
setorder(country_chacteristics, Country)
```

After country level statistics descriptive statistics are calculated for all dependent and independent variables used in the study. Like with country statistics, I also calculate variable statistics both for separate markets and pooled market. For each variable I calculate mean and standard deviation for each month. For both mean and standard deviation the final figure is the time series average of means and standard deviations of respective variables.
```{r}
var <- c("EXC.USD.RET", independend_variables)
means <- data[, (lapply(.SD, mean, na.rm = T)), .SDcols = var, by = Date]
means <- melt(means, id.vars = "Date", value.name = "MEAN.Nordic")

sds <- data[, (lapply(.SD, sd, na.rm = T)),.SDcols = var, by = Date]
sds <- melt(sds, id.vars = "Date", value.name = "SD.Nordic")

mean_sd <- merge(means, sds, by = c("Date", "variable"))

means <- data[, (lapply(.SD, mean, na.rm = T)), .SDcols = var, 
              by = c("Date", "Country")]
means[, Country := paste("MEAN.", Country, sep = "")]
means <- melt(means, id.vars = c("Date", "Country"), value.name = "Mean")
means <- dcast(means, Date + variable ~ Country, value.var = "Mean")

sds <- data[, (lapply(.SD, sd, na.rm = T)),.SDcols = var, 
            by = c("Date", "Country")]
sds[, Country := paste("SD.", Country, sep = "")]
sds <- melt(sds, id.vars = c("Date", "Country"), value.name = "SD")
sds <- dcast(sds, Date + variable ~ Country, value.var = "SD")

mean_sd_country <- merge(means, sds, by = c("Date", "variable"))
mean_sd <- merge(mean_sd, mean_sd_country, by = c("Date", "variable"))

mean_sd <- mean_sd[, lapply(.SD, mean, na.rm = T), 
              .SDcols = colnames(mean_sd[, -c("Date", "variable")]), by = variable]

round <- colnames(mean_sd[, -c("variable")])
mean_sd[, (round) := round(.SD, 3), .SDcols = round]
```

As a final step development of the firm characteristics are plotted. This done simply by plotting the time series of cross-sectional means of each variable.
```{r, fig.height = 24}
variables <- c("Date", "USD.RET", "Company", independend_variables)
charc_ts <- data[, ..variables]
charc_ts <- melt(charc_ts, id.vars = c("Date", "Company"))
# Calculate cross-sectional means
charc_ts <- charc_ts[, .(mean = mean(value)), by = c("Date", "variable")]


tikz(file = "Latex/R_graphs/variable_ts.tex", width = 5.5, height = 8)
plot9 <- ggplot(charc_ts[Date > as.POSIXct("1994-01-01", tz = "UTC")], aes(Date, mean)) +
  geom_line() + facet_wrap(~variable, scales = "free", ncol = 3) +
  theme(axis.title = element_blank(), text = element_text(size = 9))
print(plot9)
dev.off()
```

